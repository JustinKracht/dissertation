
@article{1978,
  title = {Alternative Weighting Schemes for Linear Prediction},
  year = {1978},
  month = jun,
  journal = {Organizational Behavior and Human Performance},
  volume = {21},
  number = {3},
  pages = {316--345},
  publisher = {{Academic Press}},
  issn = {0030-5073},
  doi = {10/c43qjw},
  abstract = {An important problem in applied research is the prediction of scores on a criterion from scores on a set of predictors. Typically, least squares regre\ldots},
  langid = {english},
  file = {/home/justin/Zotero/storage/JTFEBCLK/0030507378900570.html}
}

@article{2013,
  title = {On the Selection of the Weighting Parameter Value in {{Principal Covariates Regression}}},
  year = {2013},
  month = apr,
  journal = {Chemometrics and Intelligent Laboratory Systems},
  volume = {123},
  pages = {36--43},
  publisher = {{Elsevier}},
  issn = {0169-7439},
  doi = {10/gh64jv},
  abstract = {Ordinary linear regression falls short when many predictors are available, especially when some of these are highly correlated with (a linear combinat\ldots},
  langid = {english},
  file = {/home/justin/Zotero/storage/JNCSGH75/S0169743913000324.html}
}

@misc{2018a,
  title = {Tutorial: 21 Fairness Definitions and Their Politics},
  shorttitle = {Tutorial},
  year = {2018},
  month = mar
}

@book{abadir2005,
  title = {Matrix {{Algebra}}},
  author = {Abadir, Karim},
  year = {2005},
  month = aug,
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York}},
  abstract = {The first volume of the Econometric Exercises Series, Matrix Algebra contains exercises relating to course material in matrix algebra that students are expected to know while enrolled in an (advanced) undegraduate or a postgraduate course in econometrics or statistics. The book features a comprehensive collection of exercises with complete answers. More than just a collection of exercises, the volume is a textbook organized in a completely different manner than the usual textbook. It can be used as a self-contained course in matrix algebra or as a supplementary text.},
  isbn = {978-0-521-53746-9},
  langid = {english}
}

@article{adachi2019,
  title = {Factor Analysis: {{Latent}} Variable, Matrix Decomposition, and Constrained Uniqueness Formulations},
  shorttitle = {Factor Analysis},
  author = {Adachi, Kohei},
  year = {2019},
  month = may,
  journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
  volume = {11},
  number = {3},
  pages = {e1458},
  issn = {1939-5108, 1939-0068},
  doi = {10/gjrkc8},
  langid = {english},
  file = {/home/justin/Zotero/storage/G6DQVXCZ/Adachi - 2019 - Factor analysis Latent variable, matrix decomposi.pdf}
}

@article{adams1997scaling,
  title = {Scaling Methodology and Procedures for the Mathematics and Science Scales},
  author = {Adams, Raymond J and Wu, Margaret L and Macaskill, Greg},
  year = {1997},
  journal = {TIMSS technical report},
  volume = {2},
  pages = {111--145},
  keywords = {⛔ No DOI found}
}

@article{aguinis2009,
  title = {Scale Coarseness as a Methodological Artifact: {{Correcting}} Correlation Coefficients Attenuated from Using Coarse Scales},
  shorttitle = {Scale {{Coarseness}} as a {{Methodological Artifact}}},
  author = {Aguinis, Herman and Pierce, Charles A. and Culpepper, Steven A.},
  year = {2009},
  journal = {Organizational Research Methods},
  volume = {12},
  number = {4},
  pages = {623--652},
  publisher = {{SAGE Publications Inc}},
  issn = {1094-4281},
  doi = {10/d9w5c7},
  abstract = {Scale coarseness is a pervasive yet ignored methodological artifact that attenuates observed correlation coefficients in relation to population coefficients. The authors describe how to disattenuate correlations that are biased by scale coarseness in primary-level as well as meta-analytic studies and derive the sampling error variance for the corrected correlation. Results of two Monte Carlo simulations reveal that the correction procedure is accurate and show the extent to which coarseness biases the correlation coefficient under various conditions (i.e., value of the population correlation, number of item scale points, and number of scale items). The authors also offer a Web-based computer program that disattenuates correlations at the primary-study level and computes the sampling error variance as well as confidence intervals for the corrected correlation. Using this program, which implements the correction in primary-level studies, and incorporating the suggested correction in meta-analytic reviews will lead to more accurate estimates of construct-level correlation coefficients.},
  langid = {english},
  keywords = {artifact,correlation coefficient,Likert-type scale,meta-analysis,scale coarseness},
  file = {/home/justin/Zotero/storage/ACGMHHIJ/Aguinis et al_2009_Scale Coarseness as a Methodological Artifact.pdf}
}

@article{aguinis2010,
  title = {Revival of Test Bias Research in Preemployment Testing},
  author = {Aguinis, Herman and Culpepper, Steven A. and Pierce, Charles A.},
  year = {2010},
  journal = {Journal of Applied Psychology},
  volume = {95},
  number = {4},
  pages = {648--680},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1854(Electronic),0021-9010(Print)},
  doi = {10/dbjtfx},
  abstract = {We developed a new analytic proof and conducted Monte Carlo simulations to assess the effects of methodological and statistical artifacts on the relative accuracy of intercept- and slope-based test bias assessment. The main simulation design included 3,185,000 unique combinations of a wide range of values for true intercept- and slope-based test bias, total sample size, proportion of minority group sample size to total sample size, predictor (i.e., preemployment test scores) and criterion (i.e., job performance) reliability, predictor range restriction, correlation between predictor scores and the dummy-coded grouping variable (e.g., ethnicity), and mean difference between predictor scores across groups. Results based on 15 billion 925 million individual samples of scores and more than 8 trillion 662 million individual scores raise questions about the established conclusion that test bias in preemployment testing is nonexistent and, if it exists, it only occurs regarding intercept-based differences that favor minority group members. Because of the prominence of test fairness in the popular media, legislation, and litigation, our results point to the need to revive test bias research in preemployment testing. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Human Resource Management,Job Applicant Screening,Job Performance,Minority Groups,Simulation,Test Bias,Testing},
  file = {/home/justin/Zotero/storage/6N5KN932/Aguinis et al. - 2010 - Revival of test bias research in preemployment tes.pdf;/home/justin/Zotero/storage/EMNSR4YN/2010-13313-004.html}
}

@incollection{akaike1973,
  title = {Information Theory and an Extension of the Maximum Likelihood Principle},
  booktitle = {Proceedings of the Second International Symposium on Information Theory},
  author = {Akaike, H.},
  editor = {Petrov, B. N. and Caski, F.},
  year = {1973},
  pages = {267--281},
  publisher = {{Akademiai Kiado}},
  address = {{Budapest}}
}

@article{al-homidan2008,
  title = {Semidefinite Programming for the Educational Testing Problem},
  author = {{Al-Homidan}, Suliman},
  year = {2008},
  month = sep,
  journal = {Central European Journal of Operations Research},
  volume = {16},
  number = {3},
  pages = {239--249},
  issn = {1435-246X, 1613-9178},
  doi = {10/bw6pht},
  langid = {english},
  file = {/home/justin/Zotero/storage/PPXYTKSL/Al-Homidan2008_Article_SemidefiniteProgrammingForTheE.pdf}
}

@book{americaneducationalresearchassociation2011a,
  title = {Report and Recommendations for the Reauthorization of the Institute of Education Sciences},
  editor = {American Educational Research Association},
  year = {2011},
  publisher = {{American Educational Research Association}},
  address = {{Washington, D.C}},
  isbn = {978-0-935302-35-6},
  lccn = {LB1028 .A435 2011},
  keywords = {Education,Research},
  annotation = {OCLC: ocn826867074},
  file = {/home/justin/Zotero/storage/T7TCWLBD/American Educational Research Association - 2011 - Report and recommendations for the reauthorization.pdf}
}

@article{auerswald2019,
  title = {How to Determine the Number of Factors to Retain in Exploratory Factor Analysis: {{A}} Comparison of Extraction Methods under Realistic Conditions},
  shorttitle = {How to Determine the Number of Factors to Retain in Exploratory Factor Analysis},
  author = {Auerswald, Max and Moshagen, Morten},
  year = {2019},
  journal = {Psychological Methods},
  volume = {24},
  number = {4},
  pages = {468--491},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10/gf5spw},
  abstract = {Exploratory factor analyses are commonly used to determine the underlying factors of multiple observed variables. Many criteria have been suggested to determine how many factors should be retained. In this study, we present an extensive Monte Carlo simulation to investigate the performance of extraction criteria under varying sample sizes, numbers of indicators per factor, loading magnitudes, underlying multivariate distributions of observed variables, as well as how the performance of the extraction criteria are influenced by the presence of cross-loadings and minor factors for unidimensional, orthogonal, and correlated factor models. We compared several variants of traditional parallel analysis (PA), the Kaiser-Guttman Criterion, and sequential {$\chi$}2 model tests (SMT) with 4 recently suggested methods: revised PA, comparison data (CD), the Hull method, and the Empirical Kaiser Criterion (EKC). No single extraction criterion performed best for every factor model. In unidimensional and orthogonal models, traditional PA, EKC, and Hull consistently displayed high hit rates even in small samples. Models with correlated factors were more challenging, where CD and SMT outperformed other methods, especially for shorter scales. Whereas the presence of cross-loadings generally increased accuracy, non-normality had virtually no effect on most criteria. We suggest researchers use a combination of SMT and either Hull, the EKC, or traditional PA, because the number of factors was almost always correctly retrieved if those methods converged. When the results of this combination rule are inconclusive, traditional PA, CD, and the EKC performed comparatively well. However, disagreement also suggests that factors will be harder to detect, increasing sample size requirements to N {$\geq$} 500. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Exploratory Factor Analysis,Factor Analysis,Sample Size,Simulation},
  file = {/home/justin/Zotero/storage/R5TG6TGY/2019-02883-001.html}
}

@article{austin2020,
  title = {Adjusting Group Intercept and Slope Bias in Predictive Equations},
  author = {Austin, Bruce W. and French, Brian F.},
  year = {2020},
  month = sep,
  journal = {Methodology},
  volume = {16},
  number = {3},
  pages = {241--257},
  issn = {1614-2241},
  doi = {10/gjrkdm},
  abstract = {Methods to assess measurement invariance in constructs have received much attention, as invariance is critical for accurate group comparisons. Less attention has been given to the identification and correction of the sources of non-invariance in predictive equations. This work developed correction factors for structural intercept and slope bias in common regression equations to address calls in the literature to revive test bias research. We demonstrated the correction factors in regression analyses within the context of a large international dataset containing 68 countries and regions (groups). A mathematics achievement score was predicted by a math self-efficacy score, which exhibited a lack of invariance across groups. The proposed correction factors significantly corrected structural intercept and slope bias across groups. The impact of the correction factors was greatest for groups with the largest amount of bias. Implications for both practice and methodological extensions are discussed.},
  copyright = {Copyright (c) 2020 Bruce W. Austin, Brian F. French},
  langid = {english},
  file = {/home/justin/Zotero/storage/T9V2IWFR/Austin and French - 2020 - Adjusting Group Intercept and Slope Bias in Predic.pdf;/home/justin/Zotero/storage/PDZI6XZ4/4001.html}
}

@book{banerjee2014,
  ids = {royLinearAlgebraMatrix2014},
  title = {Linear {{Algebra}} and {{Matrix Analysis}} for {{Statistics}}},
  author = {Banerjee, Sudipto and Roy, Anindya},
  year = {2014},
  publisher = {{CRC Press}},
  doi = {10.1201/b17040},
  abstract = {Linear Algebra and Matrix Analysis for Statistics offers a gradual exposition to linear algebra without sacrificing the rigor of the subject. It presents both the vector space approach and the canonical forms in matrix theory. The book is as self-contained as possible, assuming no prior knowledge of linear algebra.  The authors first address the rudimentary mechanics of linear systems using Gaussian elimination and the resulting decompositions. They introduce Euclidean vector spaces using less abstract concepts and make connections to systems of linear equations wherever possible. After illustrating the importance of the rank of a matrix, they discuss complementary subspaces, oblique projectors, orthogonality, orthogonal projections and projectors, and orthogonal reduction.  The text then shows how the theoretical concepts developed are handy in analyzing solutions for linear systems. The authors also explain how determinants are useful for characterizing and deriving properties concerning matrices and linear systems. They then cover eigenvalues, eigenvectors, singular value decomposition, Jordan decomposition (including a proof), quadratic forms, and Kronecker and Hadamard products. The book concludes with accessible treatments of advanced topics, such as linear iterative systems, convergence of matrices, more general vector spaces, linear transformations, and Hilbert spaces.},
  date-added = {2020-01-02 11:59:09 -0600},
  date-modified = {2020-01-02 11:59:16 -0600},
  googlebooks = {iIOhAwAAQBAJ},
  isbn = {978-1-4200-9538-8},
  langid = {english},
  keywords = {Mathematics / Algebra / General,Mathematics / Probability \& Statistics / General}
}

@article{barendse2015,
  title = {Using Exploratory Factor Analysis to Determine the Dimensionality of Discrete Responses},
  author = {Barendse, M. T. and Oort, F. J. and Timmerman, M. E.},
  year = {2015},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {22},
  number = {1},
  pages = {87--101},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/gdvmsz},
  abstract = {Exploratory factor analysis (EFA) is commonly used to determine the dimensionality of continuous data. In a simulation study we investigate its usefulness with discrete data. We vary response scales (continuous, dichotomous, polytomous), factor loadings (medium, high), sample size (small, large), and factor structure (simple, complex). For each condition, we generate 1,000 data sets and apply EFA with 5 estimation methods (maximum likelihood [ML] of covariances, ML of polychoric correlations, robust ML, weighted least squares [WLS], and robust WLS) and 3 fit criteria (chi-square test, root mean square error of approximation, and root mean square residual). The various EFA procedures recover more factors when sample size is large, factor loadings are high, factor structure is simple, and response scales have more options. Robust WLS of polychoric correlations is the preferred method, as it is theoretically justified and shows fewer convergence problems than the other estimation methods.},
  keywords = {discrete data,exploratory factor analysis,robust maximum likelihood estimation,weighted least squares estimation},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2014.934850},
  file = {/home/justin/Zotero/storage/YZHV5TNP/Barendse et al_2015_Using Exploratory Factor Analysis to Determine the Dimensionality of Discrete.pdf;/home/justin/Zotero/storage/VR8ZI853/10705511.2014.html}
}

@book{barocas-hardt-narayanan,
  title = {Fairness and Machine Learning},
  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  year = {2019},
  publisher = {{fairmlbook.org}},
  file = {/home/justin/Zotero/storage/KUU8B4VG/index.html}
}

@article{bauer2016,
  ids = {bauer2017},
  title = {A More General Model for Testing Measurement Invariance and Differential Item Functioning.},
  author = {Bauer, Daniel J.},
  year = {2016},
  journal = {Psychological Methods},
  volume = {22},
  number = {3},
  pages = {507},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1463},
  doi = {10/gbww6z},
  file = {/home/justin/Zotero/storage/T5NAAPT3/Bauer_2017_A more general model for testing measurement invariance and differential item.pdf;/home/justin/Zotero/storage/VWNWZX4D/Bauer_2016_A more general model for testing measurement invariance and differential item.pdf;/home/justin/Zotero/storage/6DMC9XTJ/2016-27721-001.html;/home/justin/Zotero/storage/BAMMP47D/2016-27721-001.html}
}

@article{beauducel2016,
  title = {On the Correlation of Common Factors with Variance Not Accounted for by the Factor Model},
  author = {Beauducel, Andr{\'e} and Hilger, Norbert},
  year = {2016},
  month = jul,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {45},
  number = {6},
  pages = {2145--2157},
  issn = {0361-0918, 1532-4141},
  doi = {10/gh64s7},
  langid = {english},
  file = {/home/justin/Zotero/storage/NZKBZR4P/Beauducel_Hilger_2016_On the Correlation of Common Factors with Variance Not Accounted for by the.pdf}
}

@article{bentler1972,
  ids = {bentlerLowerboundMethodDimensionfree1972},
  title = {A Lower-Bound Method for the Dimension-Free Measurement of Internal Consistency},
  author = {Bentler, Peter M.},
  year = {1972},
  journal = {Social Science Research},
  volume = {1},
  number = {4},
  pages = {343--357},
  publisher = {{Elsevier}},
  issn = {0049-089X},
  doi = {10/dthjjm},
  abstract = {Many social science variables are composites of number of component variables. For example, a social class measure may be obtained as a sum of occupation, income, dwelling area, and education measures. Generally, the components represent different aspects or facets of an attribute. A special case occurs when the components presumably measure the same aspects of an attribute; this is the well-known case of reliability. We obtain an index of the quality of the composite, as indicated by the internal consistency among the components. The more highly interrelated the components are, the more adequately the composite represents the components. We make some rather natural assumptions and derive a lower-bound index, called coefficient theta, for the internal consistency of the composite. Theta provides a better lower bound for the measurement of internal consistency than a typically-used index, coefficient alpha, although it is more difficult to compute. We present a significance test to determine whether \texttheta{} = 0 in the population. Finally, a number of examples are presented. In an appendix we prove that \texttheta{} {$\geq$} {$\alpha$}.},
  date-added = {2020-01-02 12:44:33 -0600},
  date-modified = {2020-01-02 12:44:40 -0600},
  langid = {english},
  keywords = {factor analysis,matrix smooth},
  file = {/home/justin/Zotero/storage/FIXME84D/0049089X72900828.html}
}

@article{bentler1980,
  title = {Significance Tests and Goodness of Fit in the Analysis of Covariance Structures},
  author = {Bentler, Peter M. and Bonett, Douglas G.},
  year = {1980},
  journal = {Psychological Bulletin},
  volume = {88},
  number = {3},
  pages = {588--606},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10/dbm},
  abstract = {Factor analysis, path analysis, structural equation modeling, and related multivariate statistical methods are based on maximum likelihood or generalized least squares estimation developed for covariance structure models (CSMs). Large-sample theory provides a chi-square goodness-of-fit test for comparing a model (M) against a general alternative M based on correlated variables. It is suggested that this comparison is insufficient for M evaluation. A general null M based on modified independence among variables is proposed as an additional reference point for the statistical and scientific evaluation of CSMs. Use of the null M in the context of a procedure that sequentially evaluates the statistical necessity of various sets of parameters places statistical methods in covariance structure analysis into a more complete framework. The concepts of ideal Ms and pseudo chi-square tests are introduced, and their roles in hypothesis testing are developed. The importance of supplementing statistical evaluation with incremental fit indices associated with the comparison of hierarchical Ms is also emphasized. Normed and nonnormed fit indices are developed and illustrated. (43 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Analysis of Covariance,Statistical Correlation,Statistical Significance,Statistical Tests},
  file = {/home/justin/Zotero/storage/67J8VMQF/Bentler_Bonett_1980_Significance tests and goodness of fit in the analysis of covariance structures.pdf;/home/justin/Zotero/storage/PWN8RE5A/1981-06898-001.html}
}

@article{bentler1990,
  ids = {Bentler_1990},
  title = {Comparative Fit Indexes in Structural Models.},
  author = {Bentler, Peter M.},
  year = {1990},
  journal = {Psychological Bulletin},
  volume = {107},
  number = {2},
  pages = {238--246},
  issn = {1939-1455, 0033-2909},
  doi = {10/dbj},
  langid = {english}
}

@article{bentler1994,
  title = {Gramian Matrices in Covariance Structure Models},
  author = {Bentler, Peter M. and Jamshidian, Mortaza},
  year = {1994},
  journal = {Applied Psychological Measurement},
  volume = {18},
  number = {1},
  pages = {79--94},
  doi = {10/bbpvxs},
  keywords = {matrix smooth}
}

@article{bentler2011,
  ids = {bentlerPositiveDefinitenessOffdiagonal2011},
  title = {Positive Definiteness via Off-Diagonal Scaling of a Symmetric Indefinite Matrix},
  author = {Bentler, Peter M. and Yuan, Ke-Hai},
  year = {2011},
  journal = {Psychometrika},
  volume = {76},
  number = {1},
  pages = {119--123},
  publisher = {{Springer}},
  issn = {1860-0980},
  doi = {10/dwdvc2},
  abstract = {Indefinite symmetric matrices that are estimates of positive-definite population matrices occur in a variety of contexts such as correlation matrices computed from pairwise present missing data and multinormal based methods for discretized variables. This note describes a methodology for scaling selected off-diagonal rows and columns of such a matrix to achieve positive definiteness. As a contrast to recently developed ridge procedures, the proposed method does not need variables to contain measurement errors. When minimum trace factor analysis is used to implement the theory, only correlations that are associated with Heywood cases are shrunk.},
  date-added = {2019-12-11 11:45:48 -0600},
  date-modified = {2019-12-11 11:45:48 -0600},
  langid = {english},
  keywords = {fungibleR,matrix smooth},
  file = {/home/justin/Zotero/storage/CXJ3XBKR/Bentler and Yuan - 2011 - Positive Definiteness via Off-diagonal Scaling of .pdf}
}

@article{berk2019,
  title = {Certifiably Optimal Sparse Principal Component Analysis},
  author = {Berk, Lauren and Bertsimas, Dimitris},
  year = {2019},
  month = sep,
  journal = {Mathematical Programming Computation},
  volume = {11},
  number = {3},
  pages = {381--420},
  issn = {1867-2949, 1867-2957},
  doi = {10/gjrkdd},
  langid = {english}
}

@article{bertsimas2016,
  title = {Certifiably {{Optimal Low Rank Factor Analysis}}},
  author = {Bertsimas, Dimitris and Copenhaver, Martin S. and Mazumder, Rahul},
  year = {2016},
  month = apr,
  journal = {arXiv:1604.06837 [math, stat]},
  eprint = {1604.06837},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Factor Analysis (FA) is a technique of fundamental importance that is widely used in classical and modern multivariate statistics, psychometrics and econometrics. In this paper, we revisit the classical rank-constrained FA problem, which seeks to approximate an observed covariance matrix (\$\textbackslash boldsymbol\textbackslash Sigma\$), by the sum of a Positive Semidefinite (PSD) low-rank component (\$\textbackslash boldsymbol\textbackslash Theta\$) and a diagonal matrix (\$\textbackslash boldsymbol\textbackslash Phi\$) (with nonnegative entries) subject to \$\textbackslash boldsymbol\textbackslash Sigma - \textbackslash boldsymbol\textbackslash Phi\$ being PSD. We propose a flexible family of rank-constrained, nonlinear Semidefinite Optimization based formulations for this task. We introduce a reformulation of the problem as a smooth optimization problem with convex compact constraints and propose a unified algorithmic framework, utilizing state of the art techniques in nonlinear optimization to obtain high-quality feasible solutions for our proposed formulation. At the same time, by using a variety of techniques from discrete and global optimization, we show that these solutions are certifiably optimal in many cases, even for problems with thousands of variables. Our techniques are general and make no assumption on the underlying problem data. The estimator proposed herein, aids statistical interpretability, provides computational scalability and significantly improved accuracy when compared to current, publicly available popular methods for rank-constrained FA. We demonstrate the effectiveness of our proposal on an array of synthetic and real-life datasets. To our knowledge, this is the first paper that demonstrates how a previously intractable rank-constrained optimization problem can be solved to provable optimality by coupling developments in convex analysis and in discrete optimization.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Mathematics - Optimization and Control,Statistics - Computation,Statistics - Methodology},
  file = {/home/justin/Zotero/storage/KGBS2UJM/Bertsimas et al. - 2016 - Certifiably Optimal Low Rank Factor Analysis.pdf;/home/justin/Zotero/storage/BJUK9XWS/1604.html}
}

@article{bezanson2017julia,
  title = {Julia: {{A}} Fresh Approach to Numerical Computing},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
  year = {2017},
  journal = {SIAM review},
  volume = {59},
  number = {1},
  pages = {65--98},
  publisher = {{SIAM}},
  doi = {10/f9wkpj}
}

@article{biau2016,
  title = {A Random Forest Guided Tour},
  author = {Biau, G{\'e}rard and Scornet, Erwan},
  year = {2016},
  month = jun,
  journal = {TEST},
  volume = {25},
  number = {2},
  pages = {197--227},
  issn = {1863-8260},
  doi = {10/gdqdv3},
  abstract = {The random forest algorithm, proposed by L. Breiman in 2001, has been extremely successful as a general-purpose classification and regression method. The approach, which combines several randomized decision trees and aggregates their predictions by averaging, has shown excellent performance in settings where the number of variables is much larger than the number of observations. Moreover, it is versatile enough to be applied to large-scale problems, is easily adapted to various ad hoc learning tasks, and returns measures of variable importance. The present article reviews the most recent theoretical and methodological developments for random forests. Emphasis is placed on the mathematical forces driving the algorithm, with special attention given to the selection of parameters, the resampling mechanism, and variable importance measures. This review is intended to provide non-experts easy access to the main ideas.},
  langid = {english},
  file = {/home/justin/Zotero/storage/CICKWAEK/Biau and Scornet - 2016 - A random forest guided tour.pdf}
}

@inproceedings{binns2020,
  title = {On the Apparent Conflict between Individual and Group Fairness},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Binns, Reuben},
  year = {2020},
  month = jan,
  series = {{{FAT}}* '20},
  pages = {514--524},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10/ggjpc9},
  abstract = {A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.},
  isbn = {978-1-4503-6936-7},
  keywords = {discrimination,fairness,individual fairness,justice,machine learning,statistical parity},
  file = {/home/justin/Zotero/storage/KRY34C6J/Binns - 2020 - On the apparent conflict between individual and gr.pdf}
}

@article{birnbaum1968some,
  title = {Some Latent Trait Models and Their Use in Inferring an Examinee's Ability},
  author = {Birnbaum, A Lord},
  year = {1968},
  journal = {Statistical theories of mental test scores},
  publisher = {{Addison-Wesley}},
  date-added = {2019-12-12 11:24:38 -0600},
  date-modified = {2019-12-12 11:24:38 -0600},
  keywords = {⛔ No DOI found}
}

@article{bock1975,
  title = {A Multivariate Correction for Attenuation},
  author = {Bock, R Darrell and Petersen, Anne C},
  year = {1975},
  journal = {Biometrika},
  volume = {62},
  number = {3},
  pages = {673--678},
  doi = {10/ffdgc8},
  keywords = {correction for attenuation,smooth matrix}
}

@article{bock1981,
  title = {Marginal Maximum Likelihood Estimation of Item Parameters: {{Application}} of an {{EM}} Algorithm},
  shorttitle = {Marginal Maximum Likelihood Estimation of Item Parameters},
  author = {Bock, R. Darrell and Aitkin, Murray},
  year = {1981},
  journal = {Psychometrika},
  volume = {46},
  number = {4},
  pages = {443--459},
  issn = {1860-0980},
  doi = {10/cqxqhb},
  abstract = {Maximum likelihood estimation of item parameters in the marginal distribution, integrating over the distribution of ability, becomes practical when computing procedures based on an EM algorithm are used. By characterizing the ability distribution empirically, arbitrary assumptions about its form are avoided. The Em procedure is shown to apply to general item-response models lacking simple sufficient statistics for ability. This includes models with more than one latent dimension.},
  abstractnote = {Maximum likelihood estimation of item parameters in the marginal distribution, integrating over the distribution of ability, becomes practical when computing procedures based on an EM algorithm are used. By characterizing the ability distribution empirically, arbitrary assumptions about its form are avoided. The Em procedure is shown to apply to general item-response models lacking simple sufficient statistics for ability. This includes models with more than one latent dimension.},
  bdsk-url-1 = {10.1007/BF02293801},
  langid = {english},
  file = {/home/justin/Zotero/storage/J77JXK87/Bock and Aitkin - 1981 - Marginal maximum likelihood estimation of item par.pdf}
}

@article{bock1988,
  title = {Full-Information Item Factor Analysis},
  author = {Bock, R. Darrell and Gibbons, Robert and Muraki, Eiji},
  year = {1988},
  journal = {Applied Psychological Measurement},
  volume = {12},
  number = {3},
  pages = {261--280},
  publisher = {{Sage Publications Sage CA: Thousand Oaks, CA}},
  issn = {0146-6216, 1552-3497},
  doi = {10/c2j48x},
  date-added = {2019-12-11 12:10:09 -0600},
  date-modified = {2019-12-11 12:10:21 -0600},
  langid = {english},
  file = {/home/justin/Zotero/storage/B5MN96AA/Bock et al. - 1988 - Full-Information Item Factor Analysis.pdf}
}

@article{bodner2008,
  title = {What Improves with Increased Missing Data Imputations?},
  author = {Bodner, Todd E},
  year = {2008},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {15},
  number = {4},
  pages = {651--675},
  publisher = {{Taylor \& Francis}},
  doi = {10/c9dgr4}
}

@article{boettiger2015,
  title = {An Introduction to {{Docker}} for Reproducible Research},
  author = {Boettiger, Carl},
  year = {2015},
  month = jan,
  journal = {ACM SIGOPS Operating Systems Review},
  volume = {49},
  number = {1},
  pages = {71--79},
  issn = {0163-5980},
  doi = {10/gdz6f9},
  abstract = {As computational work becomes more and more integral to many aspects of scientific research, computational reproducibility has become an issue of increasing importance to computer systems researchers and domain scientists alike. Though computational reproducibility seems more straight forward than replicating physical experiments, the complex and rapidly changing nature of computer environments makes being able to reproduce and extend such work a serious challenge. In this paper, I explore common reasons that code developed for one research project cannot be successfully executed or extended by subsequent researchers. I review current approaches to these issues, including virtual machines and workflow systems, and their limitations. I then examine how the popular emerging technology Docker combines several areas from systems research - such as operating system virtualization, cross-platform portability, modular re-usable elements, versioning, and a 'DevOps' philosophy, to address these challenges. I illustrate this with several examples of Docker use with a focus on the R statistical environment.},
  file = {/home/justin/Zotero/storage/RLPTKJGE/Boettiger_2015_An introduction to Docker for reproducible research.pdf}
}

@article{bollen1989,
  title = {A New Incremental Fit Index for General Structural Equation Models},
  author = {Bollen, K.A.},
  year = {1989},
  month = feb,
  journal = {Sociological Methods \& Research},
  volume = {17},
  number = {3},
  pages = {303--316},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10/cfgdt5},
  abstract = {Assessing overall model fit is an important problem in general structural equation models. One of the most widely used fit measures is Bentler and Bonett's (1980) normed index. This article has three purposes: (1) to propose a new incremental fit measure that provides an adjustment to the normed index for sample size and degrees of freedom, (2) to explain the relation between this new fit measure and the other ones, and (3) to illustrate its properties with an empirical example and a Monte Carlo simulation. The simulation suggests that the mean of the sampling distribution of the new fit measure stays at about one for different sample sizes whereas that for the normed fit index increases with N. In addition, the standard deviation of the new measure is relatively low compared to some other measures (e.g., Tucker and Lewis's (1973) and Bentler and Bonett's (1980) nonnormed index). The empirical example suggests that the new fit measure is relatively stable for the same model in different samples. In sum, it appears that the new incremental measure is a useful complement to the existing fit measures.},
  langid = {english},
  file = {/home/justin/Zotero/storage/X78KN5WS/BOLLEN_1989_A New Incremental Fit Index for General Structural Equation Models.pdf}
}

@book{bollen1989a,
  title = {Structural {{Equations}} with {{Latent Variables}}},
  shorttitle = {Structural {{Equations}} with {{Latent Variables}}},
  author = {Bollen, Kenneth A.},
  year = {1989},
  month = apr,
  publisher = {{John Wiley \& Sons, Inc.}},
  address = {{Hoboken, NJ, USA}},
  doi = {10.1002/9781118619179},
  isbn = {978-1-118-61917-9 978-0-471-01171-2},
  langid = {english}
}

@article{bolt2003,
  title = {Estimation of {{Compensatory}} and {{Noncompensatory Multidimensional Item Response Models Using Markov Chain Monte Carlo}}},
  author = {Bolt, Daniel M. and Lall, Venessa F.},
  year = {2003},
  month = nov,
  journal = {Applied Psychological Measurement},
  volume = {27},
  number = {6},
  pages = {395--414},
  publisher = {{SAGE Publications Inc}},
  issn = {0146-6216},
  doi = {10/dnc6rq},
  abstract = {Markov chain Monte Carlo (MCMC) estimation is investigated for multidimensional compensatory and noncompensatory item response models. Simulation analyses are used to evaluate parameter recovery for the multidimensional two-parameter logistic model (M2PL) and the multidimensional latent trait model (MLTM) under varying conditions of sample size (1,000, 3,000), number of items (25, 50), and correlation between abilities (.0, .3, and .6). Results suggest that an MCMC procedure using a Metropolis-Hastings algorithm can recover the parameters of both models but is less successful for the MLTM as the correlation between abilities increases. In general, estimation is more accurate for the M2PL than the MLTM. A Bayes factor criterion for comparing the relative .t of the models to a common data set is investigated using simulated data. Using real data, the M2PL is found to be the superior model for a test of English usage.},
  langid = {english},
  file = {/home/justin/Zotero/storage/BRPL57EH/Bolt_Lall_2003_Estimation of Compensatory and Noncompensatory Multidimensional Item Response.pdf}
}

@article{bonchi2011,
  title = {Social {{Network Analysis}} and {{Mining}} for {{Business Applications}}},
  author = {Bonchi, Francesco and Castillo, Carlos and Gionis, Aristides and Jaimes, Alejandro},
  year = {2011},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {2},
  number = {3},
  pages = {1--37},
  issn = {2157-6904, 2157-6912},
  doi = {10/gf93hh},
  abstract = {Social network analysis has gained significant attention in recent years, largely due to the success of online social networking and media-sharing sites, and the consequent availability of a wealth of social network data. In spite of the growing interest, however, there is little understanding of the potential business applications of mining social networks. While there is a large body of research on different problems and methods for social network mining, there is a gap between the techniques developed by the research community and their deployment in real-world applications. Therefore the potential business impact of these techniques is still largely unexplored.             In this article we use a business process classification framework to put the research topics in a business context and provide an overview of what we consider key problems and techniques in social network analysis and mining from the perspective of business applications. In particular, we discuss data acquisition and preparation, trust, expertise, community structure, network dynamics, and information propagation. In each case we present a brief overview of the problem, describe state-of-the art approaches, discuss business application examples, and map each of the topics to a business process classification framework. In addition, we provide insights on prospective business applications, challenges, and future research directions. The main contribution of this article is to provide a state-of-the-art overview of current techniques while providing a critical perspective on business applications of social network analysis and mining.},
  langid = {english},
  file = {/home/justin/Zotero/storage/ALWS66MX/Bonchi et al. - 2011 - Social Network Analysis and Mining for Business Ap.pdf}
}

@article{bonifay2017,
  title = {On the {{Complexity}} of {{Item Response Theory Models}}},
  author = {Bonifay, Wes and Cai, Li},
  year = {2017},
  month = jul,
  journal = {Multivariate Behavioral Research},
  volume = {52},
  number = {4},
  pages = {465--484},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/gf4mrc},
  abstract = {Complexity in item response theory (IRT) has traditionally been quantified by simply counting the number of freely estimated parameters in the model. However, complexity is also contingent upon the functional form of the model. We examined four popular IRT models\textemdash exploratory factor analytic, bifactor, DINA, and DINO\textemdash with different functional forms but the same number of free parameters. In comparison, a simpler (unidimensional 3PL) model was specified such that it had 1 more parameter than the previous models. All models were then evaluated according to the minimum description length principle. Specifically, each model was fit to 1,000 data sets that were randomly and uniformly sampled from the complete data space and then assessed using global and item-level fit and diagnostic measures. The findings revealed that the factor analytic and bifactor models possess a strong tendency to fit any possible data. The unidimensional 3PL model displayed minimal fitting propensity, despite the fact that it included an additional free parameter. The DINA and DINO models did not demonstrate a proclivity to fit any possible data, but they did fit well to distinct data patterns. Applied researchers and psychometricians should therefore consider functional form\textemdash and not goodness-of-fit alone\textemdash when selecting an IRT model.},
  pmid = {28426237},
  keywords = {Bifactor model,diagnostic classification model,item response theory,minimum description length,model evaluation},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2017.1309262},
  file = {/home/justin/Zotero/storage/YPV3A6EZ/Bonifay and Cai - 2017 - On the Complexity of Item Response Theory Models.pdf;/home/justin/Zotero/storage/S3MHQXYP/00273171.2017.html}
}

@article{boomsma2013,
  title = {Reporting {{Monte Carlo}} Studies in Structural Equation Modeling},
  author = {Boomsma, Anne},
  year = {2013},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {20},
  number = {3},
  pages = {518--540},
  issn = {1070-5511, 1532-8007},
  doi = {10/gj3d4m},
  langid = {english},
  file = {/home/justin/Zotero/storage/25B7KUCV/Boomsma_2013_Reporting Monte Carlo Studies in Structural Equation Modeling.pdf}
}

@article{borchers1999,
  title = {{{CSDP}}, {{AC}} Library for Semidefinite Programming},
  author = {Borchers, Brian},
  year = {1999},
  journal = {Optimization methods and Software},
  volume = {11},
  number = {1-4},
  pages = {613--623},
  keywords = {❓ Multiple DOI,matrix smooth}
}

@book{borgatti2018,
  title = {Analyzing {{Social Networks}}},
  author = {Borgatti, Stephen P. and Everett, Martin G. and Johnson, Jeffrey C.},
  year = {2018},
  journal = {http://journals.openedition.org/lectures},
  publisher = {{Sage}},
  abstract = {Designed to walk beginners through core aspects of collecting, visualizing, analyzing, and interpreting social network data, this book will get you up-to-speed on the theory and skills you nee...},
  copyright = {\textcopyright{} Lectures - Toute reproduction interdite sans autorisation explicite de la r\'edaction / Any replication is submitted to the authorization of the editors},
  isbn = {978-1-5264-0410-7},
  langid = {english},
  file = {/home/justin/Zotero/storage/YGT3XC47/Johnson, Jeffrey C_Everett, Martin G_Borgatti, Stephen P - Analyzing social networks-Sage Publications Ltd (2018).epub}
}

@incollection{borges1998,
  title = {On Exactitude in Science},
  booktitle = {Collected Fictions},
  author = {Borges, Jorge Luis},
  editor = {Hurley, Andrew},
  year = {1998},
  publisher = {{New York:Viking}},
  address = {{New York}},
  isbn = {0-670-84970-7},
  keywords = {1899-1986 -- Translations into English,Borges,Jorge Luis}
}

@article{borsboom2006,
  title = {The Attack of the Psychometricians},
  author = {Borsboom, Denny},
  year = {2006},
  month = sep,
  journal = {Psychometrika},
  volume = {71},
  number = {3},
  pages = {425--440},
  issn = {0033-3123, 1860-0980},
  doi = {10/bpr6h3},
  langid = {english},
  file = {/home/justin/Zotero/storage/XL5RXC4X/Borsboom_2006_The attack of the psychometricians.pdf}
}

@article{borsboom2008,
  title = {Measurement Invariance versus Selection Invariance: {{Is}} Fair Selection Possible?},
  shorttitle = {Measurement Invariance versus Selection Invariance},
  author = {Borsboom, Denny and Romeijn, Jan-Willem and Wicherts, Jelte M.},
  year = {2008},
  journal = {Psychological Methods},
  volume = {13},
  number = {2},
  pages = {75},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1463},
  doi = {10/bb8gwg},
  file = {/home/justin/Zotero/storage/GR9M5PCI/Borsboom et al. - Measurement invariance versus selection invariance.pdf;/home/justin/Zotero/storage/NHXATIIB/borsboom2008.pdf;/home/justin/Zotero/storage/8KZ55VPM/2008-06808-001.html;/home/justin/Zotero/storage/HYSLWDZ2/2008-06808-001.html}
}

@article{borsdorf2010,
  title = {Computing a Nearest Correlation Matrix with Factor Structure},
  author = {Borsdorf, R{\"u}diger and Higham, Nicholas J and Raydan, Marcos},
  year = {2010},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {31},
  number = {5},
  pages = {2603--2622},
  doi = {10/cdz8c8},
  keywords = {matrix smooth}
}

@article{borsdorf2010a,
  title = {A Preconditioned {{Newton}} Algorithm for the Nearest Correlation Matrix},
  author = {Borsdorf, R{\"u}diger and Higham, Nicholas J},
  year = {2010},
  journal = {IMA Journal of Numerical Analysis},
  volume = {30},
  number = {1},
  pages = {94--107},
  doi = {10/fd9cmc},
  keywords = {matrix smooth}
}

@phdthesis{borsdorfStructuredMatrixNearness2012,
  title = {Structured Matrix Nearness Problems: {{Theory}} and Algorithms},
  author = {Borsdorf, Ruediger},
  year = {2012}
}

@article{bouchardjr.1993,
  title = {Creativity, {{Heritability}}, {{Familiarity}}: {{Which Word Does Not Belong}}?},
  shorttitle = {Creativity, {{Heritability}}, {{Familiarity}}},
  author = {Bouchard Jr., Thomas J. and Lykken, David T. and Tellegen, Auke and Blacker, Dawn M. and Waller, Niels G.},
  year = {1993},
  month = jul,
  journal = {Psychological Inquiry},
  volume = {4},
  number = {3},
  pages = {235--237},
  issn = {1047-840X, 1532-7965},
  doi = {10/cx54fx},
  langid = {english}
}

@article{boulesteix2012,
  title = {Overview of Random Forest Methodology and Practical Guidance with Emphasis on Computational Biology and Bioinformatics},
  author = {Boulesteix, Anne-Laure and Janitza, Silke and Kruppa, Jochen and K{\"o}nig, Inke R.},
  year = {2012},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {2},
  number = {6},
  pages = {493--507},
  issn = {1942-4795},
  doi = {10/f4cc43},
  abstract = {The random forest (RF) algorithm by Leo Breiman has become a standard data analysis tool in bioinformatics. It has shown excellent performance in settings where the number of variables is much larger than the number of observations, can cope with complex interaction structures as well as highly correlated variables and return measures of variable importance. This paper synthesizes 10 years of RF development with emphasis on applications to bioinformatics and computational biology. Special attention is paid to practical aspects such as the selection of parameters, available RF implementations, and important pitfalls and biases of RF and its variable importance measures (VIMs). The paper surveys recent developments of the methodology relevant to bioinformatics as well as some representative examples of RF applications in this context and possible directions for future research. \textcopyright{} 2012 Wiley Periodicals, Inc. This article is categorized under: Algorithmic Development {$>$} Hierarchies and Trees Algorithmic Development {$>$} Statistics Application Areas {$>$} Health Care},
  copyright = {Copyright \textcopyright{} 2012 John Wiley \& Sons, Inc.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1072},
  file = {/home/justin/Zotero/storage/K3NAMLL7/Boulesteix et al. - 2012 - Overview of random forest methodology and practica.pdf;/home/justin/Zotero/storage/5BCSTJP3/widm.html}
}

@article{box1964,
  title = {An Analysis of Transformations},
  author = {Box, George EP and Cox, David R},
  year = {1964},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {26},
  number = {2},
  pages = {211--243},
  publisher = {{Wiley Online Library}},
  doi = {10/gfrhvs}
}

@article{box1979,
  title = {Some Problems of Statistics and Everyday Life},
  author = {Box, George E. P.},
  year = {1979},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {74},
  number = {365},
  pages = {1--4},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10/gjrkcp},
  file = {/home/justin/Zotero/storage/63QDFH6V/01621459.1979.html}
}

@book{box1987,
  title = {Empirical Model-Building and Response Surfaces.},
  author = {Box, George E. P. and Draper, Norman R.},
  year = {1987},
  series = {Empirical Model-Building and Response Surfaces.},
  pages = {xiv, 669},
  publisher = {{John Wiley \& Sons}},
  address = {{Oxford,  England}},
  abstract = {The object of much experimentation is to find out how a number of experimental variables, such as temperature, pressure, concentration, etc., affect a response, such as yield, and, in particular, to find experimentally the optimal combination of conditions that provides the highest response.  "Empirical Model-Building and Response Surfaces" shows how to do this efficiently. Written by two practitioners and active researchers who are in the forefront of their field, the book begins from first principles and logically develops basic ideas, finally moving to a discussion of important research findings.  Numerous exercises are included; many of these feature real data sets, and all have answers. An extensive bibliography is provided. Features of the book are: [(a)] A detailed analysis and explanation of ridge maxima. This is of great practical importance because it can show alternative sets of near-optimal conditions. [(b)] A practical discussion of the considerations which should motivate the choice of an experimental design. Included is a critique of alphabetic optimality. [(c)] A summary of important results in least squares, together with the geometry necessary to provide intuitive understanding.  The tools and techniques presented in "Empirical Model-Building and Response Surfaces" are useful to experimenters in many fields of research. Engineers, quality technologists, chemists, physicists, biologists, botanists, agricultural scientists, psychologists, as well as statisticians, will find many things of value within these pages. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {0-471-81033-9 (Hardcover)},
  keywords = {*Least Squares,*Response Parameters,*Statistical Analysis,Analysis of Variance,Estimation,Factor Structure,Models}
}

@book{boyd2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  isbn = {978-0-521-83378-3},
  langid = {english},
  lccn = {QA402.5 .B69 2004},
  keywords = {Convex functions,Mathematical optimization},
  file = {/home/justin/Zotero/storage/PHMB86E9/Boyd and Vandenberghe - 2004 - Convex optimization.pdf}
}

@article{braeken2017,
  title = {An Empirical {{Kaiser}} Criterion.},
  author = {Braeken, Johan and {van Assen}, Marcel A. L. M.},
  year = {2017},
  month = sep,
  journal = {Psychological Methods},
  volume = {22},
  number = {3},
  pages = {450--466},
  issn = {1939-1463, 1082-989X},
  doi = {10/ggdb8q},
  langid = {english},
  file = {/home/justin/Zotero/storage/EVN742YZ/Braeken and van Assen - 2017 - An empirical Kaiser criterion..pdf}
}

@article{briggs2003,
  ids = {briggsRecoveryWeakCommon2003},
  title = {Recovery of Weak Common Factors by Maximum Likelihood and Ordinary Least Squares Estimation},
  author = {Briggs, Nancy E. and MacCallum, Robert C.},
  year = {2003},
  journal = {Multivariate Behavioral Research},
  volume = {38},
  number = {1},
  pages = {25--56},
  publisher = {{Taylor \& Francis}},
  issn = {0027-3171},
  doi = {10/bgnz9w},
  abstract = {This article examines the relative performance of two commonly used methods of parameter estimation in factor analysis, maximum likelihood (ML) and ordinary least squares (OLS). It is shown that ML will sometimes fail to recover a known population factor structure when OLS succeeds. A simulation study was conducted in which two types of error (model and sampling error) were introduced separately and in combination into correlation matrices generated from known population structures with at least one relatively weak major domain factor. Simulated correlation matrices were factor analyzed using both ML and OLS, and recovery of the relatively weak factor(s) was assessed. In situations with a moderate amount of error, ML often failed to recover the weak factor while OLS succeeded. It is suggested that the correspondence between the assumptions inherent in each method regarding error and the actual nature of error in the data may affect the success of recovery of weak common factors. An example using empirical data is also presented.},
  date-added = {2019-11-20 15:15:55 -0600},
  date-modified = {2019-11-20 15:15:55 -0600},
  pmid = {26771123},
  annotation = {\_eprint: https://doi.org/10.1207/S15327906MBR3801\_2},
  file = {/home/justin/Zotero/storage/7M8QP8WF/Briggs and MacCallum - 2003 - Recovery of Weak Common Factors by Maximum Likelih.pdf;/home/justin/Zotero/storage/QU5XZLQH/S15327906MBR3801_2.html}
}

@article{brown1977,
  ids = {brown1977mean,brownMeanVarianceTetrachoric1977},
  title = {On the Mean and Variance of the Tetrachoric Correlation Coefficient},
  author = {Brown, Morton B and Benedetti, Jacqueline K},
  year = {1977},
  journal = {Psychometrika},
  volume = {42},
  number = {3},
  pages = {347--355},
  publisher = {{Springer}},
  doi = {10/b7cm3d},
  date-added = {2019-12-03 14:02:36 -0600},
  date-modified = {2019-12-03 14:02:36 -0600},
  file = {/home/justin/Zotero/storage/GMYC4J5L/Brown and Benedetti - 1977 - On the mean and variance of the tetrachoric correl.pdf}
}

@article{brown1977a,
  title = {Algorithm {{AS}} 116: {{The Tetrachoric Correlation}} and Its {{Asymptotic Standard Error}}},
  shorttitle = {Algorithm {{AS}} 116},
  author = {Brown, Morton B.},
  year = {1977},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {26},
  number = {3},
  pages = {343--351},
  publisher = {{[Wiley, Royal Statistical Society]}},
  issn = {0035-9254},
  doi = {10/bwhqmj}
}

@article{browne1968comparison,
  title = {A Comparison of Factor Analytic Techniques},
  author = {Browne, M. W.},
  year = {1968},
  journal = {Psychometrika},
  volume = {33},
  number = {3},
  pages = {267--334},
  publisher = {{Springer}},
  doi = {10/dt6rxk},
  x-fetchedfrom = {Google Scholar}
}

@article{browne1984,
  title = {Asymptotically Distribution-Free Methods for the Analysis of Covariance Structures},
  author = {Browne, M. W.},
  year = {1984},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {37},
  number = {1},
  pages = {62--83},
  issn = {2044-8317},
  doi = {10/cd4vn7},
  abstract = {Methods for obtaining tests of fit of structural models for covariance matrices and estimator standard errors which are asymptotically distribution free are derived. Modifications to standard normal theory tests and standard errors which make them applicable to the wider class of elliptical distributions are provided. A random sampling experiment to investigate some of the proposed methods is described.},
  copyright = {1984 The British Psychological Society},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1984.tb00789.x},
  file = {/home/justin/Zotero/storage/5K8JQZNT/Browne_1984_Asymptotically distribution-free methods for the analysis of covariance.pdf}
}

@article{browne1988,
  title = {Robustness of Normal Theory Methods in the Analysis of Linear Latent Variate Models},
  author = {Browne, M. W. and Shapiro, Alexander},
  year = {1988},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {41},
  number = {2},
  pages = {193--208},
  issn = {2044-8317},
  doi = {10/bdk8xk},
  abstract = {The structure of the covariance matrix of sample covariances under the class of linear latent variate models is derived using properties of cumulants. This is employed to provide a general framework for robustness of statistical inference in the analysis of covariance structures arising from linear latent variate models. Conditions for normal theory estimators and test statistics to retain each of their usual asymptotic properties under non-normality of latent variates are given. Factor analysis, LISREL and other models are discussed as examples.},
  copyright = {1988 The British Psychological Society},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1988.tb00896.x},
  file = {/home/justin/Zotero/storage/FR6WQ2MI/Browne_Shapiro_1988_Robustness of normal theory methods in the analysis of linear latent variate.pdf;/home/justin/Zotero/storage/KUBVGVD3/j.2044-8317.1988.tb00896.html}
}

@article{browne1992,
  title = {Alternative Ways of Assessing Model Fit},
  author = {Browne, M. W. and Cudeck, R.},
  year = {1992},
  month = nov,
  journal = {Sociological Methods \& Research},
  volume = {21},
  number = {2},
  pages = {230--258},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10/dbn},
  abstract = {This article is concerned with measures of fit of a model. Two types of error involved in fitting a model are considered. The first is error of approximation which involves the fit of the model, with optimally chosen but unknown parameter values, to the population covariance matrix. The second is overall error which involves the fit of the model, with parameter values estimated from the sample, to the population covariance matrix. Measures of the two types of error are proposed and point and interval estimates of the measures are suggested. These measures take the number of parameters in the model into account in order to avoid penalizing parsimonious models. Practical difficulties associated with the usual tests of exact fit or a model are discussed and a test of ``close fit'' of a model is suggested.},
  langid = {english},
  file = {/home/justin/Zotero/storage/ECVM5C5J/Browne and Cudeck - 1992 - Alternative Ways of Assessing Model Fit.pdf}
}

@article{browne2001,
  ids = {browne2001a},
  title = {An {{Overview}} of {{Analytic Rotation}} in {{Exploratory Factor Analysis}}},
  author = {Browne, Michael W.},
  year = {2001},
  month = jan,
  journal = {Multivariate Behavioral Research},
  volume = {36},
  number = {1},
  pages = {111--150},
  publisher = {{Routledge}},
  issn = {0027-3171, 1532-7906},
  doi = {10.1207/S15327906MBR3601_05},
  langid = {english},
  file = {/home/justin/Zotero/storage/5J3EEPJ3/Browne_2001_An Overview of Analytic Rotation in Exploratory Factor Analysis.pdf;/home/justin/Zotero/storage/XUU3QMI7/S15327906MBR3601_05.html}
}

@article{browne2002,
  title = {When Fit Indices and Residuals Are Incompatible},
  author = {Browne, M. W. and MacCallum, R. C. and Kim, C. T. and Andersen, B. L. and Glaser, R.},
  year = {2002},
  month = dec,
  journal = {Psychological Methods},
  volume = {7},
  number = {4},
  pages = {403--421},
  publisher = {{Amer Psychological Assoc}},
  address = {{Washington}},
  issn = {1082-989X},
  doi = {10/fhq5t3},
  abstract = {Standard chi-square-based fit indices for factor analysis and related models have a little known property: They are more sensitive to misfit when unique variances are small than when they are large. Consequently, very small correlation residuals indicating excellent fit can be accompanied by indications of bad fit by the fit indices when unique variances are small. An empirical example of this incompatibility between residuals and fit indices is provided. For illustrative purposes, an artificial example is provided that yields exactly the same correlation residuals as the empirical example but has larger unique variances. For this example, the fit indices indicate excellent fit. A theoretical explanation for this phenomenon is provided using relationships between unique variances and eigenvalues of the fitted correlation matrix.},
  langid = {english},
  keywords = {goodness-of-fit,model,stress},
  annotation = {WOS:000180256800001},
  file = {/home/justin/Zotero/storage/9BKH2VD6/Browne et al_2002_When fit indices and residuals are incompatible.pdf}
}

@article{butler1992,
  title = {Random Effects Models with Non-Parametric Priors},
  author = {Butler, Steven M and Louis, Thomas A},
  year = {1992},
  journal = {Statistics in medicine},
  volume = {11},
  number = {14-15},
  pages = {1981--2000},
  publisher = {{Wiley Online Library}},
  doi = {10/chqdzg}
}

@article{byrd1995,
  title = {A Limited Memory Algorithm for Bound Constrained Optimization},
  author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
  year = {1995},
  month = sep,
  journal = {SIAM Journal on Scientific Computing},
  volume = {16},
  number = {5},
  pages = {1190--1208},
  issn = {1064-8275, 1095-7197},
  doi = {10/bpjm24},
  langid = {english},
  file = {/home/justin/Zotero/storage/I6PNEQBV/Byrd et al_1995_A Limited Memory Algorithm for Bound Constrained Optimization.pdf}
}

@article{cai2013,
  title = {Limited-Information Goodness-of-Fit Testing of Hierarchical Item Factor Models},
  author = {Cai, Li and Hansen, Mark},
  year = {2013},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {66},
  number = {2},
  pages = {245--276},
  issn = {2044-8317},
  doi = {10/f4t9nj},
  abstract = {In applications of item response theory, assessment of model fit is a critical issue. Recently, limited-information goodness-of-fit testing has received increased attention in the psychometrics literature. In contrast to full-information test statistics such as Pearson's X2 or the likelihood ratio G2, these limited-information tests utilize lower-order marginal tables rather than the full contingency table. A notable example is Maydeu-Olivares and colleagues'M2 family of statistics based on univariate and bivariate margins. When the contingency table is sparse, tests based on M2 retain better Type I error rate control than the full-information tests and can be more powerful. While in principle the M2 statistic can be extended to test hierarchical multidimensional item factor models (e.g., bifactor and testlet models), the computation is non-trivial. To obtain M2, a researcher often has to obtain (many thousands of) marginal probabilities, derivatives, and weights. Each of these must be approximated with high-dimensional numerical integration. We propose a dimension reduction method that can take advantage of the hierarchical factor structure so that the integrals can be approximated far more efficiently. We also propose a new test statistic that can be substantially better calibrated and more powerful than the original M2 statistic when the test is long and the items are polytomous. We use simulations to demonstrate the performance of our new methods and illustrate their effectiveness with applications to real data.},
  copyright = {\textcopyright{} 2012 The British Psychological Society},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.2012.02050.x},
  file = {/home/justin/Zotero/storage/Q58KS8HQ/Cai_Hansen_2013_Limited-information goodness-of-fit testing of hierarchical item factor models.pdf;/home/justin/Zotero/storage/QD89T4L4/j.2044-8317.2012.02050.html}
}

@article{camilli1994,
  title = {Teacher's {{Corner}}: {{Origin}} of the {{Scaling Constant}} d = 1.7 in {{Item Response Theory}}},
  shorttitle = {Teacher's {{Corner}}},
  author = {Camilli, Gregory},
  year = {1994},
  month = sep,
  journal = {Journal of Educational Statistics},
  volume = {19},
  number = {3},
  pages = {293--295},
  publisher = {{American Educational Research Association}},
  issn = {0362-9791},
  doi = {10/d6ws29},
  abstract = {The scaling constant d = 1.702 used in item response theory minimizes the maximum difference between the normal and logistic distribution functions. The theoretical and numerical derivation of d given by Haley (1952) is briefly recapitulated to provide access to curious researchers, instructors, and students.},
  langid = {english},
  file = {/home/justin/Zotero/storage/Z5QD6V67/Camilli - 1994 - Teacher’s Corner Origin of the Scaling Constant d.pdf}
}

@article{camilli2013,
  title = {Ongoing Issues in Test Fairness},
  author = {Camilli, Gregory},
  year = {2013},
  month = apr,
  journal = {Educational Research and Evaluation},
  volume = {19},
  number = {2-3},
  pages = {104--120},
  issn = {1380-3611},
  doi = {10/ggfxv3},
  abstract = {In the attempt to identify or prevent unfair tests, both quantitative analyses and logical evaluation are often used. For the most part, fairness evaluation is a pragmatic attempt at determining whether procedural or substantive due process has been accorded to either a group of test takers or an individual. In both the individual and comparative approaches to test fairness, counterfactual reasoning is useful to clarify a potential charge of unfairness: Is it plausible to believe that with an alternative assessment (test or item) or under different test conditions an individual or groups of individuals may have fared better? Beyond comparative questions, fairness can also be framed by moral and ethical choices. A number of ongoing issues are evaluated with respect to these topics including accommodations, differential item functioning (DIF), differential prediction and selection, employment testing, test validation, and classroom assessment.},
  keywords = {DIF,differential prediction,employment testing,test fairness},
  file = {/home/justin/Zotero/storage/6LLBXAHI/Camilli - 2013 - Ongoing issues in test fairness.pdf}
}

@incollection{camilli2013a,
  title = {Psychometric Perspectives on Test Fairness: {{Shrinkage}} Estimation},
  shorttitle = {Psychometric Perspectives on Test Fairness},
  booktitle = {{{APA}} Handbook of Testing and Assessment in Psychology, {{Vol}}. 3: {{Testing}} and Assessment in School Psychology and Education},
  author = {Camilli, Gregory and Briggs, Derek C. and Sloane, Finbarr C. and Chiu, Ting-Wei},
  year = {2013},
  series = {{{APA}} Handbooks in Psychology\textregistered},
  pages = {571--589},
  publisher = {{American Psychological Association}},
  address = {{Washington, DC, US}},
  doi = {10.1037/14049-027},
  abstract = {Test fairness has many dimensions, some of which concern the consequences of test use, and some the validity of inferences based on test results. Still others have a more mathematical nature and can be demonstrated through proof, derivation, or simulation studies. In this latter case, conceptual and empirical results may have important implications for procedural choices in how test scores are constructed. Furthermore, they can be used to link technical considerations to decision-making processes and outcomes based on test scores. In this chapter, three psychometric issues are considered and related through the idea of shrinkage\textemdash a statistical procedure for purportedly obtaining more ``stable'' estimates of test scores. As will be argued, the term stability carries a heavy burden of assumptions. Although this chapter focuses on psychometric and related statistical issues in this paper, the topic of test fairness is considerably broader. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  isbn = {978-1-4338-1231-6},
  keywords = {Decision Making,Fairness,Inference,Psychometrics,Statistics,Test Scores,Test Validity},
  file = {/home/justin/Zotero/storage/3DKYEP84/2012-22487-027.html}
}

@book{cardAppliedMetaanalysisSocial2015,
  title = {Applied Meta-Analysis for Social Science Research},
  author = {Card, Noel A},
  year = {2015},
  publisher = {{Guilford Publications}},
  address = {{New York, NY}},
  keywords = {meta analysis,smooth matrix}
}

@misc{carroll1894,
  title = {Sylvie and {{Bruno}}, {{Volume II}}, by {{Lewis Carroll}}, Illustrated by {{Harry Furniss}}},
  author = {Carroll, Lewis},
  year = {1894},
  howpublished = {https://www.gutenberg.org/files/48795/48795-h/48795-h.htm},
  langid = {english},
  file = {/home/justin/Zotero/storage/ZAUBPX7J/48795-h.html}
}

@article{carroll1957,
  title = {Biquartimin Criterion for Rotation to Oblique Simple Structure in Factor Analysis},
  author = {Carroll, John B.},
  year = {1957},
  journal = {Science},
  volume = {126},
  pages = {1114--1115},
  publisher = {{American Assn for the Advancement of Science}},
  address = {{US}},
  issn = {1095-9203(Electronic),0036-8075(Print)},
  doi = {10/c3mhmx},
  abstract = {"This report presents the 'biquartimin' criterion for simple structure in the oblique case. When applied to several 'school problems' such as Thurstone's box problem, it yields results which appear to be closer to graphical solutions than those yielded by other analytical approaches. The complete evaluation of this and other methods awaits the development of parallel highspeed computational systems and their application to a wide variety of data\ldots{} . The principle utilized by the quartimin criterion could be applied easily to the special case where one requires orthogonality. This has not yet been done; at any rate, it would seem that the criterion of simple structure should alone determine to what extent any given set of data approaches orthogonality. Like other oblique solutions, the biquartimin criterion allows complete freedom in this respect." (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  abstractnote = {``This report presents the `biquartimin' criterion for simple structure in the oblique case. When applied to several `school problems' such as Thurstone's box problem, it yields results which appear to be closer to graphical solutions than those yielded by other analytical approaches. The complete evaluation of this and other methods awaits the development of parallel highspeed computational systems and their application to a wide variety of data\ldots{} . The principle utilized by the quartimin criterion could be applied easily to the special case where one requires orthogonality. This has not yet been done; at any rate, it would seem that the criterion of simple structure should alone determine to what extent any given set of data approaches orthogonality. Like other oblique solutions, the biquartimin criterion allows complete freedom in this respect.'' (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  place = {US},
  file = {/home/justin/Zotero/storage/VQ3RGMZM/1959-00091-001.html}
}

@article{cattell1947,
  title = {P-Technique Demonstrated in Determining Psychophysiological Source Traits in a Normal Individual},
  author = {Cattell, R. B. and Cattell, A. K. S. and Rhymer, R. M.},
  year = {1947},
  month = dec,
  journal = {Psychometrika},
  volume = {12},
  number = {4},
  pages = {267--288},
  issn = {1860-0980},
  doi = {10/dffqmk},
  abstract = {P-technique, a method employing intra-individual correlation, is tried out for the first time. As part of the general design it uses some variables the same as those in a coordinatedR-technique study and a second, parallelP-technique study with a clinical case. Definite factors are obtained among the psychological and physiological variables, which can be mutually matched. One is a fatigue factor, but the rest are general personality factors readily identifiable with those obtained in pastR-technique researches.},
  langid = {english},
  file = {/home/justin/Zotero/storage/T9AWMIUJ/Cattell et al_1947_P-technique demonstrated in determining psychophysiological source traits in a.pdf}
}

@article{censor2015,
  title = {Projection Methods: An Annotated Bibliography of Books and Reviews},
  author = {Censor, Yair and Cegielski, Andrzej},
  year = {2015},
  journal = {Optimization},
  volume = {64},
  number = {11},
  pages = {2343--2358},
  doi = {10/gjrkc2}
}

@misc{centerforhistoryandnewmediaZoteroQuickStart,
  title = {Zotero {{Quick Start Guide}}},
  author = {{Center for History and New Media}},
  howpublished = {http://zotero.org/support/quick\_start\_guide}
}

@article{chalmers2012,
  title = {{{mirt}}: {{A}} Multidimensional Item Response Theory Package for the {{R}} Environment},
  author = {Chalmers, R. Philip},
  year = {2012},
  journal = {Journal of Statistical Software},
  volume = {48},
  number = {6},
  pages = {1--29},
  doi = {10/gckfrg}
}

@article{chalmers2018,
  title = {Numerical Approximation of the Observed Information Matrix with {{Oakes}}' Identity},
  author = {Chalmers, R. Philip},
  year = {2018},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {71},
  number = {3},
  pages = {415--436},
  issn = {2044-8317},
  doi = {10/gjrkc6},
  abstract = {An efficient and accurate numerical approximation methodology useful for obtaining the observed information matrix and subsequent asymptotic covariance matrix when fitting models with the EM algorithm is presented. The numerical approximation approach is compared to existing algorithms intended for the same purpose, and the computational benefits and accuracy of this new approach are highlighted. Instructive and real-world examples are included to demonstrate the methodology concretely, properties of the estimator are discussed in detail, and a Monte Carlo simulation study is included to investigate the behaviour of a multi-parameter item response theory model using three competing finite-difference algorithms.},
  copyright = {\textcopyright{} 2018 The British Psychological Society},
  langid = {english},
  keywords = {EM algorithm,finite differences,item response theory,Oakes's identity,observed information,supplemented EM},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/bmsp.12127},
  file = {/home/justin/Zotero/storage/ST9XY9R3/Chalmers - 2018 - Numerical approximation of the observed informatio.pdf;/home/justin/Zotero/storage/ZFWLW6MZ/bmsp.html}
}

@inproceedings{chaphalkar2016,
  title = {Sample Sufficiency for Principle Component Analysis in Real Property Valuation},
  booktitle = {2016 {{SAI Computing Conference}} ({{SAI}})},
  author = {Chaphalkar, N. B. and Sandbhor, S.},
  year = {2016},
  month = jul,
  pages = {507--512},
  doi = {10/gh68tz},
  abstract = {Multivariate analysis methods are used to reduce the dimensionality of the vast data available. In this process, the size of available sample happens to be a great deal of concern. Principle Component Analysis (PCA) method has been implemented in various fields including psychology, sociology, economics, engineering and so on. Recommendations made in the literature about the sample adequacy vary and are found to depend upon the study area under consideration. Very less reported work of implementation of PCA to real property valuation has been found so far. Naturally, sample sufficiency for applying PCA to real property valuation has not been studied in detail in the available literature. With no previous assessment of sample requirement, stability of outcomes of PCA applied to real property valuation is checked based on varying sample size. Comparison with the standards mentioned in literature is also done to validate the results of sample sufficiency for application of PCA to ascertain factors affecting real property valuation.},
  keywords = {Correlation,Cost accounting,Economics,multivariate analysis methods,PCA,principal component analysis,Principal component analysis,principle component analysis,property market,Real property,real property valuation,Reliability,sample sufficiency,Sociology,Standards,valuation},
  file = {/home/justin/Zotero/storage/D4DFHHLI/Chaphalkar_Sandbhor_2016_Sample sufficiency for principle component analysis in real property valuation.pdf;/home/justin/Zotero/storage/PYPG6TR7/7556028.html}
}

@article{chen2001improper,
  title = {Improper Solutions in Structural Equation Models: {{Causes}}, Consequences, and Strategies},
  author = {Chen, Feinian and Bollen, Kenneth A and Paxton, Pamela and Curran, Patrick J and Kirby, James B},
  year = {2001},
  journal = {Sociological methods \& research},
  volume = {29},
  number = {4},
  pages = {468--508},
  publisher = {{Sage Publications}},
  doi = {10/cs43xw},
  date-added = {2020-02-07 11:30:00 -0600},
  date-modified = {2020-02-07 11:30:00 -0600}
}

@article{chen2008,
  title = {An {{Empirical Evaluation}} of the {{Use}} of {{Fixed Cutoff Points}} in {{RMSEA Test Statistic}} in {{Structural Equation Models}}},
  author = {Chen, Feinian and Curran, Patrick J. and Bollen, Kenneth A. and Kirby, James and Paxton, Pamela},
  year = {2008},
  month = jan,
  journal = {Sociological methods \& research},
  volume = {36},
  number = {4},
  pages = {462--494},
  issn = {0049-1241},
  doi = {10/bkj4jz},
  abstract = {This article is an empirical evaluation of the choice of fixed cutoff points in assessing the root mean square error of approximation (RMSEA) test statistic as a measure of goodness-of-fit in Structural Equation Models. Using simulation data, the authors first examine whether there is any empirical evidence for the use of a universal cutoff, and then compare the practice of using the point estimate of the RMSEA alone versus that of using it jointly with its related confidence interval. The results of the study demonstrate that there is little empirical support for the use of .05 or any other value as universal cutoff values to determine adequate model fit, regardless of whether the point estimate is used alone or jointly with the confidence interval. The authors' analyses suggest that to achieve a certain level of power or Type I error rate, the choice of cutoff values depends on model specifications, degrees of freedom, and sample size.},
  pmcid = {PMC2743032},
  pmid = {19756246},
  file = {/home/justin/Zotero/storage/UEIDSUHD/Chen et al. - 2008 - An Empirical Evaluation of the Use of Fixed Cutoff.pdf}
}

@article{cheung2002,
  title = {Evaluating Goodness-of-Fit Indexes for Testing Measurement Invariance},
  author = {Cheung, Gordon W. and Rensvold, Roger B.},
  year = {2002},
  month = apr,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {9},
  number = {2},
  pages = {233--255},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/fd4n2m},
  abstract = {Measurement invariance is usually tested using Multigroup Confirmatory Factor Analysis, which examines the change in the goodness-of-fit index (GFI) when cross-group constraints are imposed on a measurement model. Although many studies have examined the properties of GFI as indicators of overall model fit for single-group data, there have been none to date that examine how GFIs change when between-group constraints are added to a measurement model. The lack of a consensus about what constitutes significant GFI differences places limits on measurement invariance testing. We examine 20 GFIs based on the minimum fit function. A simulation under the two-group situation was used to examine changes in the GFIs ({$\Delta$}GFIs) when invariance constraints were added. Based on the results, we recommend using {$\Delta$}comparative fit index, {$\Delta$}Gamma hat, and {$\Delta$}McDonald's Noncentrality Index to evaluate measurement invariance. These three {$\Delta$}GFIs are independent of both model complexity and sample size, and are not correlated with the overall fit measures. We propose critical values of these {$\Delta$}GFIs that indicate measurement invariance.},
  annotation = {\_eprint: https://doi.org/10.1207/S15328007SEM0902\_5},
  file = {/home/justin/Zotero/storage/JCTWDFNU/Cheung_Rensvold_2002_Evaluating Goodness-of-Fit Indexes for Testing Measurement Invariance.pdf}
}

@article{cheung2005,
  title = {Meta-Analytic Structural Equation Modeling: A Two-Stage Approach.},
  author = {Cheung, Mike W-L and Chan, Wai},
  year = {2005},
  journal = {Psychological methods},
  volume = {10},
  number = {1},
  pages = {40},
  doi = {10/b38vh4},
  keywords = {correlation,meta analysis,smooth matrix}
}

@article{cho2009,
  title = {Accuracy of the {{Parallel Analysis Procedure With Polychoric Correlations}}},
  author = {Cho, Sun-Joo and Li, Feiming and Bandalos, Deborah},
  year = {2009},
  journal = {Educational and Psychological Measurement},
  volume = {69},
  number = {5},
  pages = {748--759},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10/d3cpb6},
  abstract = {The purpose of this study was to investigate the application of the parallel analysis (PA) method for choosing the number of factors in component analysis for situations in which data are dichotomous or ordinal. Although polychoric correlations are sometimes used as input for component analyses, the random data matrices generated for use in PA typically consist of Pearson correlations. In this study, the authors matched the type of random data matrix to the type of input matrix. Analyses were conducted on both polychoric and Pearson correlation matrices, and random matrices of the same type (polychoric or Pearson) were generated for the PA procedure. PA based on random Pearson correlations was found to perform at least as well as PA based on random polychoric correlations, for nearly all of the conditions studied.},
  langid = {english},
  file = {/home/justin/Zotero/storage/FATX3JXJ/Cho et al. - 2009 - Accuracy of the Parallel Analysis Procedure With P.pdf}
}

@article{choi2010,
  title = {Correlational Analysis of Ordinal Data: {{From Pearson}}'s r to {{Bayesian}} Polychoric Correlation},
  shorttitle = {Correlational Analysis of Ordinal Data},
  author = {Choi, Jaehwa and Peters, Michelle and Mueller, Ralph},
  year = {2010},
  month = dec,
  journal = {Asia Pacific Education Review},
  volume = {11},
  pages = {459--466},
  doi = {10/b9s9g9},
  abstract = {Correlational analyses are one of the most popular quantitative methods, yet also one of the mostly frequently misused methods in social and behavioral research, especially when analyzing ordinal data from Likert or other rating scales. Although several correlational analysis options have been developed for ordinal data, there seems to be a lack of didactically written literature illustrating the appropriate use and differences among them. The purpose of this paper is to provide a synthesis of correlational analysis options when analyzing ordinal data. These options span from the traditional methods, such as Pearson's r, to more recent developments, such as Bayesian estimation of polychoric correlations. An illustration of these methods utilizing a contemporary dataset is provided. KeywordsCorrelation coefficients-Ordinal data analysis-Polychoric correlations-Bayesian analysis},
  file = {/home/justin/Zotero/storage/MKMDIG6N/Choi et al. - 2010 - Correlational analysis of ordinal data From Pears.pdf}
}

@article{choi2011,
  ids = {choi2011a},
  title = {A Comparison of Maximum Likelihood and {{Bayesian}} Estimation for Polychoric Correlation Using {{Monte Carlo}} Simulation},
  author = {Choi, Jaehwa and Kim, Sunhee and Chen, Jinsong and Dannels, Sharon},
  year = {2011},
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {36},
  number = {4},
  pages = {523--549},
  publisher = {{[American Educational Research Association, Sage Publications, Inc., American Statistical Association]}},
  issn = {1076-9986},
  doi = {10/fcghts},
  abstract = {The purpose of this study is to compare the maximum likelihood (ML) and Bayesian estimation methods for polychoric correlation (PCC) under diverse conditions using a Monte Carlo simulation. Two new Bayesian estimates, maximum a posteriori (MAP) and expected a posteriori (EAP), are compared to ML, the classic solution, to estimate PCC. Different types of prior distributions are used to investigate the sensitivity of a prior distribution onto the Bayesian PCC estimation. In this simulation study, it appears that the MAP would be the estimator of choice for the PCC. The performance of the MAP is not only better than the ML but also appears to overcome the limitations of the EAP (i.e., the shrinkage effect).},
  abstractnote = {The purpose of this study is to compare the maximum likelihood (ML) and Bayesian estimation methods for polychoric correlation (PCC) under diverse conditions using a Monte Carlo simulation. Two new Bayesian estimates, maximum a posteriori (MAP) and expected a posteriori (EAP), are compared to ML, the classic solution, to estimate PCC. Different types of prior distributions are used to investigate the sensitivity of a prior distribution onto the Bayesian PCC estimation. In this simulation study, it appears that the MAP would be the estimator of choice for the PCC. The performance of the MAP is not only better than the ML but also appears to overcome the limitations of the EAP (i.e., the shrinkage effect).},
  file = {/home/justin/Zotero/storage/FZU4XMWL/Choi et al. - 2011 - A Comparison of Maximum Likelihood and Bayesian Es.pdf}
}

@article{chun2009,
  ids = {chun2009a},
  title = {Normal versus Noncentral Chi-Square Asymptotics of Misspecified Models},
  author = {Chun, So Yeon and Shapiro, Alexander},
  year = {2009},
  month = nov,
  journal = {Multivariate Behavioral Research},
  volume = {44},
  number = {6},
  pages = {803--827},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/cw66qh},
  abstract = {The noncentral chi-square approximation of the distribution of the likelihood ratio (LR) test statistic is a critical part of the methodology in structural equation modeling. Recently, it was argued by some authors that in certain situations normal distributions may give a better approximation of the distribution of the LR test statistic. The main goal of this article is to evaluate the validity of employing these distributions in practice. Monte Carlo simulation results indicate that the noncentral chi-square distribution describes behavior of the LR test statistic well under small, moderate, and even severe misspecifications regardless of the sample size (as long as it is sufficiently large), whereas the normal distribution, with a bias correction, gives a slightly better approximation for extremely severe misspecifications. However, neither the noncentral chi-square distribution nor the theoretical normal distributions give a reasonable approximation of the LR test statistics under extremely severe misspecifications. Of course, extremely misspecified models are not of much practical interest. We also use the Thurstone data (Thurstone \& Thurstone, 1941) from a classic study of mental ability for our illustration.},
  pmid = {26801797},
  annotation = {\_eprint: https://doi.org/10.1080/00273170903352186},
  file = {/home/justin/Zotero/storage/2YQ7K5W5/Chun_Shapiro_2009_Normal Versus Noncentral Chi-square Asymptotics of Misspecified Models.pdf;/home/justin/Zotero/storage/3WRBGTYG/Chun_Shapiro_2009_Normal Versus Noncentral Chi-square Asymptotics of Misspecified Models.pdf;/home/justin/Zotero/storage/3J9P3YQE/00273170903352186.html;/home/justin/Zotero/storage/6FIM8TUR/00273170903352186.html}
}

@article{chun2010,
  title = {Construction of Covariance Matrices with a Specified Discrepancy Function Minimizer, with Application to Factor Analysis},
  author = {Chun, So Yeon and Shapiro, Alexander},
  year = {2010},
  month = jan,
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {31},
  number = {4},
  pages = {1570--1583},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0895-4798},
  doi = {10/cs83nq},
  abstract = {The main goal of this paper is to develop a numerical procedure for construction of covariance matrices such that for a given covariance structural model and a discrepancy function the corresponding minimizer of the discrepancy function has a specified value. Often construction of such matrices is a first step in Monte Carlo studies of statistical inferences of misspecified models. We analyze theoretical aspects of the problem and suggest a numerical procedure based on semidefinite programming techniques. As an example, we discuss in detail the factor analysis model.},
  file = {/home/justin/Zotero/storage/7X862NL8/Chun_Shapiro_2010_Construction of Covariance Matrices with a Specified Discrepancy Function.pdf;/home/justin/Zotero/storage/ULJAHTRP/080735515.html}
}

@article{chung2019,
  title = {Alternative Multiple Imputation Inference for Categorical Structural Equation Modeling},
  author = {Chung, Seungwon and Cai, Li},
  year = {2019},
  month = may,
  journal = {Multivariate Behavioral Research},
  volume = {54},
  number = {3},
  pages = {323--337},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/gh64rk},
  abstract = {The use of item responses from questionnaire data is ubiquitous in social science research. One side effect of using such data is that researchers must often account for item level missingness. Multiple imputation is one of the most widely used missing data handling techniques. The traditional multiple imputation approach in structural equation modeling has a number of limitations. Motivated by Lee and Cai's approach, we propose an alternative method for conducting statistical inference from multiple imputation in categorical structural equation modeling. We examine the performance of our proposed method via a simulation study and illustrate it with one empirical data set.},
  pmid = {30950634},
  keywords = {Categorical variables,goodness-of-fit test,multiple imputation,structural equation modeling},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2018.1523000},
  file = {/home/justin/Zotero/storage/G2TUXQZ8/Chung_Cai_2019_Alternative Multiple Imputation Inference for Categorical Structural Equation.pdf;/home/justin/Zotero/storage/99R7XMVY/00273171.2018.html}
}

@article{ciccone2018,
  title = {An Alternating Minimization Algorithm for {{Factor Analysis}}},
  author = {Ciccone, Valentina and Ferrante, Augusto and Zorzi, Mattia},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.04433 [math]},
  eprint = {1806.04433},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {The problem of decomposing a given covariance matrix as the sum of a positive semi-definite matrix of given rank and a positive semi-definite diagonal matrix, is considered. We present a projection-type algorithm to address this problem. This algorithm appears to perform extremely well and is extremely fast even when the given covariance matrix has a very large dimension. The effectiveness of the algorithm is assessed through simulation studies and by applications to three real datasets that are considered as benchmark for the problem. A local convergence analysis of the algorithm is also presented.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Mathematics - Optimization and Control},
  file = {/home/justin/Zotero/storage/FY3ZVQ8C/Ciccone et al. - 2018 - An alternating minimization algorithm for Factor A.pdf;/home/justin/Zotero/storage/KTUE9IWW/1806.html}
}

@article{ciesla2007,
  ids = {ciesla2007a},
  title = {Extending the Trait\textendash State\textendash Occasion Model: {{How}} Important Is within-Wave Measurement Equivalence?},
  shorttitle = {Extending the Trait\textendash State\textendash Occasion Model},
  author = {Ciesla, Jeffrey A. and Cole, David A. and Steiger, James H.},
  year = {2007},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {14},
  number = {1},
  pages = {77--97},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/dkwfjm},
  abstract = {Trait\textendash State\textendash Occasion (TSO) covariance models represent an important advance in methods for studying the longitudinal stability of latent constructs. Such models have only been examined under fairly restricted conditions (e.g., having only 2 tau-equivalent indicators per wave). In this study, Monte Carlo simulations revealed the effects of having 2 versus 3 indicators per wave and relaxing the within-wave tau equivalence. These modifications were examined under conditions that varied with regard to the within-wave trait variance and the cross-wave stability of occasion influences. In general, the TSO model performed well (i.e., few convergence problems or out-of-range parameter estimates) under most of these conditions; however, the likelihood of improper solutions increased when only 2 indicators were used per wave, when factor loadings were small, when the proportion of trait variance was either very high or very low, and when the occasion factor was highly stable. Based on these findings, recommendations are made for successful use of TSO models with real data.},
  annotation = {\_eprint: https://doi.org/10.1080/10705510709336737},
  file = {/home/justin/Zotero/storage/RQ263KIN/Ciesla et al_2007_Extending the Trait–State–Occasion Model.pdf;/home/justin/Zotero/storage/TBLQ9HCQ/Ciesla et al_2007_Extending the Trait–State–Occasion Model.pdf;/home/justin/Zotero/storage/X9LTFE8R/10705510709336737.html}
}

@article{clarkson1988,
  title = {Quartic Rotation Criteria and Algorithms},
  author = {Clarkson, Douglas B. and Jennrich, Robert I.},
  year = {1988},
  month = jun,
  journal = {Psychometrika},
  volume = {53},
  number = {2},
  pages = {251--259},
  issn = {1860-0980},
  doi = {10/bzkbmg},
  abstract = {Most of the currently used analytic rotation criteria for simple structure in factor analysis are summarized and identified as members of a general symmetric family of quartic criteria. A unified development of algorithms for orthogonal and direct oblique rotation using arbitrary criteria from this family is given. These algorithms represent fairly straightforward extensions of present methodology, and appear to be the best methods currently available.},
  langid = {english},
  file = {/home/justin/Zotero/storage/KUKU5AUV/Clarkson and Jennrich - 1988 - Quartic rotation criteria and algorithms.pdf}
}

@article{coenders1997,
  title = {Alternative Approaches to Structural Modeling of Ordinal Data: {{A Monte Carlo}} Study},
  shorttitle = {Alternative Approaches to Structural Modeling of Ordinal Data},
  author = {Coenders, Germ{\`a} and Satorra, Albert and Saris, Willem E.},
  year = {1997},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {4},
  number = {4},
  pages = {261--282},
  issn = {1070-5511, 1532-8007},
  doi = {10/dqk6bb},
  langid = {english},
  file = {/home/justin/Zotero/storage/D82AU3JB/Coenders et al_1997_Alternative approaches to structural modeling of ordinal data.pdf}
}

@phdthesis{coffman2005,
  ids = {coffman2005a},
  title = {Consequences of Violating the Parameter Drift Assumption in Covariance Structure Models},
  author = {Coffman, Donna L.},
  year = {2005},
  address = {{Chapel Hill, N.C.}},
  abstract = {This dissertation involves an examination of the parameter drift assumption underlying the testing and evaluation of model fit in covariance structure models (CSMs). The assumption states that as sample size increases the degree to which the model is incorrect in the population (model error) decreases. This assumption is never valid in practice because the degree to which the model is wrong in the population will not change by increasing the sample size. Nearly all previous research on model misspecification has generated model error of a parametric nature by deleting or adding paths or latent variables. In contrast, I used a technique to generate a covariance matrix with a specified discrepancy function value in the population. This model misspecification does not have any particular parametric structure. Using simulation methods, I varied the degree of misfit, sample size; and model structure and examined how closely the empirical distribution of the test statistic followed a noncentral chi-square distribution, the degree of bias of the root mean square error of approximation (RMSEA), the relative noncentrality index (RNI), and the comparative fit index (CFI) point estimates, the coverage proportions of the RMSEA confidence intervals (CIs), the Type I error and power of the test of close fit, and power of the test of exact fit under violations of the assumption. Across the conditions studied, the mean and variance of the empirical distribution did not differ significantly from the mean and variance of the theoretical noncentral chi-square distribution. The CFI and RNI were unbiased in nearly every condition. The RMSEA point estimates underestimated the population RMSEA in most conditions. However, the coverage proportions for the RMSEA CIs maintained the coverage rate well. The Type I error rate for the test of close fit maintained the nominal level. The power for the test of close fit matched the theoretical power well. Thus, the results indicate robustness to violations of the parameter drift assumption for the conditions examined here. These results were unexpected based on previous research. It is speculated that the method for introducing model error is the reason for the unexpected results.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9780542463648},
  langid = {english},
  school = {The University of North Carolina},
  keywords = {Covariance structure models,Model evaluation,Parameter drift assumption,Psychology,Pure sciences},
  file = {/home/justin/Zotero/storage/FLPKG4UX/Coffman_2005_Consequences of violating the parameter drift assumption in covariance.pdf;/home/justin/Zotero/storage/IZUHG4SS/Coffman_2005_Consequences of violating the parameter drift assumption in covariance.pdf}
}

@article{coffman2008,
  title = {Model Error in Covariance Structure Models: {{Some}} Implications for Power and {{Type I}} Error.},
  shorttitle = {Model Error in Covariance Structure Models},
  author = {Coffman, Donna L.},
  year = {2008},
  journal = {Methodology: European Journal of Research Methods for the Behavioral and Social Sciences},
  volume = {4},
  number = {4},
  pages = {159},
  publisher = {{Germany: Hogrefe \& Huber Publishers}},
  issn = {1614-2241},
  doi = {10/dc6db2},
  file = {/home/justin/Zotero/storage/3EQWTA4Q/Coffman_2008_Model error in covariance structure models.pdf;/home/justin/Zotero/storage/CNXQQRZ3/2008-16415-003.html}
}

@article{cole2005,
  title = {Empirical and Conceptual Problems with Longitudinal Trait-State Models: {{Introducing}} a Trait-State-Occasion Model.},
  shorttitle = {Empirical and Conceptual Problems with Longitudinal Trait-State Models},
  author = {Cole, David A. and Martin, Nina C. and Steiger, James H.},
  year = {2005},
  month = mar,
  journal = {Psychological Methods},
  volume = {10},
  number = {1},
  pages = {3--20},
  issn = {1939-1463, 1082-989X},
  doi = {10/fp3k2r},
  langid = {english},
  file = {/home/justin/Zotero/storage/6S4L8A6J/Empirical and Conceptual Problems With Longitudinal Trait-State Models.pdf}
}

@article{cole2014,
  title = {Manifest Variable Path Analysis: {{Potentially}} Serious and Misleading Consequences Due to Uncorrected Measurement Error},
  shorttitle = {Manifest Variable Path Analysis},
  author = {Cole, David A. and Preacher, Kristopher J.},
  year = {2014},
  journal = {Psychological Methods},
  volume = {19},
  number = {2},
  pages = {300--315},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10/gcz6vx},
  abstract = {[Correction Notice: An Erratum for this article was reported in Vol 19(2) of Psychological Methods (see record 2014-22989-001). Footnote 2 should have stated, ``Throughout, we assume that all path coefficients are non-negative. In this context, the words attenuation and underestimation refer to the shrinkage toward zero of path coefficient estimates due to measurement error. The words inflation and overestimation refer to the expansion away from zero of path coefficient estimates.'' Footnote 4 should have stated, ``We make several assumptions in our path diagrams . . .'' The first sentence in the first full paragraph on page 5 should have stated, ``Fourth, the overestimation of any directional path is due to the underestimation of the sum of all valid tracings responsible for the covariation between the two variables of interest (other than the tracing that consists of the target path). All versions have been corrected.] Despite clear evidence that manifest variable path analysis requires highly reliable measures, path analyses with fallible measures are commonplace even in premier journals. Using fallible measures in path analysis can cause several serious problems: (a) As measurement error pervades a given data set, many path coefficients may be either over- or underestimated. (b) Extensive measurement error diminishes power and can prevent invalid models from being rejected. (c) Even a little measurement error can cause valid models to appear invalid. (d) Differential measurement error in various parts of a model can change the substantive conclusions that derive from path analysis. (e) All of these problems become increasingly serious and intractable as models become more complex. Methods to prevent and correct these problems are reviewed. The conclusion is that researchers should use more reliable measures (or correct for measurement error in the measures they do use), obtain multiple measures for use in latent variable modeling, and test simpler models containing fewer variables. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Consequence,Goodness of Fit,Measurement,Path Analysis},
  file = {/home/justin/Zotero/storage/J9BBD66E/Cole_Preacher_2014_Manifest variable path analysis.pdf;/home/justin/Zotero/storage/JCNSLRGC/2013-34332-001.html}
}

@article{collins1986,
  ids = {collins1986a},
  title = {Factor {{Recovery}} in {{Binary Data Sets}}: {{A Simulation}}},
  shorttitle = {Factor {{Recovery}} in {{Binary Data Sets}}},
  author = {Collins, Linda M. and Cliff, Norman and McCormick, Douglas J. and Zatkin, Judith L.},
  year = {1986},
  journal = {Multivariate Behavioral Research},
  volume = {21},
  number = {3},
  pages = {377--391},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/dtb4n7},
  abstract = {The present study compares the performance of phi coefficients and tetrachorics along two dimensions of factor recovery in binary data. These dimensions are (a) accuracy of nontrivial factor identification, and (b) factor structure recovery given a priori knowledge of the correct number of factors to rotate. Nontrivial factor identification was poor for both indices, with phi's performing slightly better than tetrachorics. In contrast, factor structure recovery was quite good when the correct number of factors was rotated. Phi coefficients generally yielded better factor structure recovery than tetrachorics and were better at preventing items from intruding onto factors where they did not belong, while tetrachorics were better than phi's at preventing items from being omitted from factors where they should have been included. The solutions based on tetrachorics contained many Heywood cases. It is suggested that for most applications it is preferable to base factor analysis on phi coefficients.},
  pmid = {26800967},
  annotation = {\_eprint: https://doi.org/10.1207/s15327906mbr2103\_6},
  file = {/home/justin/Zotero/storage/CEGWXHQY/Collins et al_1986_Factor Recovery in Binary Data Sets.pdf;/home/justin/Zotero/storage/J2QCTP7L/Collins et al. - 1986 - Factor Recovery in Binary Data Sets A Simulation.pdf;/home/justin/Zotero/storage/XTTU7CWN/Collins et al_1986_Factor Recovery in Binary Data Sets.pdf;/home/justin/Zotero/storage/HLAG3AJM/s15327906mbr2103_6.html;/home/justin/Zotero/storage/HWAKUTPG/s15327906mbr2103_6.html}
}

@article{comrey1958,
  title = {A Comparison of Three Point Coefficients in Factor Analyses of {{MMPI}} Items},
  author = {Comrey, Andrew L. and Levonian, Edward},
  year = {1958},
  journal = {Educational and Psychological Measurement},
  volume = {18},
  pages = {739--755},
  publisher = {{Sage Publications}},
  address = {{US}},
  issn = {1552-3888(Electronic),0013-1644(Print)},
  doi = {10/cmmdv7},
  abstract = {"To test the effect of the correlation coefficient on factor structure, factor analyses of\ldots{} 55 MMPI items were carried out entirely independently of one another using phi coefficients, phi-over-phi-max coefficients, and tetrachoric coefficients as estimated by the cosine-pi formula." The results indicated that different analyses do not yield identical factors despite certain similarity upon visual inspection. The phi coefficient is considered to be satisfactory for factor analytic work being relatively free of excessively high communalities. 17 refs. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/home/justin/Zotero/storage/F293XTGT/Comrey_Levonian_1958_A comparison of three point coefficients in factor analyses of MMPI items.pdf;/home/justin/Zotero/storage/5GQXHKZ4/1960-00107-001.html}
}

@article{comrey1962minimum,
  title = {The Minimum Residual Method of Factor Analysis},
  author = {Comrey, Andrew L.},
  year = {1962},
  journal = {Psychological Reports},
  volume = {11},
  number = {1},
  pages = {15--18},
  publisher = {{SAGE Publications Sage CA: Los Angeles, CA}},
  date-added = {2020-02-19 15:45:34 -0600},
  date-modified = {2020-02-19 15:45:34 -0600},
  keywords = {❓ Multiple DOI}
}

@article{cooperman2021,
  title = {Heywood You Go Away! {{Examining}} Causes, Effects, and Treatments for {{Heywood}} Cases in Exploratory Factor Analysis},
  author = {Cooperman, Allison W. and Waller, Niels G.},
  year = {2021},
  journal = {Psychological Methods},
  doi = {10.1037/met0000384},
  file = {/home/justin/Zotero/storage/LXCVSH2Q/Cooperman_Waller_2021_Heywood you go away.pdf}
}

@article{criminisi2012,
  title = {Decision {{Forests}}: {{A Unified Framework}} for {{Classification}}, {{Regression}}, {{Density Estimation}}, {{Manifold Learning}} and {{Semi-Supervised Learning}}},
  shorttitle = {Decision {{Forests}}},
  author = {Criminisi, Antonio and Shotton, Jamie and Konukoglu, Ender},
  year = {2012},
  month = mar,
  journal = {Foundations and Trends\textregistered{} in Computer Graphics and Vision},
  volume = {7},
  number = {2\textendash 3},
  pages = {81--227},
  publisher = {{Now Publishers, Inc.}},
  issn = {1572-2740, 1572-2759},
  doi = {10/gcpgfd},
  abstract = {Decision Forests: A Unified Framework for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning},
  langid = {english},
  file = {/home/justin/Zotero/storage/MPMJVW8E/Criminisi et al. - 2012 - Decision Forests A Unified Framework for Classifi.pdf;/home/justin/Zotero/storage/4EBID462/CGV-035.html}
}

@book{CrockerAlgina,
  title = {Introduction to Classical and Modern Test Theory.},
  author = {Crocker, Linda and Algina, James},
  year = {1986},
  publisher = {{ERIC}},
  date-added = {2019-11-21 13:30:18 -0600},
  date-modified = {2019-11-21 13:30:18 -0600}
}

@article{cross2011,
  title = {The Collaborative Organization: {{How}} to Make Employee Networks Really Work},
  shorttitle = {The Collaborative Organization},
  author = {Cross, Rob and Gray, Peter and Cunningham, Shirley and Showers, Mark and Thomas, Robert},
  year = {2011},
  journal = {IEEE Engineering Management Review},
  volume = {39},
  number = {1},
  pages = {59--68},
  issn = {0360-8581},
  doi = {10/cjfnzr},
  langid = {english},
  file = {/home/justin/Zotero/storage/GCP7GT2J/Cross et al. - 2011 - The collaborative organization how to make employ.pdf}
}

@article{cross2018,
  title = {Connect and Adapt},
  author = {Cross, Rob and Opie, Tina and Pryor, Greg and Rollag, Keith},
  year = {2018},
  journal = {Organizational Dynamics},
  volume = {47},
  number = {2},
  pages = {115--123},
  issn = {00902616},
  doi = {10/gj735c},
  langid = {english},
  file = {/home/justin/Zotero/storage/NDSAJANN/Cross et al_2018_Connect and adapt.pdf}
}

@misc{csirosdata61,
  title = {Node Classification with {{Graph Convolutional Network}} ({{GCN}}) \textemdash{} {{StellarGraph}} 1.2.1 Documentation},
  author = {CSIRO's Data61},
  howpublished = {https://stellargraph.readthedocs.io/en/stable/demos/node-classification/gcn-node-classification.html},
  file = {/home/justin/Zotero/storage/ZKP5TQWF/gcn-node-classification.html}
}

@article{cudeck1989,
  ids = {cudeck1989a},
  title = {Analysis of Correlation Matrices Using Covariance Structure Models},
  author = {Cudeck, Robert},
  year = {1989},
  journal = {Psychological Bulletin},
  volume = {105},
  number = {2},
  pages = {317--327},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10/fqks6n},
  abstract = {It is often assumed that covariance structure models can be arbitrarily applied to sample correlation matrices as readily as to sample covariance matrices. Although this is true in many cases and leads to an analysis that is mostly correct, it is not permissible for all structures. This article reviews three interrelated problems associated with the analysis of structural models using a matrix of sample correlations. Depending upon the model, applying a covariance structure to a matrix of correlations may (a) modify the model being studied, (b) produce incorrect values of the omnibus test statistic, or (c) yield incorrect standard errors. An important class of models are those that are scale invariant (Browne, 1982), for then Errors a and b cannot occur when a correlation matrix is analyzed. A number of examples based on restricted factor analysis are presented to illustrate the concepts described in the article. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Mathematical Modeling,Statistical Analysis,Statistical Correlation},
  file = {/home/justin/Zotero/storage/5CBAHDVJ/1989-21154-001.html;/home/justin/Zotero/storage/T2AHP3Z6/1989-21154-001.html}
}

@article{cudeck1991,
  title = {Model Selection in Covariance Structures Analysis and the ``Problem'' of Sample Size: {{A}} Clarification.},
  author = {Cudeck, Robert and Henly, Susan J},
  year = {1991},
  journal = {Psychological bulletin},
  volume = {109},
  number = {3},
  pages = {512},
  publisher = {{American Psychological Association}},
  doi = {10/bpqvdv},
  date-added = {2020-01-07 12:49:07 -0600},
  date-modified = {2020-01-07 12:49:07 -0600},
  file = {/home/justin/Zotero/storage/WDJ7H57S/Cudeck_Henly_1991_Model selection in covariance structures analysis and the “problem” of sample.pdf}
}

@article{cudeck1992,
  title = {Constructing a Covariance Matrix That Yields a Specified Minimizer and a Specified Minimum Discrepancy Function Value},
  author = {Cudeck, Robert and Browne, Michael W.},
  year = {1992},
  month = sep,
  journal = {Psychometrika},
  volume = {57},
  number = {3},
  pages = {357--369},
  issn = {1860-0980},
  doi = {10/cq6ckd},
  abstract = {A method is presented for constructing a covariance matrix {$\Sigma$}*0 that is the sum of a matrix {$\Sigma$}({$\gamma$}0) that satisfies a specified model and a perturbation matrix,E, such that {$\Sigma$}*0={$\Sigma$}({$\gamma$}0) +E. The perturbation matrix is chosen in such a manner that a class of discrepancy functionsF({$\Sigma$}*0, {$\Sigma$}({$\gamma$}0)), which includes normal theory maximum likelihood as a special case, has the prespecified parameter value {$\gamma$}0 as minimizer and a prespecified minimum {$\delta$} A matrix constructed in this way seems particularly valuable for Monte Carlo experiments as the covariance matrix for a population in which the model does not hold exactly. This may be a more realistic conceptualization in many instances. An example is presented in which this procedure is employed to generate a covariance matrix among nonnormal, ordered categorical variables which is then used to study the performance of a factor analysis estimator.},
  langid = {english},
  file = {/home/justin/Zotero/storage/FJXG86RJ/Cudeck_Browne_1992_Constructing a covariance matrix that yields a specified minimizer and a.pdf}
}

@article{culpepper2009,
  title = {Assessing Differential Prediction of College Grades by Race/Ethnicity with a Multilevel Model},
  author = {Culpepper, Steven A. and Davenport, Ernest C.},
  year = {2009},
  journal = {Journal of Educational Measurement},
  volume = {46},
  number = {2},
  pages = {220--242},
  issn = {1745-3984},
  doi = {10/cpxkc4},
  abstract = {Previous research notes the importance of understanding racial/ethnic differential prediction of college grades across multiple institutions. Institutional variation in selection indices is especially important given some states' laws governing public institutions' admissions decisions. This paper employed multilevel moderated multiple regression to study the variation of selection indices across 30 institutions and the accuracy of selection indices in predicting college grades for students of different racial/ethnic backgrounds. Several benefits of multilevel models for cross-institutional differential prediction studies were described and include: controlling for institutional differences in range restriction, providing reliability estimates of least squares estimates, and adjusting criterion scores for differences in coursework difficulty. The findings from this study provide evidence of institutional variation in selection indices, which challenges current laws aimed at standardizing them. Specifically, there was evidence that the predictor slope coefficients varied across institutions, in addition to the estimates that measured intercept differences for African and Asian American students. Across universities, the results mirrored previous findings: high school grade point average (GPA) differentially predicted grades for African Americans, SAT verbal scores differentially predict grades for Asian Americans, and SAT math scores were better predictors of Asian Americans' grades.},
  copyright = {\textcopyright{} 2009 by the National Council on Measurement in Education},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1745-3984.2009.00079.x},
  file = {/home/justin/Zotero/storage/2HKKZFHL/Culpepper and Davenport - 2009 - Assessing Differential Prediction of College Grade.pdf;/home/justin/Zotero/storage/EIFH9R4K/j.1745-3984.2009.00079.html}
}

@article{culpepper2012,
  title = {Using the Criterion-Predictor Factor Model to Compute the Probability of Detecting Prediction Bias with Ordinary Least Squares Regression},
  author = {Culpepper, Steven Andrew},
  year = {2012},
  month = jul,
  journal = {Psychometrika},
  volume = {77},
  number = {3},
  pages = {561--580},
  issn = {1860-0980},
  doi = {10/f323p7},
  abstract = {The study of prediction bias is important and the last five decades include research studies that examined whether test scores differentially predict academic or employment performance. Previous studies used ordinary least squares (OLS) to assess whether groups differ in intercepts and slopes. This study shows that OLS yields inaccurate inferences for prediction bias hypotheses. This paper builds upon the criterion-predictor factor model by demonstrating the effect of selection, measurement error, and measurement bias on prediction bias studies that use OLS. The range restricted, criterion-predictor factor model is used to compute Type I error and power rates associated with using regression to assess prediction bias hypotheses. In short, OLS is not capable of testing hypotheses about group differences in latent intercepts and slopes. Additionally, a theorem is presented which shows that researchers should not employ hierarchical regression to assess intercept differences with selected samples.},
  langid = {english},
  file = {/home/justin/Zotero/storage/M2RZESQE/Culpepper - 2012 - Using the Criterion-Predictor Factor Model to Comp.pdf}
}

@article{culpepper2012a,
  title = {Evaluating {{EIV}}, {{OLS}}, and {{SEM}} Estimators of Group Slope Differences in the Presence of Measurement Error: The Single-Indicator Case},
  shorttitle = {Evaluating Eiv, Ols, and Sem Estimators of Group Slope Differences in the Presence of Measurement Error},
  author = {Culpepper, Steven Andrew},
  year = {2012},
  month = jul,
  journal = {Applied Psychological Measurement},
  volume = {36},
  number = {5},
  pages = {349--374},
  publisher = {{SAGE Publications Inc}},
  issn = {0146-6216},
  doi = {10/gckfsc},
  abstract = {Measurement error significantly biases interaction effects and distorts researchers' inferences regarding interactive hypotheses. This article focuses on the single-indicator case and shows how to accurately estimate group slope differences by disattenuating interaction effects with errors-in-variables (EIV) regression. New analytic findings were presented along with simulation results to compare the relative bias, power, and Type I error rates of EIV, ordinary least squares (OLS), and sparse (i.e., single indicator) multigroup structural equation model (SEM) estimators of interaction effects in the presence of measurement error. The results suggest that EIV was less biased than were OLS and sparse SEM. Furthermore, OLS and sparse SEM were unable to control the Type I error rate for tests of slope differences in circumstances where groups differ in predictor reliability. Additional derivations examined the impact of using Cronbach's alpha, which is typically a lower bound for reliability, with EIV. The results provided evidence that using alpha does result in overcorrected EIV estimates and the bias in EIV estimates associated with using Cronbach's alpha increases with variability in item loadings and bias decreases as either test length or the average loading increases. The bias in EIV estimates when using alpha is not larger than the bias produced by using OLS or sparse SEM. In summary, the results provide compelling evidence that researchers should use EIV instead of OLS and sparse SEM to estimate group slope differences in the presence of measurement error.},
  langid = {english},
  file = {/home/justin/Zotero/storage/K55AL8PG/Culpepper - 2012 - Evaluating EIV, OLS, and SEM Estimators of Group S.pdf}
}

@article{culpepper2019,
  ids = {culpepper2019a},
  title = {High-Stakes Testing Case Study: A Latent Variable Approach for Assessing Measurement and Prediction Invariance},
  shorttitle = {High-Stakes Testing Case Study},
  author = {Culpepper, Steven Andrew and Aguinis, Herman and Kern, Justin L. and Millsap, Roger},
  year = {2019},
  month = mar,
  journal = {Psychometrika},
  volume = {84},
  number = {1},
  pages = {285--309},
  issn = {1860-0980},
  doi = {10/gfzrp5},
  abstract = {The existence of differences in prediction systems involving test scores across demographic groups continues to be a thorny and unresolved scientific, professional, and societal concern. Our case study uses a two-stage least squares (2SLS) estimator to jointly assess measurement invariance and prediction invariance in high-stakes testing. So, we examined differences across groups based on latent as opposed to observed scores with data for 176 colleges and universities from The College Board. Results showed that evidence regarding measurement invariance was rejected for the SAT mathematics (SAT-M) subtest at the 0.01 level for 74.5\% and 29.9\% of cohorts for Black versus White and Hispanic versus White comparisons, respectively. Also, on average, Black students with the same standing on a common factor had observed SAT-M scores that were nearly a third of a standard deviation lower than for comparable Whites. We also found evidence that group differences in SAT-M measurement intercepts may partly explain the well-known finding of observed differences in prediction intercepts. Additionally, results provided evidence that nearly a quarter of the statistically significant observed intercept differences were not statistically significant at the 0.05 level once predictor measurement error was accounted for using the 2SLS procedure. Our joint measurement and prediction invariance approach based on latent scores opens the door to a new high-stakes testing research agenda whose goal is to not simply assess whether observed group-based differences exist and the size and direction of such differences. Rather, the goal of this research agenda is to assess the causal chain starting with underlying theoretical mechanisms (e.g., contextual factors, differences in latent predictor scores) that affect the size and direction of any observed differences.},
  langid = {english},
  file = {/home/justin/Zotero/storage/XBUDVBKN/Culpepper et al. - 2019 - High-Stakes Testing Case Study A Latent Variable .pdf}
}

@article{cureton1975,
  title = {The Weighted Varimax Rotation and the Promax Rotation},
  author = {Cureton, Edward E. and Mulaik, Stanley A.},
  year = {1975},
  month = jun,
  journal = {Psychometrika},
  volume = {40},
  number = {2},
  pages = {183--195},
  issn = {1860-0980},
  doi = {10/dq6wn2},
  abstract = {Kaiser's iterative algorithm for the varimax rotation fails when (a) there is a substantial cluster of test vectors near the middle of each bounding hyperplane, leading to non-bounding hyperplanes more heavily overdetermined than those at the boundaries of the configuration of test vectors, and/or (b) there are appreciably more thanm (m factors) tests whose loadings on one of the factors of the initialF-matrix, usually the first, are near-zero, leading to overdetermination of the hyperplane orthogonal to this initialF-axis before rotation. These difficulties are overcome by weighting the test vectors, giving maximum weights to those likely to be near the primary axes, intermediate weights to those likely to be near hyperplanes but not near primary axes, and near-zero weights to those almost collinear with or almost orthogonal to the first initialF-axis. Applications to the Promax rotation are discussed, and it is shown that these procedures solve Thurstone's hitherto intractable ``invariant'' box problem as well as other more common problems based on real data.},
  langid = {english},
  file = {/home/justin/Zotero/storage/3XPNZ9DQ/Cureton and Mulaik - 1975 - The weighted varimax rotation and the promax rotat.pdf}
}

@article{dahlke2017,
  title = {The Relationship between Cognitive-Ability Saturation and Subgroup Mean Differences across Predictors of Job Performance.},
  author = {Dahlke, Jeffrey A. and Sackett, Paul R.},
  year = {2017},
  month = oct,
  journal = {Journal of Applied Psychology},
  volume = {102},
  number = {10},
  pages = {1403--1420},
  issn = {1939-1854, 0021-9010},
  doi = {10/gb3cg5},
  langid = {english},
  file = {/home/justin/Zotero/storage/JZV5BSY9/Dahlke and Sackett - 2017 - The relationship between cognitive-ability saturat.pdf}
}

@book{daley1999,
  title = {Epidemic {{Modelling}} : {{An Introduction}}},
  author = {Daley, D. J.},
  editor = {Gani, J.},
  year = {1999},
  publisher = {{Cambridge : Cambridge University Press}},
  address = {{Cambridge}},
  abstract = {This general introduction to the ideas and techniques required for the mathematical modelling of diseases begins with an outline of some disease statistics dating from Daniel Bernoulli's 1760 smallpox data. The authors then describe simple deterministic and stochastic models in continuous and discrete time for epidemics taking place in either homogeneous or stratified (non-homogeneous) populations. Several techniques for constructing and analysing models are provided, mostly in the context of viral and bacterial diseases of human populations. These models are contrasted with models for rumours and vector-borne diseases like malaria. Questions of fitting data to models, and their use in understanding methods for controlling the spread of infection, are discussed. Exercises and complementary results at the end of each chapter extend the scope of the text, which will be useful for students taking courses in mathematical biology who have some basic knowledge of probability and statistics.},
  isbn = {978-0-521-64079-4},
  file = {/home/justin/Zotero/storage/I94F577B/Daley_1999_Epidemic Modelling.pdf}
}

@misc{davey1997,
  title = {{{ACT}} Research Report Series: {{Realistic}} Simulation of Item Response Data: (427542008-001)},
  shorttitle = {{{ACT}} Research Report Series},
  author = {Davey, Tim and Nering, Michael L. and Thompson, Tony},
  year = {1997},
  publisher = {{American Psychological Association}},
  doi = {10.1037/e427542008-001},
  langid = {english},
  file = {/home/justin/Zotero/storage/ZYKJYJA7/ACT_RR97-04.pdf}
}

@book{de2013theory,
  title = {The Theory and Practice of Item Response Theory},
  author = {{de Ayala}, Rafael Jaime},
  year = {2013},
  publisher = {{Guilford Publications}},
  date-added = {2019-12-12 11:25:43 -0600},
  date-modified = {2019-12-12 11:26:19 -0600}
}

@article{deayala2002,
  title = {Differential Item Functioning: {{A}} Mixture Distribution Conceptualization},
  shorttitle = {Differential Item Functioning},
  author = {De Ayala, R. J. and Kim, Seock-Ho and Stapleton, Laura M. and Dayton, C. Mitchell},
  year = {2002},
  month = sep,
  journal = {International Journal of Testing},
  volume = {2},
  number = {3-4},
  pages = {243--276},
  issn = {1530-5058, 1532-7574},
  doi = {10/fk9mwz},
  langid = {english},
  file = {/home/justin/Zotero/storage/PNPVYBTY/De Ayala et al_2002_Differential Item Functioning.pdf}
}

@article{debelak2013,
  ids = {debelakPrincipalComponentAnalysis2013},
  title = {Principal Component Analysis of Smoothed Tetrachoric Correlation Matrices as a Measure of Dimensionality},
  author = {Debelak, Rudolf and Tran, Ulrich S.},
  year = {2013},
  journal = {Educational and Psychological Measurement},
  volume = {73},
  number = {1},
  pages = {63--77},
  publisher = {{SAGE Publications Sage CA: Los Angeles, CA}},
  issn = {0013-1644},
  doi = {10/ghx8xx},
  abstract = {The application of principal component analysis and parallel analysis to smoothed tetrachoric correlation matrices was investigated in a simulation study. To evaluate the effect of several smoothing algorithms, 360 different types of data sets were simulated. Under each simulated condition, two item sets, each fitting a unidimensional two-parameter logistic model, were combined with each other. The simulations differed in the size of the simulated item sets, the size of the person samples, the distribution of the difficulty and discrimination parameters, and the correlation between the person parameters. In general, the application of a smoothing algorithm led to an improved performance in the assessment of dimensionality, but minor differences between the three investigated smoothing algorithms were found. Procedures to apply two of the three investigated smoothing algorithms via R software packages are presented.},
  date-added = {2019-12-11 11:45:29 -0600},
  date-modified = {2019-12-11 11:45:29 -0600},
  langid = {english},
  keywords = {matrix smooth,tetrachoric},
  file = {/home/justin/Zotero/storage/XBPFBPDL/Debelak and Tran - 2013 - Principal Component Analysis of Smoothed Tetrachor.pdf}
}

@article{debelak2016,
  ids = {debelak2016a,debelakComparingEffectsDifferent2016},
  title = {Comparing the Effects of Different Smoothing Algorithms on the Assessment of Dimensionality of Ordered Categorical Items with Parallel Analysis},
  author = {Debelak, Rudolf and Tran, Ulrich S.},
  year = {2016},
  journal = {PLOS ONE},
  volume = {11},
  number = {2},
  pages = {1--18},
  publisher = {{Public Library of Science}},
  doi = {10/f8q4j2},
  abstract = {The analysis of polychoric correlations via principal component analysis and exploratory factor analysis are well-known approaches to determine the dimensionality of ordered categorical items. However, the application of these approaches has been considered as critical due to the possible indefiniteness of the polychoric correlation matrix. A possible solution to this problem is the application of smoothing algorithms. This study compared the effects of three smoothing algorithms, based on the Frobenius norm, the adaption of the eigenvalues and eigenvectors, and on minimum-trace factor analysis, on the accuracy of various variations of parallel analysis by the means of a simulation study. We simulated different datasets which varied with respect to the size of the respondent sample, the size of the item set, the underlying factor model, the skewness of the response distributions and the number of response categories in each item. We found that a parallel analysis and principal component analysis of smoothed polychoric and Pearson correlations led to the most accurate results in detecting the number of major factors in simulated datasets when compared to the other methods we investigated. Of the methods used for smoothing polychoric correlation matrices, we recommend the algorithm based on minimum trace factor analysis.},
  date-added = {2019-12-11 11:45:29 -0600},
  date-modified = {2019-12-12 15:00:48 -0600},
  keywords = {matrix smooth},
  file = {/home/justin/Zotero/storage/6YD26VNG/Debelak and Tran - 2016 - Comparing the Effects of Different Smoothing Algor.pdf}
}

@article{decorte2011,
  title = {Designing Pareto-Optimal Selection Systems: {{Formalizing}} the Decisions Required for Selection System Development.},
  shorttitle = {Designing Pareto-Optimal Selection Systems},
  author = {De Corte, Wilfried and Sackett, Paul R. and Lievens, Filip},
  year = {2011},
  journal = {Journal of Applied Psychology},
  volume = {96},
  number = {5},
  pages = {907--926},
  issn = {1939-1854, 0021-9010},
  doi = {10/d7nmmv},
  langid = {english},
  file = {/home/justin/Zotero/storage/ZWZIKNX2/De Corte et al. - 2011 - Designing pareto-optimal selection systems Formal.pdf}
}

@article{delgado2014,
  title = {A {{Rank-Constrained Optimization}} Approach: {{Application}} to {{Factor Analysis}}},
  shorttitle = {A {{Rank-Constrained Optimization}} Approach},
  author = {Delgado, Ram{\'o}n A. and Ag{\"u}ero, Juan C. and Goodwin, Graham C.},
  year = {2014},
  journal = {IFAC Proceedings Volumes},
  volume = {47},
  number = {3},
  pages = {10373--10378},
  issn = {14746670},
  doi = {10/gjrkcx},
  langid = {english},
  file = {/home/justin/Zotero/storage/SSFTKYAZ/Delgado et al. - 2014 - A Rank-Constrained Optimization approach Applicat.pdf}
}

@article{devlin1975robust,
  ids = {devlinRobustEstimationOutlier1975},
  title = {Robust Estimation and Outlier Detection with Correlation Coefficients},
  author = {Devlin, Susan J and Gnanadesikan, Ramanathan and Kettenring, Jon R},
  year = {1975},
  journal = {Biometrika},
  volume = {62},
  number = {3},
  pages = {531--545},
  publisher = {{Oxford University Press}},
  doi = {10/bx5bqh},
  date-added = {2019-12-11 12:40:07 -0600},
  date-modified = {2019-12-11 12:40:07 -0600},
  keywords = {matrix smooth}
}

@article{devlin1981,
  title = {Robust Estimation of Dispersion Matrices and Principal Components},
  author = {Devlin, S.J. and Gnanadesikan, R. and Kettenring, J.R.},
  year = {1981},
  journal = {Journal of the American Statistical Association},
  volume = {76},
  number = {374},
  pages = {354--362},
  doi = {10/ghx8xz},
  keywords = {matrix smooth}
}

@article{dewinter2009,
  ids = {de2009exploratory},
  title = {Exploratory Factor Analysis with Small Sample Sizes},
  author = {{de Winter}, J. C. F. and Dodou, D. and Wieringa, P. A.},
  year = {2009},
  journal = {Multivariate Behavioral Research},
  volume = {44},
  number = {2},
  pages = {147--181},
  publisher = {{Taylor \& Francis}},
  issn = {0027-3171, 1532-7906},
  doi = {10/bczhf7},
  date-added = {2020-02-07 10:53:37 -0600},
  date-modified = {2020-02-07 10:53:37 -0600},
  langid = {english},
  keywords = {Hong (1999)},
  file = {/home/justin/Zotero/storage/IS32MFQQ/de Winter et al_2009_Exploratory Factor Analysis With Small Sample Sizes.pdf}
}

@article{dewinter2012,
  ids = {de2012factor,dewinterFactorRecoveryPrincipal2012,winter2012},
  title = {Factor Recovery by Principal Axis Factoring and Maximum Likelihood Factor Analysis as a Function of Factor Pattern and Sample Size},
  author = {{de Winter}, J. C.F. and Dodou, D.},
  year = {2012},
  journal = {Journal of Applied Statistics},
  volume = {39},
  number = {4},
  eprint = {https://doi.org/10.1080/02664763.2011.610445},
  pages = {695--710},
  publisher = {{Taylor \& Francis}},
  doi = {10/dpk8jr},
  abstract = {Principal axis factoring (PAF) and maximum likelihood factor analysis (MLFA) are two of the most popular estimation methods in exploratory factor analysis. It is known that PAF is better able to recover weak factors and that the maximum likelihood estimator is asymptotically efficient. However, there is almost no evidence regarding which method should be preferred for different types of factor patterns and sample sizes. Simulations were conducted to investigate factor recovery by PAF and MLFA for distortions of ideal simple structure and sample sizes between 25 and 5000. Results showed that PAF is preferred for population solutions with few indicators per factor and for overextraction. MLFA outperformed PAF in cases of unequal loadings within factors and for underextraction. It was further shown that PAF and MLFA do not always converge with increasing sample size. The simulation findings were confirmed by an empirical study as well as by a classic plasmode, Thurstone's box problem. The present results are of practical value for factor analysts.},
  date-added = {2020-02-07 10:39:54 -0600},
  date-modified = {2020-02-07 10:40:08 -0600},
  keywords = {empirical data,exploratory factor analysis,Hong (1999),maximum likelihood factor analysis,parameter estimation,plasmode,principal axis factoring,READ,simulations},
  file = {/home/justin/Zotero/storage/3N7V54EH/Winter_Dodou_2012_Factor recovery by principal axis factoring and maximum likelihood factor.pdf;/home/justin/Zotero/storage/QAJDBATF/02664763.2011.html}
}

@techreport{dewinter2014,
  type = {{{SSRN Scholarly Paper}}},
  title = {Common {{Factor Analysis}} versus {{Principal Component Analysis}}: {{A Comparison}} of {{Loadings}} by {{Means}} of {{Simulations}}},
  shorttitle = {Common {{Factor Analysis}} versus {{Principal Component Analysis}}},
  author = {{de Winter}, Joost and Dodou, Dimitra},
  year = {2014},
  month = jun,
  number = {ID 2450825},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {Common factor analysis (CFA) and principal component analysis (PCA) are widely used multivariate techniques. Using simulations, we compared CFA with PCA loadings for distortions of a perfect cluster configuration. Results showed that nonzero PCA loadings were higher and more stable than nonzero CFA loadings. Compared to CFA loadings, PCA loadings correlated weakly with the true factor loadings for underextraction, overextraction, and heterogeneous loadings within factors. The pattern of differences between CFA and PCA was consistent across sample sizes, levels of loadings, principal axis factoring versus maximum likelihood factor analysis, and blind versus target rotation.},
  langid = {english},
  keywords = {data reduction,exploratory factor analysis,latent variables},
  file = {/home/justin/Zotero/storage/KV8AU47M/papers.html}
}

@article{dewinter2016,
  ids = {dewinter2016a},
  title = {Common Factor Analysis versus Principal Component Analysis: {{A}} Comparison of Loadings by Means of Simulations},
  shorttitle = {Common Factor Analysis versus Principal Component Analysis},
  author = {{de Winter}, Joost C. F. and Dodou, Dimitra},
  year = {2016},
  month = jan,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {45},
  number = {1},
  pages = {299--321},
  issn = {0361-0918, 1532-4141},
  doi = {10/gh64cn},
  langid = {english},
  file = {/home/justin/Zotero/storage/QI7DWX93/de Winter_Dodou_2016_Common Factor Analysis versus Principal Component Analysis.pdf}
}

@article{dillon1987,
  title = {Offending Estimates in Covariance Structure Analysis: {{Comments}} on the Causes of and Solutions to {{Heywood}} Cases.},
  author = {Dillon, William R and Kumar, Ajith and Mulani, Narendra},
  year = {1987},
  journal = {Psychological Bulletin},
  volume = {101},
  number = {1},
  pages = {126},
  publisher = {{American Psychological Association}},
  doi = {10/bsgpv8},
  date-added = {2020-01-02 12:58:09 -0600},
  date-modified = {2020-01-02 12:58:16 -0600}
}

@article{dingman1958,
  title = {The {{Relation Between Coefficients}} of {{Correlation}} and {{Difficulty Factors}}},
  author = {Dingman, Harvey F.},
  year = {1958},
  journal = {British Journal of Statistical Psychology},
  volume = {11},
  number = {1},
  pages = {13--17},
  issn = {2044-8317},
  doi = {10/c3fs6d},
  abstract = {The purpose of the following inquiry was to test the view, put forward by Ferguson and others, that unequal dichotomization of items tends to produce spurious difficulty factors. Subtests of three levels of difficulty were compiled for each of the three factors, and applied to nearly 500 students. The correlations were calculated by the phi, cosine-pi, and full tetrachoric formulae, and four factors extracted. The results obtained do not support Ferguson's hypothesis.},
  copyright = {1958 The British Psychological Society},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1958.tb00187.x},
  file = {/home/justin/Zotero/storage/2SQLJA3Q/Dingman_1958_The Relation Between Coefficients of Correlation and Difficulty Factors.pdf;/home/justin/Zotero/storage/HEM3XA2X/j.2044-8317.1958.tb00187.html}
}

@article{distefano2014,
  title = {A Comparison of Diagonal Weighted Least Squares Robust Estimation Techniques for Ordinal Data},
  author = {DiStefano, Christine and Morgan, Grant B.},
  year = {2014},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {21},
  number = {3},
  pages = {425--438},
  issn = {1070-5511, 1532-8007},
  doi = {10/gf6rfc},
  langid = {english}
}

@article{divgi1979,
  title = {Calculation of the Tetrachoric Correlation Coefficient},
  author = {Divgi, D. R.},
  year = {1979},
  journal = {Psychometrika},
  volume = {44},
  number = {2},
  pages = {169--172},
  issn = {1860-0980},
  doi = {10/cdb49h},
  abstract = {A new subroutine has been developed for calculating the terachoric correlation coefficient. Recent advances in computing inverse normal and bivariate normal distributions have been utilized. The iterative procedure is started with an approximation with an error less than{$\pm$}.0135.},
  langid = {english},
  file = {/home/justin/Zotero/storage/FAKNRXDU/Divgi - 1979 - Calculation of the tetrachoric correlation coeffic.pdf}
}

@article{dolan1994,
  ids = {dolan1994a},
  title = {Factor Analysis of Variables with 2, 3, 5 and 7 Response Categories: {{A}} Comparison of Categorical Variable Estimators Using Simulated Data},
  shorttitle = {Factor Analysis of Variables with 2, 3, 5 and 7 Response Categories},
  author = {Dolan, Conor V.},
  year = {1994},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {47},
  number = {2},
  pages = {309--326},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {2044-8317},
  doi = {10/c6jgtv},
  abstract = {Two estimators in the factor analysis of categorical items are studied, the weighted least squares function implemented in the tandem PRELIS-LISREL 7 and a generalized least squares function implemen...},
  langid = {english},
  file = {/home/justin/Zotero/storage/ZPRNSSSZ/Dolan_1994_Factor analysis of variables with 2, 3, 5 and 7 response categories.pdf;/home/justin/Zotero/storage/RW8KL5FC/j.2044-8317.1994.tb01039.html;/home/justin/Zotero/storage/SDWH2VEX/j.2044-8317.1994.tb01039.html}
}

@article{dong1985,
  ids = {dong1985non},
  title = {Non-{{Gramian}} and Singular Matrices in Maximum Likelihood Factor Analysis},
  author = {Dong, H.},
  year = {1985},
  journal = {Applied Psychological Measurement},
  volume = {9},
  number = {4},
  pages = {363--366},
  publisher = {{Sage Publications Sage CA: Thousand Oaks, CA}},
  doi = {10/fcp478},
  date-added = {2019-12-11 12:42:53 -0600},
  date-modified = {2019-12-11 12:43:02 -0600},
  keywords = {factor analysis,matrix smooth}
}

@article{dorans1978,
  title = {Alternative Weighting Schemes for Linear Prediction},
  author = {Dorans, Neil and Drasgow, Fritz},
  year = {1978},
  month = jun,
  journal = {Organizational Behavior and Human Performance},
  volume = {21},
  number = {3},
  pages = {316--345},
  issn = {00305073},
  doi = {10/c43qjw},
  langid = {english},
  file = {/home/justin/Zotero/storage/VUX4F9GW/Dorans_Drasgow_1978_Alternative weighting schemes for linear prediction.pdf}
}

@article{dorans2014,
  title = {Simulate to Understand Models, Not Nature},
  author = {Dorans, Neil J.},
  year = {2014},
  journal = {ETS Research Report Series},
  volume = {2014},
  number = {2},
  pages = {1--9},
  issn = {2330-8516},
  doi = {10/gf4tzk},
  abstract = {Simulations are widely used. Simulations produce numbers that are deductive demonstrations of what a model says will happen. They produce numerical results that are consistent with the premises of the model used to generate the numbers. These simulated numerical results are not empirical data that address aspects of the world that lies outside the model. In contrast, empirical data are central to the scientific method. When a simulation is substituted for the assessment of hypotheses with real data, a false sense of understanding can ensue and with it a biased perspective on the world. To illustrate the limitations of simulation and their proper role, examples are drawn from simulation studies about score equating.},
  copyright = {\textcopyright{} 2014 Educational Testing Service},
  langid = {english},
  keywords = {attribute substitution,deductive demonstrations,empirical investigations,score equating,Simulations},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ets2.12013},
  file = {/home/justin/Zotero/storage/VFI3HRYZ/Dorans_2014_Simulate to Understand Models, Not Nature.pdf;/home/justin/Zotero/storage/GE9TCPBZ/ets2.html}
}

@article{dormann2013,
  title = {Collinearity: A Review of Methods to Deal with It and a Simulation Study Evaluating Their Performance},
  shorttitle = {Collinearity},
  author = {Dormann, Carsten F. and Elith, Jane and Bacher, Sven and Buchmann, Carsten and Carl, Gudrun and Carr{\'e}, Gabriel and Marqu{\'e}z, Jaime R. Garc{\'i}a and Gruber, Bernd and Lafourcade, Bruno and Leit{\~a}o, Pedro J. and M{\"u}nkem{\"u}ller, Tamara and McClean, Colin and Osborne, Patrick E. and Reineking, Bj{\"o}rn and Schr{\"o}der, Boris and Skidmore, Andrew K. and Zurell, Damaris and Lautenbach, Sven},
  year = {2013},
  journal = {Ecography},
  volume = {36},
  number = {1},
  pages = {27--46},
  issn = {1600-0587},
  doi = {10/f4scb2},
  abstract = {Collinearity refers to the non independence of predictor variables, usually in a regression-type analysis. It is a common feature of any descriptive ecological data set and can be a problem for parameter estimation because it inflates the variance of regression parameters and hence potentially leads to the wrong identification of relevant predictors in a statistical model. Collinearity is a severe problem when a model is trained on data from one region or time, and predicted to another with a different or unknown structure of collinearity. To demonstrate the reach of the problem of collinearity in ecology, we show how relationships among predictors differ between biomes, change over spatial scales and through time. Across disciplines, different approaches to addressing collinearity problems have been developed, ranging from clustering of predictors, threshold-based pre-selection, through latent variable methods, to shrinkage and regularisation. Using simulated data with five predictor-response relationships of increasing complexity and eight levels of collinearity we compared ways to address collinearity with standard multiple regression and machine-learning approaches. We assessed the performance of each approach by testing its impact on prediction to new data. In the extreme, we tested whether the methods were able to identify the true underlying relationship in a training dataset with strong collinearity by evaluating its performance on a test dataset without any collinearity. We found that methods specifically designed for collinearity, such as latent variable methods and tree based models, did not outperform the traditional GLM and threshold-based pre-selection. Our results highlight the value of GLM in combination with penalised methods (particularly ridge) and threshold-based pre-selection when omitted variables are considered in the final interpretation. However, all approaches tested yielded degraded predictions under change in collinearity structure and the `folk lore'-thresholds of correlation coefficients between predictor variables of |r| {$>$}0.7 was an appropriate indicator for when collinearity begins to severely distort model estimation and subsequent prediction. The use of ecological understanding of the system in pre-analysis variable selection and the choice of the least sensitive statistical approaches reduce the problems of collinearity, but cannot ultimately solve them.},
  copyright = {\textcopyright{} 2012 The Authors},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1600-0587.2012.07348.x},
  file = {/home/justin/Zotero/storage/6NIRWMQX/Dormann et al. - 2013 - Collinearity a review of methods to deal with it .pdf;/home/justin/Zotero/storage/MW2VN2A7/(ISSN)1600-0587.html}
}

@article{dudgeon2004,
  title = {A Note on Extending {{Steiger}}'s (1998) Multiple Sample {{RMSEA}} Adjustment to Other Noncentrality Parameter-Based Statistics},
  author = {Dudgeon, Paul},
  year = {2004},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {11},
  number = {3},
  pages = {305--319},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/dtqg7q},
  abstract = {This article considers the implications for other noncentrality parameter-based statistics from Steiger's (1998) multiple sample adjustment to the root mean square error of approximation (RMSEA) measure. When a structural equation model is fitted simultaneously in more than 1 sample, it is shown that the calculation of the noncentrality parameter used in tests of approximate fit and in point and interval estimators of other noncentral fit statistics (except the expected cross-validation index) also requires a likeminded adjustment. Furthermore, it is shown that an adjustment is needed in multiple sample models for correctly calculating MacCallum, Browne, and Sugawara's (1996) approach to power analysis. The accuracy of these proposals is investigated and demonstrated in a small Monte Carlo study in which particular attention is paid to using appropriately constructed covariance matrices that give specified nonzero population discrepancy values under maximum likelihood estimation.},
  annotation = {\_eprint: https://doi.org/10.1207/s15328007sem1103\_1},
  file = {/home/justin/Zotero/storage/3RUJ9PQN/Dudgeon_2004_A Note on Extending Steiger's (1998) Multiple Sample RMSEA Adjustment to Other.pdf}
}

@article{durrett2005,
  title = {The {{Structure}} of {{Axis II Disorders}} in {{Adolescents}}: {{A Cluster-}} and {{Factor-Analytic Investigation}} of {{DSM-IV Categories}} and {{Criteria}}},
  shorttitle = {The {{Structure}} of {{Axis II Disorders}} in {{Adolescents}}},
  author = {Durrett, Christine and Westen, Drew},
  year = {2005},
  month = aug,
  journal = {Journal of Personality Disorders},
  volume = {19},
  number = {4},
  pages = {440--461},
  publisher = {{Guilford Publications Inc.}},
  issn = {0885-579X},
  doi = {10/b68g7c},
  abstract = {The aim of this study was to ascertain whether the structure of personality disorder (PD) symptoms in adolescents assessed using DSM-IV diagnoses and diagnostic criteria resembles the structure intended for the diagnosis of PDs in adults. A national sample of clinicians rated DSM-IV Axis II criteria on 294 adolescent patients in treatment for enduring maladaptive personality patterns. Cluster analysis replicating procedures used in an adult sample by Morey (1988) identified considerable similarity between adult and adolescent PDs, as did exploratory factor analysis of ratings of diagnostic criteria, which yielded ten empirically derived factors that resembled the ten DSM-IV PDs. Cluster analysis and confirmatory factor analysis with indicators of Axis II symptoms produced mixed results in replicating the DSM-IV hierarchical structure of PDs (Clusters A, B, and C), although hierarchical models generally fared better than models specifying only first-order factors or clusters. The structure of personality pathology as assessed by Axis II criteria in adolescents resembles that outlined in DSM-IV Axis II for adults, suggesting that PDs can be assessed in adolescents as in adults. Whether this is an optimal way of diagnosing personality pathology in adolescence, however, requires further investigation.},
  file = {/home/justin/Zotero/storage/W3GFX6DK/Durrett and Westen - 2005 - The Structure of Axis II Disorders in Adolescents.pdf;/home/justin/Zotero/storage/MIWJVQT7/pedi.2005.19.4.html}
}

@inproceedings{dwork2012,
  title = {Fairness through Awareness},
  booktitle = {Proceedings of the 3rd {{Innovations}} in {{Theoretical Computer Science Conference}}},
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  year = {2012},
  month = jan,
  series = {{{ITCS}} '12},
  pages = {214--226},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10/fzd3f9},
  abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  isbn = {978-1-4503-1115-1},
  file = {/home/justin/Zotero/storage/6DR9KDWR/Dwork et al. - 2012 - Fairness through awareness.pdf}
}

@article{dwyer1939contribution,
  title = {The Contribution of an Orthogonal Multiple Factor Solution to Multiple Correlation},
  author = {Dwyer, Paul S},
  year = {1939},
  journal = {Psychometrika},
  volume = {4},
  number = {2},
  pages = {163--171},
  publisher = {{Springer}},
  doi = {10/d6tqkf},
  date-added = {2020-01-08 11:30:57 -0600},
  date-modified = {2020-01-08 11:30:57 -0600}
}

@article{dykstra1983,
  title = {An Algorithm for Restricted Least Squares Regression},
  author = {Dykstra, Richard L},
  year = {1983},
  journal = {Journal of the American Statistical Association},
  volume = {78},
  number = {384},
  pages = {837--842},
  doi = {10/gjrkcq}
}

@incollection{eco1994,
  title = {On the Impossibility of Drawing a Map of the Empire on a Scale of 1 to 1},
  booktitle = {How to Travel with a Salmon \& Other Essays},
  author = {Eco, Umberto},
  editor = {Weaver, William and Eco, Umberto},
  year = {1994},
  edition = {1st ed..},
  publisher = {{New York:Harcourt, Brace}},
  address = {{New York}},
  isbn = {0-15-100136-7},
  file = {/home/justin/Zotero/storage/IZ75Q5IB/Eco_1994_On the Impossibility of Drawing a Map of the Empire on a Scale of 1 to 1.pdf}
}

@article{ekstrom2011,
  title = {An {{Empirical Polychoric Correlation Coefficient}}},
  author = {Ekstr{\"o}m, Joakim},
  year = {2011},
  month = oct,
  abstract = {A new measure of association for ordinal variables is proposed. The new measure of association, named the empirical polychoric correlation coefficient, builds upon the theoretical framework of the polychoric correlation coefficient, but relaxes its fundamental assumption so that an underlying continuous joint distribution is only assumed to exist, not to be of any specific distributional family. The empirical polychoriccorrelation has good properties in terms of statistical robustness and asymptotics, and is easy to compute by hand. Moreover, a simulation study indicates that the new measure of association is more stable, in terms of standard deviation, than conventional polychoric correlation coefficients.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/justin/Zotero/storage/LZYA7J56/Ekström - 2011 - An Empirical Polychoric Correlation Coefficient.pdf;/home/justin/Zotero/storage/MRU4W3LP/2tw0b313.html}
}

@misc{elinas,
  title = {Knowing {{Your Neighbours}}: {{Machine Learning}} on {{Graphs}} | by {{Pantelis Elinas}} | Stellargraph | {{Medium}}},
  author = {Elinas, Pantelis},
  howpublished = {https://medium.com/stellargraph/knowing-your-neighbours-machine-learning-on-graphs-9b7c3d0d5896},
  file = {/home/justin/Zotero/storage/ZSCXTEGN/knowing-your-neighbours-machine-learning-on-graphs-9b7c3d0d5896.html}
}

@book{embretson2000,
  title = {Item Response Theory for Psychologists},
  author = {Embretson, Susan E. and Reise, Steven Paul},
  year = {2000},
  series = {Multivariate Applications Book Series},
  publisher = {{L. Erlbaum Associates}},
  address = {{Mahwah, N.J}},
  isbn = {978-0-8058-2818-4 978-0-8058-2819-1},
  lccn = {BF39 .E495 2000},
  keywords = {Item response theory,Psychometrics}
}

@book{enders2010,
  title = {Applied Missing Data Analysis},
  author = {Enders, Craig K},
  year = {2010},
  publisher = {{Guilford press}}
}

@misc{epskampTutorialRegularizedPartial2018,
  type = {Article / {{Letter}} to Editor},
  title = {A {{Tutorial}} on {{Regularized Partial Correlation Networks}}},
  author = {Epskamp, S. and Fried, E. I. and Psychologie, Instituut},
  year = {2018},
  journal = {24},
  abstract = {Recent years have seen an emergence of network modeling applied to moods, attitudes, and problems in the realm of psychology. In this framework, psychological variables are understood to directly affect each other rather than being caused by an unobserved latent entity. In this tutorial, we introduce the reader to estimating the most popular network model for psychological data: the partial correlation network. We describe how regularization techniques can be used to efficiently estimate a parsimonious and interpretable network structure in psychological data. We show how to perform these analyses in R and demonstrate the method in an empirical example on posttraumatic stress disorder data. In addition, we discuss the effect of the hyperparameter that needs to be manually set by the researcher, how to handle non-normal data, how to determine the required sample size for a network analysis, and provide a checklist with potential solutions for problems that can arise when estimating regularized partial correlation networks. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  howpublished = {https://openaccess.leidenuniv.nl/handle/1887/79580},
  annotation = {Accepted: 2019-10-15T17:45:00Z},
  file = {/home/justin/Zotero/storage/59VBN2TQ/79580.html}
}

@article{ernestc.davenport2020,
  title = {The {{Relative Performance}} Index: {{Neutralizing Simpson}}'s Paradox},
  shorttitle = {The Relative Performance Index},
  author = {Ernest C. Davenport, Jr. and Nickodem, Kyle and Davison, Mark L. and Phillips, Gareth and Graham, Edmund},
  year = {2020},
  month = apr,
  journal = {The American Statistician},
  volume = {74},
  number = {2},
  pages = {116--124},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10/ggvrpc},
  abstract = {Comparing populations on one or more variables is often of interest. These comparisons are typically made using the mean; however, it is well known that mean comparisons can lead to misinterpretation because of Simpson's paradox. Simpson's paradox occurs when there is a differential distribution of subpopulations across the populations being compared and the means of those subpopulations are different. This article develops the relative performance index (RPI) to ameliorate effects of Simpson's paradox. Data from the National Assessment of Educational Progress (NAEP) are used to illustrate use of the new index. The utility of RPI is compared to the population mean and a prior index, the balanced index. This article shows how RPI can be generalized to a variety of contexts with implications for decision making.},
  keywords = {Means,Mixture distributions,Multiple populations,Simpson's paradox},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2018.1451777},
  file = {/home/justin/Zotero/storage/FU8GBZUM/Ernest C. Davenport et al. - 2020 - The Relative Performance Index Neutralizing Simps.pdf;/home/justin/Zotero/storage/K6DLV39P/00031305.2018.html}
}

@article{fabrigar1999,
  ids = {fabrigar1999evaluating},
  title = {Evaluating the Use of Exploratory Factor Analysis in Psychological Research.},
  author = {Fabrigar, Leandre R. and Wegener, Duane T. and MacCallum, Robert C. and Strahan, Erin J.},
  year = {1999},
  journal = {Psychological Methods},
  volume = {4},
  number = {3},
  pages = {272--299},
  publisher = {{American Psychological Association}},
  issn = {1939-1463, 1082-989X},
  doi = {10/b2ztct},
  date-added = {2020-01-07 13:37:29 -0600},
  date-modified = {2020-01-07 13:37:29 -0600},
  langid = {english},
  file = {/home/justin/Zotero/storage/VYSBW7XU/Fabrigar et al. - 1999 - Evaluating the use of exploratory factor analysis .pdf}
}

@article{falk2018,
  title = {On {{Lagrange Multiplier Tests}} in {{Multidimensional Item Response Theory}}: {{Information Matrices}} and {{Model Misspecification}}},
  shorttitle = {On {{Lagrange Multiplier Tests}} in {{Multidimensional Item Response Theory}}},
  author = {Falk, Carl F. and Monroe, Scott},
  year = {2018},
  month = aug,
  journal = {Educational and Psychological Measurement},
  volume = {78},
  number = {4},
  pages = {653--678},
  issn = {0013-1644},
  doi = {10/gjrkc7},
  abstract = {Lagrange multiplier (LM) or score tests have seen renewed interest for the purpose of diagnosing misspecification in item response theory (IRT) models. LM tests can also be used to test whether parameters differ from a fixed value. We argue that the utility of LM tests depends on both the method used to compute the test and the degree of misspecification in the initially fitted model. We demonstrate both of these points in the context of a multidimensional IRT framework. Through an extensive Monte Carlo simulation study, we examine the performance of LM tests under varying degrees of model misspecification, model size, and different information matrix approximations. A generalized LM test designed specifically for use under misspecification, which has apparently not been previously studied in an IRT framework, performed the best in our simulations. Finally, we reemphasize caution in using LM tests for model specification searches.},
  pmcid = {PMC6096471},
  pmid = {30147121},
  file = {/home/justin/Zotero/storage/CCHINHWB/Falk and Monroe - 2018 - On Lagrange Multiplier Tests in Multidimensional I.pdf}
}

@article{feeley2008,
  title = {Predicting {{Employee Turnover}} from {{Friendship Networks}}},
  author = {Feeley, Thomas Hugh and Hwang, Jennie and Barnett, George A.},
  year = {2008},
  journal = {Journal of Applied Communication Research},
  volume = {36},
  number = {1},
  pages = {56--73},
  publisher = {{Routledge}},
  issn = {0090-9882},
  doi = {10/bvxkt2},
  abstract = {Employees (n=40) at a fast-food restaurant were surveyed about characteristics of their position and their level of satisfaction. Employees were then asked to report with whom they regularly communicated inside and outside the workplace and to indicate how close they were to employees with whom they were linked. Employee turnover was measured after three months had elapsed. A goal of the research was to replicate a model of employee turnover that predicts employees more central in their social network to be less likely to leave, and to test a social support explanation of the centrality model. The results indicated that employees who reported a greater number of out-degree links with friends were less likely to leave. The number of in-degree links with friends did not significantly predict turnover, and neither did network links with peers. Friendship prestige, measured by the number of in-degree links, was strongly correlated with relational closeness and amount of time spent with employees outside the workplace.},
  file = {/home/justin/Zotero/storage/2CHBP44A/Feeley et al_2008_Predicting Employee Turnover from Friendship Networks.pdf}
}

@article{felps2009,
  title = {Turnover {{Contagion}}: {{How Coworkers}}' {{Job Embeddedness}} and {{Job Search Behaviors Influence Quitting}}},
  shorttitle = {Turnover {{Contagion}}},
  author = {Felps, Will and Mitchell, Terence R. and Hekman, David R. and Lee, Thomas W. and Holtom, Brooks C. and Harman, Wendy S.},
  year = {2009},
  journal = {Academy of Management Journal},
  volume = {52},
  number = {3},
  pages = {545--561},
  issn = {0001-4273, 1948-0989},
  doi = {10/cpb97s},
  langid = {english},
  file = {/home/justin/Zotero/storage/D44UCSVE/Felps et al. - 2009 - Turnover Contagion How Coworkers' Job Embeddednes.pdf}
}

@article{ferrando2004,
  title = {Kernel-Smoothing Estimation of Item Characteristic Functions for Continuous Personality Items: {{An}} Empirical Comparison with the Linear and the Continuous-Response Models},
  author = {Ferrando, Pere J},
  year = {2004},
  journal = {Applied psychological measurement},
  volume = {28},
  number = {2},
  pages = {95--109},
  doi = {10/cx7cbk},
  keywords = {nonparametric IRT}
}

@article{feuerstahler2018,
  title = {Sources of {{Error}} in {{IRT Trait Estimation}}},
  author = {Feuerstahler, Leah M.},
  year = {2018},
  month = jul,
  journal = {Applied Psychological Measurement},
  volume = {42},
  number = {5},
  pages = {359--375},
  issn = {0146-6216},
  doi = {10/gfx5sm},
  abstract = {In item response theory (IRT), item response probabilities are a function of item characteristics and latent trait scores. Within an IRT framework, trait score misestimation results from (a) random error, (b) the trait score estimation method, (c) errors in item parameter estimation, and (d) model misspecification. This study investigated the relative effects of these error sources on the bias and confidence interval coverage rates for trait scores. Our results showed that overall, bias values were close to 0, and coverage rates were fairly accurate for central trait scores and trait estimation methods that did not use a strong Bayesian prior. However, certain types of model misspecifications were found to produce severely biased trait estimates with poor coverage rates, especially at extremes of the latent trait continuum. It is demonstrated that biased trait estimates result from estimated item response functions (IRFs) that exhibit systematic conditional bias, and that these conditionally biased IRFs may not be detected by model or item fit indices. One consequence of these results is that certain types of model misspecifications can lead to estimated trait scores that are nonlinearly related to the data-generating latent trait. Implications for item and trait score estimation and interpretation are discussed.},
  pmcid = {PMC6023095},
  pmid = {30034054},
  file = {/home/justin/Zotero/storage/LFC6WGXN/Feuerstahler - 2018 - Sources of Error in IRT Trait Estimation.pdf}
}

@book{filhoComputerScienceDistilled2017,
  title = {Computer {{Science Distilled}}},
  author = {Filho, Wladston Ferreira},
  year = {2017},
  publisher = {{Code Energy LLC}}
}

@article{finch2010,
  title = {Item {{Parameter Estimation}} for the {{MIRT Model}}: {{Bias}} and {{Precision}} of {{Confirmatory Factor Analysis}}\textemdash{{Based Models}}},
  author = {Finch, Holmes},
  year = {2010},
  journal = {Applied Psychological Measurement},
  volume = {34},
  number = {1},
  pages = {10--26},
  doi = {10/fxdpjp}
}

@article{finch2011,
  title = {Multidimensional Item Response Theory Parameter Estimation with Nonsimple Structure Items},
  author = {Finch, Holmes},
  year = {2011},
  journal = {Applied Psychological Measurement},
  volume = {35},
  number = {1},
  pages = {67--82},
  doi = {10/fgc56m}
}

@phdthesis{finger2001,
  title = {A Comparison of Full-Information and Unweighted Least-Squares Limited-Information Methods Used with the 2-Parameter Normal Ogive Model},
  author = {Finger, Michael Steven},
  year = {2001},
  address = {{United States -- Minnesota}},
  abstract = {Discrimination (a) and difficulty (b) were estimated using three least-squares methods and marginal maximum likelihood using an EM-like algorithm (MMLE-EM) without item prior distributions. The dichotomous item responses were generated to fit the 2-parameter normal ogive IRT model (2PNOM). True b distribution and the true \&thetas; distribution were standard normal and true a was distributed either normal with a mean of .75 and standard deviation of .1 or normal with a mean of 1.5 and standard deviation of .2. The two distributions for true a were completely crossed with six levels of sample size (250, 500, 750, 1,000, 1,500, and 2,000), four levels of test length (10, 15, 25, and 50 items). Each of the 48 conditions from the 2 \texttimes{} 6 \texttimes{} 4 between factorial design was replicated five times, from which 240 separate datasets resulted. Parameter recovery was evaluated in terms of root mean square error ( RMSE), average signed bias and Pearson correlation for a and for b as well as RMSE for the 2PNOM item response function. Based on many of the indices, the most accurate estimates came from McDonald's factor analysis of joint proportions correct (FA-JPC). Future research should compare FA-JPC to MMLE-EM under nonnormal \&thetas; distributions and should compare FA-JPC to MMLE-EM with item priors.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9780493377773},
  langid = {english},
  school = {University of Minnesota},
  keywords = {Item factor analysis,Item response,Parameter estimation,Psychology},
  file = {/home/justin/Zotero/storage/R6PUDDYH/Finger_2001_A comparison of full-information and unweighted least-squares.pdf}
}

@article{finkelman2010,
  title = {Item Selection and Hypothesis Testing for the Adaptive Measurement of Change},
  author = {Finkelman, Matthew D and Weiss, David J and {Kim-Kang}, Gyenam},
  year = {2010},
  journal = {Applied Psychological Measurement},
  volume = {34},
  number = {4},
  pages = {238--254},
  doi = {10/fndr9k}
}

@article{fleming2005,
  ids = {flemingTETCORRComputerProgram2005a},
  title = {{{TETCORR}}: A Computer Program to Compute Smoothed Tetrachoric Correlation Matrices},
  author = {Fleming, James S},
  year = {2005},
  journal = {Behavior research methods},
  volume = {37},
  number = {1},
  pages = {59--64},
  publisher = {{Springer}},
  doi = {10/bchw5h},
  date-added = {2019-12-11 12:27:37 -0600},
  date-modified = {2019-12-11 12:27:47 -0600},
  keywords = {smooth matrix,tetrachoric},
  file = {/home/justin/Zotero/storage/DTX7TDFQ/Fleming - 2005 - TETCORR A computer program to compute smoothed te.pdf}
}

@article{flora2004,
  title = {An Empirical Evaluation of Alternative Methods of Estimation for Confirmatory Factor Analysis with Ordinal Data.},
  author = {Flora, DB},
  year = {2004},
  journal = {Psychological Methods},
  doi = {10/b6vmvn}
}

@article{flora20041213,
  title = {An {{Empirical Evaluation}} of {{Alternative Methods}} of {{Estimation}} for {{Confirmatory Factor Analysis With Ordinal Data}}.},
  author = {Flora, David B.},
  year = {20041213},
  journal = {Psychological Methods},
  volume = {9},
  number = {4},
  pages = {466},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1463},
  doi = {10/b6vmvn},
  abstract = {APA PsycNet FullTextHTML page},
  langid = {english},
  file = {/home/justin/Zotero/storage/BZZUJNAS/2004-21445-005.html}
}

@article{flora2012,
  title = {Old and {{New Ideas}} for {{Data Screening}} and {{Assumption Testing}} for {{Exploratory}} and {{Confirmatory Factor Analysis}}},
  author = {Flora, David B. and LaBrish, Cathy and Chalmers, R. Philip},
  year = {2012},
  journal = {Frontiers in Psychology},
  volume = {3},
  issn = {1664-1078},
  doi = {10/gfzcx6},
  file = {/home/justin/Zotero/storage/5ZPWZKN8/Flora et al. - 2012 - Old and New Ideas for Data Screening and Assumptio.pdf}
}

@article{foldnes2020,
  title = {Pernicious {{Polychorics}}: {{The Impact}} and {{Detection}} of {{Underlying Non-normality}}},
  shorttitle = {Pernicious {{Polychorics}}},
  author = {Foldnes, Nj{\aa}l and Gr{\o}nneberg, Steffen},
  year = {2020},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {27},
  number = {4},
  pages = {525--543},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/gjrkdk},
  abstract = {Ordinal data in social science statistics are often modeled as discretizations of a multivariate normal vector. In contrast to the continuous case, where SEM estimation is also consistent under non-normality, violation of underlying normality in ordinal SEM may lead to inconsistent estimation. In this article, we illustrate how underlying non-normality induces bias in polychoric estimates and their standard errors. This bias is strongly affected by how we discretize. It is therefore important to consider tests of underlying multivariate normality. In this study we propose a parametric bootstrap test for this purpose. Its performance relative to the test of Maydeu-Olivares is evaluated in a Monte Carlo study. At realistic sample sizes, the bootstrap exhibited substantively better Type I error control and power than the Maydeu-Olivares test in ordinal data with ten dimensions or higher. R code for the bootstrap test is provided.},
  keywords = {ordinal data,parametric bootstrap,polychoric correlation,structural equation modeling},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2019.1673168},
  file = {/home/justin/Zotero/storage/7GS33VQC/Foldnes and Grønneberg - 2020 - Pernicious Polychorics The Impact and Detection o.pdf;/home/justin/Zotero/storage/2QVDG2HQ/10705511.2019.html}
}

@article{foldnes2020a,
  title = {Pernicious Polychorics: {{The}} Impact and Detection of Underlying Non-Normality},
  author = {{Foldnes}},
  year = {2020},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  doi = {10/gjrkdk}
}

@article{ford1986,
  ids = {fordAPPLICATIONEXPLORATORYFACTOR1986},
  title = {The Application of Exploratory Factor Analysis in Applied Psychology: {{A}} Critical Review and Analysis},
  shorttitle = {The Application of Exploratory Factor Analysis in Applied Psychology},
  author = {Ford, J. Kevin and MacCALLUM, Robert C. and Tait, Marianne},
  year = {1986},
  journal = {Personnel Psychology},
  volume = {39},
  number = {2},
  pages = {291--314},
  issn = {1744-6570},
  doi = {10/dg8qs5},
  abstract = {Although factor analysis has been a major contributing factor in advancing psychological research, a systematic assessment of how it has been applied is lacking. For this review we examined the Journal of Applied Psychology, Organizational Behavior and Human Performance, and Personnel Psychology over a ten-year period (1975\textendash 1984) and located 152 studies that employed factor analysis. We then analyzed the choices made by the researchers concerning factor model, retention criteria, rotation, interpretation of factors and other issues relevant to factor analysis. The results indicate that choices made by researchers have generally been poor and that reporting practices have not allowed for informed review, cumulation of results, or replicability. A comparison of results by time interval (1975\textendash 1979; 1980\textendash 1984) revealed minimal differences in choices made or the quality of reporting practices. Suggestions for improving the use of factor analysis and the reporting of results are presented.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1744-6570.1986.tb00583.x},
  file = {/home/justin/Zotero/storage/RSGPI3Y5/Ford et al_1986_The Application of Exploratory Factor Analysis in Applied Psychology.pdf;/home/justin/Zotero/storage/WXS6K9BX/Ford et al. - 1986 - THE APPLICATION OF EXPLORATORY FACTOR ANALYSIS IN .pdf;/home/justin/Zotero/storage/N8XHATV8/j.1744-6570.1986.tb00583.html}
}

@article{forero2009,
  ids = {forero2009b,foreroFactorAnalysisOrdinal2009},
  title = {Factor Analysis with Ordinal Indicators: {{A}} Monte Carlo Study Comparing {{DWLS}} and {{ULS}} Estimation},
  author = {Forero, Carlos G. and {Maydeu-Olivares}, Alberto and {Gallardo-Pujol}, David},
  year = {2009},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {16},
  number = {4},
  eprint = {https://doi.org/10.1080/10705510903203573},
  pages = {625--641},
  publisher = {{Routledge}},
  doi = {10/fqxz8j},
  abstract = {Factor analysis models with ordinal indicators are often estimated using a 3-stage procedure where the last stage involves obtaining parameter estimates by least squares from the sample polychoric correlations. A simulation study involving 324 conditions (1,000 replications per condition) was performed to compare the performance of diagonally weighted least squares (DWLS) and unweighted least squares (ULS) in the procedure's third stage. Overall, both methods provided accurate and similar results. However, ULS was found to provide more accurate and less variable parameter estimates, as well as more precise standard errors and better coverage rates. Nevertheless, convergence rates for DWLS are higher. Our recommendation is therefore to use ULS, and, in the case of nonconvergence, to use DWLS, as this method might converge when ULS does not.},
  date-added = {2020-02-07 10:47:28 -0600},
  date-modified = {2020-02-07 10:47:39 -0600},
  doi2 = {10.1080/10705510903203573},
  file = {/home/justin/Zotero/storage/R4IGR3AV/Forero et al. - 2009 - Factor Analysis with Ordinal Indicators A Monte C.pdf;/home/justin/Zotero/storage/YXABGQT7/Forero et al_2009_Factor Analysis with Ordinal Indicators.pdf;/home/justin/Zotero/storage/TTBIXLY8/10705510903203573.html}
}

@article{formyduval1995,
  title = {A '{{Big Five}}' {{Scoring System}} for the {{Item Pool}} of the {{Adjective Check List}}},
  author = {FormyDuval, Deborah L. and Williams, John E. and Patterson, Donna J. and Fogle, Ellen E.},
  year = {1995},
  month = aug,
  journal = {Journal of Personality Assessment},
  volume = {65},
  number = {1},
  pages = {59--76},
  publisher = {{Routledge}},
  issn = {0022-3891},
  doi = {10/btpjzj},
  abstract = {The item pool of the Adjective Check List (ACL; Gough \& Heilbrun, 1980) is widely used as a means of capturing the personal characteristics associated with various target groups (e.g., women vs. men, young adults vs. old adults). The purpose of this research was to develop a system for scoring the ACL items in terms of the five-factor model of personality. In Study 1, five groups of introductory psychology students served as judges, with each group of approximately 100 persons rating the 300 ACL items for one of the five factors. The ratings of each factor were highly reliable. When corrected for favorability, the intercorrelations among the five factors were quite low, as expected, except for the positive correlation of Openness and Extraversion. Good convergence was found between our ratings and the indicative and counterindicative items identified by John's (1989) graduate student judges. In Study 2, convergent validity was demonstrated between the five-factor scores obtained from self-descriptive ACLs and corresponding factor scores obtained from Costa and McCrae's NEO-PI-R and NEO-FFI instruments (Costa \& McCrae, 1992). Data from earlier cross-cultural studies of gender and age stereotypes were rescored using the new ACL-FF system to illustrate its potential utility as a research tool.},
  pmid = {16367646},
  annotation = {\_eprint: https://doi.org/10.1207/s15327752jpa6501\_5},
  file = {/home/justin/Zotero/storage/7RKC8UDP/s15327752jpa6501_5.html}
}

@article{francois-lavet2018,
  title = {An {{Introduction}} to {{Deep Reinforcement Learning}}},
  author = {{Francois-Lavet}, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
  year = {2018},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {11},
  number = {3-4},
  eprint = {1811.12560},
  eprinttype = {arxiv},
  pages = {219--354},
  issn = {1935-8237, 1935-8245},
  doi = {10/gfs4bc},
  abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/justin/Zotero/storage/JTT5PU42/Francois-Lavet et al_2018_An Introduction to Deep Reinforcement Learning.pdf;/home/justin/Zotero/storage/AFY77NSI/1811.html}
}

@inproceedings{frane1978,
  title = {Missing Data and {{BMDP}}: {{Some}} Pragmatic Approaches},
  booktitle = {Proceedings of the {{Statistical Computing Section}}},
  author = {Frane, James W},
  year = {1978},
  number = {4},
  pages = {27--33},
  publisher = {{American Statistical Association}},
  address = {{Washington, DC}},
  organization = {{INTERNATIONAL BIOMETRIC SOC 808 17TH ST NW SUITE 200, WASHINGTON, DC 20006-3910}},
  keywords = {⛔ No DOI found}
}

@article{fraser1988,
  title = {{{NOHARM}}: {{Least Squares Item Factor Analysis}}},
  shorttitle = {{{NOHARM}}},
  author = {Fraser, Colin and McDonald, Roderick P.},
  year = {1988},
  journal = {Multivariate Behavioral Research},
  volume = {23},
  number = {2},
  pages = {267--269},
  issn = {0027-3171, 1532-7906},
  doi = {10/chcbbj},
  langid = {english},
  file = {/home/justin/Zotero/storage/VA44RFIA/Fraser and McDonald - 1988 - NOHARM Least Squares Item Factor Analysis.pdf}
}

@article{frigessi2009,
  title = {Rehabilitation of Improper Correlation Matrices},
  author = {Frigessi, Arnoldo and L{\o}land, Anders and Pievatolo, Antonio and Ruggeri, Fabrizio},
  year = {2009},
  keywords = {⛔ No DOI found,matrix smooth}
}

@article{furlow2005,
  title = {Meta-Analytic Methods of Pooling Correlation Matrices for Structural Equation Modeling under Different Patterns of Missing Data.},
  author = {Furlow, Carolyn F and Beretvas, S Natasha},
  year = {2005},
  journal = {Psychological Methods},
  volume = {10},
  number = {2},
  pages = {227},
  publisher = {{American Psychological Association}},
  doi = {10/b8hfx3}
}

@article{fushiki2009,
  ids = {fushikiEstimationPositiveSemidefinite2009},
  title = {Estimation of Positive Semidefinite Correlation Matrices by Using Convex Quadratic Semidefinite Programming},
  author = {Fushiki, Tadayoshi},
  year = {2009},
  journal = {Neural Computation},
  volume = {21},
  number = {7},
  pages = {2028--2048},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10/drcmxx},
  abstract = {The correlation matrix is a fundamental statistic that used in many fields. For example, GroupLens, a collaborative filtering system, uses the correlation between users for predictive purposes. Since the correlation is a natural similarity measure between users, the correlation matrix may be used as the Gram matrix in kernel methods. However, the estimated correlation matrix sometimes has a serious defect: although the correlation matrix is originally positive semidefinite, the estimated one may not be positive semidefinite when not all ratings are observed. To obtain a positive semidefinite correlation matrix, the nearest correlation matrix problem has recently been studied in the fields of numerical analysis and optimization. However, statistical properties are not explicitly used in such studies. To obtain a positive semidefinite correlation matrix, we assume an approximate model. By using the model, an estimate is obtained as the optimal point of an optimization problem formulated with information on the variances of the estimated correlation coefficients. The problem is solved by a convex quadratic semidefinite program. A penalized likelihood approach is also examined. The MovieLens data set is used to test our approach.},
  date-added = {2019-12-11 11:58:04 -0600},
  date-modified = {2019-12-11 11:58:04 -0600},
  keywords = {cor smooth,matrix smooth},
  file = {/home/justin/Zotero/storage/CH2N78ET/neco.2009.html}
}

@book{gana2019,
  ids = {2018},
  title = {Structural Equation Modeling with Lavaan},
  author = {Gana, Kamel and Broc, Guillaume},
  year = {2019},
  series = {Mathematics and Statistics},
  publisher = {{ISTE Ltd ; John Wilery \& Sons, Inc}},
  address = {{London : Hoboken, NJ}},
  doi = {10.1002/9781119579038},
  isbn = {978-1-78630-369-1},
  lccn = {QA278.3 .G36 2019},
  keywords = {Software,Structural equation modeling},
  annotation = {OCLC: on1088907578},
  file = {/home/justin/Zotero/storage/QL8TYCRJ/2018_Structural Equation Modeling with lavaan.pdf}
}

@article{gao2019,
  title = {Location-{{Centered House Price Prediction}}: {{A Multi-Task Learning Approach}}},
  shorttitle = {Location-{{Centered House Price Prediction}}},
  author = {Gao, Guangliang and Bao, Zhifeng and Cao, Jie and Qin, A. K. and Sellis, Timos and Fellow and IEEE and Wu, Zhiang},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.01774 [cs, stat]},
  eprint = {1901.01774},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Accurate house prediction is of great significance to various real estate stakeholders such as house owners, buyers, investors, and agents. We propose a location-centered prediction framework that differs from existing work in terms of data profiling and prediction model. Regarding data profiling, we define and capture a fine-grained location profile powered by a diverse range of location data sources, such as transportation profile (e.g., distance to nearest train station), education profile (e.g., school zones and ranking), suburb profile based on census data, facility profile (e.g., nearby hospitals, supermarkets). Regarding the choice of prediction model, we observe that a variety of approaches either consider the entire house data for modeling, or split the entire data and model each partition independently. However, such modeling ignores the relatedness between partitions, and for all prediction scenarios, there may not be sufficient training samples per partition for the latter approach. We address this problem by conducting a careful study of exploiting the Multi-Task Learning (MTL) model. Specifically, we map the strategies for splitting the entire house data to the ways the tasks are defined in MTL, and each partition obtained is aligned with a task. Furthermore, we select specific MTL-based methods with different regularization terms to capture and exploit the relatedness between tasks. Based on real-world house transaction data collected in Melbourne, Australia. We design extensive experimental evaluations, and the results indicate a significant superiority of MTL-based methods over state-of-the-art approaches. Meanwhile, we conduct an in-depth analysis on the impact of task definitions and method selections in MTL on the prediction performance, and demonstrate that the impact of task definitions on prediction performance far exceeds that of method selections.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/justin/Zotero/storage/W9HMIHCA/Gao et al. - 2019 - Location-Centered House Price Prediction A Multi-.pdf;/home/justin/Zotero/storage/IWMZQI7Y/1901.html}
}

@article{garner1975,
  title = {Effect of {{pH}} on Substrate and Inhibitor Kinetic Constants of Human Liver Alanine Aminopeptidase. {{Evidence}} for Two Ionizable Active Center Groups},
  author = {Garner, C. W. and Behal, F. J.},
  year = {1975},
  month = nov,
  journal = {Biochemistry},
  volume = {14},
  number = {23},
  pages = {5084--5088},
  issn = {0006-2960},
  doi = {10/cdj8g4},
  abstract = {The presence of at least two ionizable active center groups has been detected by a study of the effect of pH upon catalysis of hydrolysis of L-alanyl-beta-naphthylamide by human liver alanine aminopeptidase and upon the inhibition of hydrolysis by inhibitors and substrate analogs. Octanoic acid, octylamine, and peptide inhibitors have been found to be competitive inhibitors and are therefore thought to bind the active center. L-Phe was previously shown to bind the active center since it was found to be a competitive inhibitor of the hydrolysis of tripeptide substrates (Garner, C. W., and Behal, F. J. (1975), Biochemistry 14, 3208). A plot of pKm vs. pH for the substrate L-Ala-beta-naphthylamide showed that binding decreased below pH 5.9 and above 7.5, the points at which the theoretical curve undergoes an integral change in slope. These points are interpreted as the pKa either of substrate ionizable groups or binding-dependent enzyme active center groups. Similar plots of pKm vs. pH for L-alanyl-p-nitroanilide (as substrate) and pKi vs. pH for L-Leu-L-Leu-L-Leu and D-Leu-L-Tyr (as inhibitors) gave pairs fo pKa values of 5.8 and 7.4, 6.0 and 7.5, and 5.7 and 7.5, respectively. All the above substrates (and D-Leu-L-Tyr) have pKa values near 7.5; therefore, the binding-dependent group with a pKa value near 7.5 is possibly this substrate group. Similar plots of pKi vs. pH for the inhibitors L-Phe, L-Met, L-Leu, octylamine, and octanoic acid had only one bending point at 7.7, 7.6, 7.4, 6.3, and 5.9, respectively. Amino acid inhibitors, octylamine, and octanoic acid have no groups with pKa values between 5 and 9. These data indicate that there are two active center ionizable groups with pKa values of approximately 6.0 and 7.5 which are involved in substrate binding or inhibitory amino acid binding but not in catalysis since Vmax was constant at all pH values tested.},
  langid = {english},
  pmid = {38},
  keywords = {Alanine,Aminopeptidases,Binding Sites,Humans,Hydrogen-Ion Concentration,Kinetics,Liver,Mathematics,Protein Binding,Structure-Activity Relationship,Thermodynamics}
}

@article{garrido2013,
  ids = {garrido2013a},
  title = {A New Look at {{Horn}}'s Parallel Analysis with Ordinal Variables.},
  author = {Garrido, Luis Eduardo and Abad, Francisco Jos{\'e} and Ponsoda, Vicente},
  year = {2013},
  journal = {Psychological Methods},
  volume = {18},
  number = {4},
  pages = {454--474},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10/f5p4vx},
  abstract = {Previous research evaluating the performance of Horn's parallel analysis (PA) factor retention method with ordinal variables has produced unexpected findings. Specifically, PA with Pearson correlations has performed as well as or better than PA with the more theoretically appropriate polychoric correlations. Seeking to clarify these findings, the current study employed a more comprehensive simulation study that included the systematic manipulation of 7 factors related to the data (sample size, factor loading, number of variables per factor, number of factors, factor correlation, number of response categories, and skewness) as well as 3 factors related to the PA method (type of correlation matrix, extraction method, and eigenvalue percentile). The results from the simulation study show that PA with either Pearson or polychoric correlations is particularly sensitive to the sample size, factor loadings, number of variables per factor, and factor correlations. However, whereas PA with polychorics is relatively robust to the skewness of the ordinal variables, PA with Pearson correlations frequently retains difficulty factors and is generally inaccurate with large levels of skewness. In light of these findings, we recommend the use of PA with polychoric correlations for the dimensionality assessment of ordinal-level data. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {*Exploratory Factor Analysis,*Factor Analysis,*Principal Component Analysis,*Statistical Correlation,*Statistical Variables,Factor Structure},
  file = {/home/justin/Zotero/storage/LY3MSD65/Garrido et al. - 2013 - A new look at Horn’s parallel analysis with ordina.pdf}
}

@misc{gelman2008,
  title = {Some Thoughts on the Saying, ``{{All}} Models Are Wrong, but Some Are Useful''},
  author = {Gelman, Andrew},
  year = {2008},
  journal = {Statistical Modeling, Causal Inference, and Social Science},
  howpublished = {https://statmodeling.stat.columbia.edu/2008/06/12/all\_models\_are/},
  file = {/home/justin/Zotero/storage/H7BWUTNY/all_models_are.html}
}

@article{gerbing1987,
  title = {Improper Solutions in the Analysis of Covariance Structures: {{Their}} Interpretability and a Comparison of Alternate Respecifications},
  shorttitle = {Improper Solutions in the Analysis of Covariance Structures},
  author = {Gerbing, David W. and Anderson, James C.},
  year = {1987},
  month = mar,
  journal = {Psychometrika},
  volume = {52},
  number = {1},
  pages = {99--111},
  issn = {0033-3123, 1860-0980},
  doi = {10/ft7tk2},
  langid = {english},
  file = {/home/justin/Zotero/storage/K2N8BJ2F/Gerbing and Anderson - 1987 - Improper solutions in the analysis of covariance s.pdf}
}

@article{geyer2013,
  title = {Big {{Oh Pee}} and {{Little Oh Pee}}},
  author = {Geyer, Charles J},
  year = {2013},
  month = jan,
  pages = {3},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/justin/Zotero/storage/YDJ5RW4T/Geyer - Big Oh Pee and Little Oh Pee.pdf}
}

@article{gnambs2016,
  title = {Parameter Accuracy in Meta-Analyses of Factor Structures},
  author = {Gnambs, Timo and Staufenbiel, Thomas},
  year = {2016},
  journal = {Research Synthesis Methods},
  volume = {7},
  number = {2},
  pages = {168--186},
  issn = {1759-2887},
  doi = {10/gcz6x6},
  abstract = {Two new methods for the meta-analysis of factor loadings are introduced and evaluated by Monte Carlo simulations. The direct method pools each factor loading individually, whereas the indirect method synthesizes correlation matrices reproduced from factor loadings. The results of the two simulations demonstrated that the accuracy of meta-analytical derived factor loadings is primarily affected by characteristics of the pooled factor structures (e.g., model error, communality) and to a lesser degree by the sample size of the primary studies and the number of included samples. The choice of the meta-analytical method had a minor impact. In general, the indirect method produced somewhat less biased estimates, particularly for small-sample studies. Thus, the indirect method presents a viable alternative for the meta-analysis of factor structures that could also address moderator hypotheses. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {bias,factor analysis,factor congruence,factor loadings,meta-analysis},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1190},
  file = {/home/justin/Zotero/storage/PHJQQVBZ/Gnambs_Staufenbiel_2016_Parameter accuracy in meta-analyses of factor structures.pdf;/home/justin/Zotero/storage/ZH65XMYK/jrsm.html}
}

@article{golub1965,
  title = {Numerical Methods for Solving Linear Least Squares Problems},
  author = {Golub, Gene},
  year = {1965},
  journal = {Numerische Mathematik},
  volume = {7},
  number = {3},
  pages = {206--216},
  doi = {10/bmgvq9}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT press}}
}

@book{gough1983,
  title = {The {{Adjective Check List Manual}}},
  author = {Gough, Harrison G. and Heilbrun, Alfred B.},
  year = {1983},
  publisher = {{Consulting Psychologists Press}},
  googlebooks = {sRIRAQAAIAAJ},
  langid = {english}
}

@article{graham2007,
  title = {How Many Imputations Are Really Needed? {{Some}} Practical Clarifications of Multiple Imputation Theory},
  author = {Graham, John W and Olchowski, Allison E and Gilreath, Tamika D},
  year = {2007},
  journal = {Prevention science},
  volume = {8},
  number = {3},
  pages = {206--213},
  publisher = {{Springer}},
  doi = {10/ftjnkm}
}

@article{graham2009,
  title = {Missing {{Data Analysis}}: {{Making It Work}} in the {{Real World}}},
  author = {Graham, John W.},
  year = {2009},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {60},
  number = {1},
  pages = {549--576},
  issn = {1545-2085},
  doi = {10/dwh9qg}
}

@book{grahamMissingDataAnalysis2012,
  title = {Missing {{Data}}: {{Analysis}} and {{Design}}},
  author = {Graham, John W.},
  year = {2012},
  publisher = {{Springer}}
}

@article{green2016,
  ids = {green2016a},
  title = {Accuracy of {{Revised}} and {{Traditional Parallel Analyses}} for {{Assessing Dimensionality}} with {{Binary Data}}},
  author = {Green, Samuel B. and Redell, Nickalus and Thompson, Marilyn S. and Levy, Roy},
  year = {2016},
  month = feb,
  journal = {Educational and Psychological Measurement},
  volume = {76},
  number = {1},
  pages = {5--21},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10/gd59jb},
  abstract = {Parallel analysis (PA) is a useful empirical tool for assessing the number of factors in exploratory factor analysis. On conceptual and empirical grounds, we argue for a revision to PA that makes it more consistent with hypothesis testing. Using Monte Carlo methods, we evaluated the relative accuracy of the revised PA (R-PA) and traditional PA (T-PA) methods for factor analysis of tetrachoric correlations between items with binary responses. We manipulated five data generation factors: number of observations, type of factor model, factor loadings, correlation between factors, and distribution of thresholds. The R-PA method tended to be more accurate than T-PA, although not uniformly across conditions. R-PA tended to perform better relative to T-PA if the underlying model (a) was unidimensional but had some unique items, (b) had highly correlated factors, or (c) had a general factor as well as a group factor. In addition, R-PA tended to outperform T-PA if items had higher factor loadings and sample size was large. A major disadvantage of the T-PA method was that it frequently yielded inflated Type I error rates.},
  langid = {english},
  file = {/home/justin/Zotero/storage/HJW789NA/Green et al. - 2016 - Accuracy of Revised and Traditional Parallel Analy.pdf;/home/justin/Zotero/storage/TWI737E8/Green et al. - 2016 - Accuracy of Revised and Traditional Parallel Analy.pdf}
}

@article{grone1990,
  title = {Extremal Correlation Matrices},
  author = {Grone, Robert and Pierce, Stephen and Watkins, William},
  year = {1990},
  journal = {Linear Algebra and its Applications},
  volume = {134},
  pages = {63--70},
  doi = {10/bb769g},
  keywords = {correlation geometry,correlations,matrix smooth,smooth matrix}
}

@article{grubisic2007,
  title = {Efficient Rank Reduction of Correlation Matrices},
  author = {Grubi{\v s}i{\'c}, Igor and Pietersz, Raoul},
  year = {2007},
  journal = {Linear algebra and its applications},
  volume = {422},
  number = {2},
  pages = {629--653},
  doi = {10/dw7jgd},
  keywords = {matrix smooth}
}

@article{guadagnoli1988,
  ids = {guadagnoli1988a},
  title = {Relation of Sample Size to the Stability of Component Patterns.},
  author = {Guadagnoli, Edward and Velicer, Wayne F.},
  year = {1988},
  month = mar,
  journal = {Psychological Bulletin},
  volume = {103},
  number = {2},
  pages = {265--275},
  issn = {1939-1455, 0033-2909},
  doi = {10/d9sg2q},
  langid = {english},
  file = {/home/justin/Zotero/storage/6R8ZJIXA/Guadagnoli_Velicer_1988_Relation of sample size to the stability of component patterns.pdf}
}

@book{gupta2000,
  title = {Matrix Variate Distributions},
  author = {Gupta, A. K and Nagar, D. K},
  year = {2000},
  publisher = {{Chapman \& Hall}},
  address = {{Boca Raton, FL}},
  abstract = {Useful in physics, economics, psychology, and other fields, random matrices play an important role in the study of multivariate statistical methods. Until now, however, most of the material on random matrices could only be found scattered in various statistical journals. Matrix Variate Distributions gathers and systematically presents most of the recent developments in continuous matrix variate distribution theory and includes new results. After a review of the essential background material, the authors investigate the range of matrix variate distributions, including: matrix variate normal distribution; Wishart distribution; Matrix variate t-distribution; Matrix variate beta distribution; F-distribution; Matrix variate Dirichlet distribution; Matrix quadratic forms; With its inclusion of new results, Matrix Variate Distributions promises to stimulate further research and help advance the field of multivariate statistical analysis.},
  isbn = {978-1-58488-046-2},
  langid = {english},
  annotation = {OCLC: 42060983},
  file = {/home/justin/Zotero/storage/HWBVCJH2/Gupta_Nagar_2000_Matrix variate distributions.djvu}
}

@book{hair2018,
  title = {Multivariate {{Data Analysis}}},
  author = {Hair, Joseph F. and Black, William C. and Babin, Barry J. and Anderson, Rolph E.},
  year = {2018},
  publisher = {{Cengage}},
  abstract = {For over 30 years, this text has provided students with the information they need to understand and apply multivariate data analysis. The eighth edition of Multivariate Data Analysis provides an updated perspective on the analysis of all types of data as well as introducing some new perspectives and techniques that are foundational in today's world of analytics.Multivariate Data Analysis serves as the perfect companion for graduate and postgraduate students undertaking statistical analysis for business degrees, providing an application-oriented introduction to multivariate analysis for the non-statistician. By reducing heavy statistical research into fundamental concepts, the text explains to students how to understand and make use of the results of specific statistical techniques.},
  abstractnote = {For over 30 years, this text has provided students with the information they need to understand and apply multivariate data analysis. The eighth edition of Multivariate Data Analysis provides an updated perspective on the analysis of all types of data as well as introducing some new perspectives and techniques that are foundational in today's world of analytics.Multivariate Data Analysis serves as the perfect companion for graduate and postgraduate students undertaking statistical analysis for business degrees, providing an application-oriented introduction to multivariate analysis for the non-statistician. By reducing heavy statistical research into fundamental concepts, the text explains to students how to understand and make use of the results of specific statistical techniques.},
  googlebooks = {0R9ZswEACAAJ},
  isbn = {978-1-4737-5654-0},
  langid = {english},
  file = {/home/justin/Zotero/storage/HY7PR2RJ/Hair et al. - 2018 - Multivariate Data Analysis.pdf}
}

@article{hakstian1982,
  ids = {hakstian1982a},
  title = {The Behavior of Number-of-Factors Rules with Simulated Data},
  author = {Hakstian, A. Ralph and Rogers, W. Todd and Cattell, Raymond B.},
  year = {1982},
  month = apr,
  journal = {Multivariate Behavioral Research},
  volume = {17},
  number = {2},
  pages = {193--219},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/d68vwz},
  abstract = {issues related to the decision of the number of factors to retain in factor analysis are identified, and three widely-used decision rules \textendash{} the Kaiser-Guttman, scree, and likelihood ratio tests \textendash{} are isolated for empirical study. Using two differing structural models and incorporating a number of relevant independent variables (such as number of variables, ratio of number of factors to number of variables, variable communality levels, and factorial complexity), the authors simulated 144 population data sets and, then, from these, 288 sample data sets, each with a precisely known (or incorporated) number of factors. The Kaiser-Guttman and scree rules were applied to the population data in Part I of the study, and all three rules were applied to the sample data sets in Part II. Overall trends and interactive results, in terms of the independent variables examined, are discussed in detail, and methods are presented for assessing the quality of the number-of-factors indicated by a particular rule.},
  pmid = {26810948},
  annotation = {\_eprint: https://doi.org/10.1207/s15327906mbr1702\_3},
  file = {/home/justin/Zotero/storage/4TWPCDFY/Hakstian et al_1982_The Behavior Of Number-Of-Factors Rules With Simulated Data.pdf;/home/justin/Zotero/storage/W78K2D6N/Hakstian et al_1982_The Behavior Of Number-Of-Factors Rules With Simulated Data.pdf;/home/justin/Zotero/storage/5XVPMM4I/s15327906mbr1702_3.html;/home/justin/Zotero/storage/KHWEKFKF/s15327906mbr1702_3.html}
}

@article{han2019,
  ids = {han2019a},
  title = {Methodological and Statistical Advances in the Consideration of Cultural Diversity in Assessment: {{A}} Critical Review of Group Classification and Measurement Invariance Testing.},
  shorttitle = {Methodological and Statistical Advances in the Consideration of Cultural Diversity in Assessment},
  author = {Han, Kyunghee and Colarelli, Stephen M. and Weed, Nathan C.},
  year = {2019},
  month = dec,
  journal = {Psychological Assessment},
  volume = {31},
  number = {12},
  pages = {1481--1496},
  issn = {1939-134X, 1040-3590},
  doi = {10/fh8d},
  langid = {english}
}

@book{hancock2019,
  title = {The Reviewer's Guide to Quantitative Methods in the Social Sciences},
  editor = {Hancock, Gregory R. and Stapleton, Laura M. and Mueller, Ralph O.},
  year = {2019},
  edition = {Second Edition},
  publisher = {{Routledge}},
  address = {{New York}},
  isbn = {978-1-315-75564-9},
  lccn = {H62},
  keywords = {Research Methodology,SEM,Social sciences,Statistical methods},
  file = {/home/justin/Zotero/storage/HK74Z9YB/Hancock et al_2019_The reviewer's guide to quantitative methods in the social sciences.pdf}
}

@article{harman1966,
  ids = {harman},
  title = {Factor Analysis by Minimizing Residuals ({{MINRES}})},
  author = {Harman, Harry H. and Jones, Wayne H.},
  year = {1966},
  journal = {Psychometrika},
  volume = {31},
  number = {3},
  pages = {351--368},
  issn = {0033-3123, 1860-0980},
  doi = {10/d957xh},
  langid = {english},
  file = {/home/justin/Zotero/storage/SH47I9PW/Factor analysis by minimizing residuals (minres).pdf}
}

@book{hastie2009,
  title = {The Elements of Statistical Learning: {{Data}} Mining, Inference, and Prediction},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  series = {Springer Series in Statistics},
  edition = {Second Edition},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  isbn = {978-0-387-84857-0},
  langid = {english},
  keywords = {Artificial Intelligence (Incl. Robotics),Computational Biology/Bioinformatics,Computer Appl. in Life Sciences,Computer Science,Data Mining and Knowledge Discovery,Mathematics,Probability Theory and Stochastic Processes,Statistical Theory and Methods}
}

@article{hayton2004,
  ids = {haytonFactorRetentionDecisions2004},
  title = {Factor {{Retention Decisions}} in {{Exploratory Factor Analysis}}: A {{Tutorial}} on {{Parallel Analysis}}},
  shorttitle = {Factor {{Retention Decisions}} in {{Exploratory Factor Analysis}}},
  author = {Hayton, James C. and Allen, David G. and Scarpello, Vida},
  year = {2004},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {7},
  number = {2},
  pages = {191--205},
  publisher = {{SAGE Publications Inc}},
  issn = {1094-4281},
  doi = {10/fq4866},
  abstract = {The decision of how many factors to retain is a critical component of exploratory factor analysis. Evidence is presented that parallel analysis is one of the most accurate factor retention methods while also being one of the most underutilized in management and organizational research. Therefore, a step-by-step guide to performing parallel analysis is described, and an example is provided using data from the Minnesota Satisfaction Questionnaire. Recommendations for making factor retention decisions are discussed.},
  langid = {english},
  file = {/home/justin/Zotero/storage/5S8Q4D8X/Hayton et al. - 2004 - Factor Retention Decisions in Exploratory Factor A.pdf}
}

@article{heffernan2017,
  title = {Psychological Contract Breach and Turnover Intention: The Moderating Effects of Social Status and Local Ties.},
  shorttitle = {Psychological Contract Breach and Turnover Intention},
  author = {Heffernan, Margaret and Rochford, Eoin},
  year = {2017},
  journal = {The Irish Journal of Management},
  volume = {36},
  number = {2},
  pages = {99--115},
  publisher = {{De Gruyter}},
  issn = {1649-248X},
  doi = {10/gj733k},
  abstract = {The aim of this study is to examine whether social networks reduce the effects of psychological contract breach on an employee's intention to leave the organisation. This paper focuses on two particular elements of the social network in an organisation: (1) social status and (2) local ties/connectedness. Using a sample of 242 responses from officers in the Irish Defence Forces, the results provide empirical evidence of the impact of psychological contract breach on officer turnover intentions. The results also demonstrate that perceptions of social networks moderate the relationship between psychological contract breach and turnover intentions through social status. Contrary to expectations, strong connectedness with senior officers was also found to be a moderator but not in the direction that was hypothesised. The theoretical and practical implications of these results are discussed.},
  copyright = {\textcopyright{} 2017  De Gruyter Open Sp. z o.o},
  langid = {english},
  file = {/home/justin/Zotero/storage/7CJDRXYY/Heffernan_Rochford_2017_Psychological contract breach and turnover intention.pdf}
}

@article{helbich2013,
  title = {Boosting the Predictive Accuracy of Urban Hedonic House Price Models through Airborne Laser Scanning},
  author = {Helbich, Marco and Jochem, Andreas and M{\"u}cke, Werner and H{\"o}fle, Bernhard},
  year = {2013},
  month = may,
  journal = {Computers, Environment and Urban Systems},
  volume = {39},
  pages = {81--92},
  issn = {0198-9715},
  doi = {10/f429nb},
  abstract = {This paper introduces an integrative approach to hedonic house price modeling which utilizes high density 3D airborne laser scanning (ALS) data. In general, it is shown that extracting exploratory variables using 3D analysis \textendash{} thus explicitly considering high-rise buildings, shadowing effects, etc. \textendash{} is crucial in complex urban environments and is limited in well-established raster-based modeling. This is fundamental in large-scale urban analyses where essential determinants influencing real estate prices are constantly missing and are not accessible in official and mass appraiser databases. More specifically, the advantages of this methodology are demonstrated by means of a novel and economically important externality, namely incoming solar radiation, derived separately for each flat. Findings from an empirical case study in Vienna, Austria, applying a non-linear generalized additive hedonic model, suggest that solar radiation is significantly capitalized in flat prices. A model comparison clearly proves that the hedonic model accounting for ALS-based solar radiation performs significantly superior. Compared to a model without this externality, it increases the model's explanatory power by approximately 13\% and additionally reduces the prediction error by around 15\%. The results provide strong evidence that explanatory variables originating from ALS, explicitly regarding the immediate 3D surroundings, enhance traditional hedonic models in urban environments.},
  langid = {english},
  keywords = {Airborne laser scanning,Generalized additive model,GIS,Hedonic regression,LiDAR,Real estate,Solar radiation,Vienna (Austria)},
  file = {/home/justin/Zotero/storage/MISJQZF5/Helbich et al. - 2013 - Boosting the predictive accuracy of urban hedonic .pdf;/home/justin/Zotero/storage/IHCLHZ8H/S0198971513000021.html}
}

@book{henshall,
  title = {{{PDMATRIX}}\textendash{{PROGAMS TO MAKE MATRICES POSITIVE DEFINITE}}},
  author = {Henshall, John M and Meyer, Karin},
  publisher = {{Citeseer}},
  keywords = {matrix smooth}
}

@incollection{herting1985,
  title = {Respecification in Multiple Indicator Models},
  booktitle = {Causal Models in the Social Sciences},
  author = {Herting, Jerald R. and Costner, Herbert L.},
  editor = {Blalock, Hubert M},
  year = {1985},
  edition = {2nd ed..},
  publisher = {{New York : Aldine Pub. Co.}},
  address = {{New York}},
  isbn = {0-202-30313-6},
  keywords = {Social sciences -- Mathematical models,Social sciences -- Methodology},
  file = {/home/justin/Zotero/storage/48PFJHTI/Blalock_1985_Causal models in the social sciences.pdf}
}

@article{hetherington1975,
  title = {Two-, {{Three-}}, and {{Four-Atom Exchange Effects}} in Bcc \$\^\{3\}\textbackslash mathrm\{\vphantom\}{{He}}\vphantom\{\}\$},
  author = {Hetherington, J. H. and Willard, F. D. C.},
  year = {1975},
  month = nov,
  journal = {Physical Review Letters},
  volume = {35},
  number = {21},
  pages = {1442--1444},
  publisher = {{American Physical Society}},
  doi = {10/d7rz9d},
  abstract = {We have made mean-field calculations with a Hamiltonian obtained from two-, three-, and four-atom exchange in bcc solid 3He. We are able to fit the high-temperature experiments as well as the phase diagram of Kummer et al. at low temperatures. We find two kinds of antiferromagnetic phases as suggested by Kummer's experiments.},
  file = {/home/justin/Zotero/storage/XYZQ66WH/Hetherington_Willard_1975_Two-, Three-, and Four-Atom Exchange Effects in bcc $^ 3 -mathrm He $.pdf}
}

@article{higham1988,
  title = {Computing a Nearest Symmetric Positive Semidefinite Matrix},
  author = {Higham, Nicholas J},
  year = {1988},
  journal = {Linear algebra and its applications},
  volume = {103},
  pages = {103--118},
  doi = {10/dj45fb}
}

@article{higham1988a,
  title = {Matrix Nearness Problems and Applications},
  author = {Higham, NJ},
  year = {1988},
  keywords = {⛔ No DOI found,matrix smooth}
}

@article{higham1993,
  title = {Optimization by Direct Search in Matrix Computations},
  author = {Higham, Nicholas J},
  year = {1993},
  journal = {SIAM journal on matrix analysis and applications},
  volume = {14},
  number = {2},
  pages = {317--333},
  doi = {10/crmt32},
  keywords = {smooth matrix}
}

@article{higham2002,
  ids = {highamComputingNearestCorrelation2002a},
  title = {Computing the Nearest Correlation Matrix\textemdash a Problem from Finance},
  author = {Higham, Nicholas J.},
  year = {2002},
  journal = {IMA Journal of Numerical Analysis},
  volume = {22},
  number = {3},
  pages = {329--343},
  publisher = {{Oxford University Press}},
  issn = {1464-3642},
  doi = {10/d8gj5f},
  abstract = {Given a symmetric matrix, what is the nearest correlation matrix\textemdash that is, the nearest symmetric positive semidefinite matrix with unit diagonal? This problem arises in the finance industry, where the correlations are between stocks. For distance measured in two weighted Frobenius norms we characterize the solution using convex analysis. We show how the modified alternating projections method can be used to compute the solution for the more commonly used of the weighted Frobenius norms. In the finance application the original matrix has many zero or negative eigenvalues; we show that for a certain class of weights the nearest correlation matrix has correspondingly many zero eigenvalues and that this fact can be exploited in the computation.},
  date-added = {2019-12-11 11:45:40 -0600},
  date-modified = {2019-12-11 11:45:40 -0600},
  keywords = {alternating projections method,convex analysis,correlation matrix,nearness problem,positive semidefinite matrix,semidefinite programming,weighted Frobenius norm},
  file = {/home/justin/Zotero/storage/ZMK653NC/Higham - 2002 - Computing the nearest correlation matrix—a problem.pdf;/home/justin/Zotero/storage/4D7EBBZ6/8144941.html}
}

@article{higham2015,
  title = {Bounds for the {{Distance}} to the {{Nearest Correlation Matrix}}},
  author = {Higham, Nicholas J and Strabi{\'c}, Nata{\v s}a},
  year = {2015},
  keywords = {⛔ No DOI found,matrix smooth}
}

@article{higham2015a,
  title = {Anderson Acceleration of the Alternating Projections Method for Computing the Nearest Correlation Matrix},
  author = {Higham, Nicholas J and Strabi{\'c}, Nata{\v s}a},
  year = {2015},
  journal = {Numerical Algorithms},
  pages = {1--22},
  keywords = {⛔ No DOI found,alternating projection methods,anderson acceleration,MAP,matrix smooth}
}

@article{higham2016,
  title = {Restoring Definiteness via Shrinking, with an Application to Correlation Matrices with a Fixed Block},
  author = {Higham, Nicholas J and Strabic, Natasa and Sego, Vedran},
  year = {2016},
  journal = {SIAM Review},
  volume = {58},
  number = {2},
  pages = {245--263},
  doi = {10/f8x3t6},
  keywords = {correlation,matrix smooth,smooth matrix}
}

@article{hill1987,
  title = {On the Cone of Positive Semidefinite Matrices},
  author = {Hill, Richard D and Waters, Steven R},
  year = {1987},
  journal = {Linear Algebra and its Applications},
  volume = {90},
  pages = {81--88},
  doi = {10/fmtpt3}
}

@article{hjarne2020,
  title = {Group {{Differences}} in the {{Value}} of {{Subscores}}: {{A Fairness Issue}}},
  shorttitle = {Group {{Differences}} in the {{Value}} of {{Subscores}}},
  author = {Hj{\"a}rne, Marcus Str{\"o}mb{\"a}ck and Lyr{\'e}n, Per-Erik},
  year = {2020},
  month = may,
  journal = {Frontiers in Education},
  volume = {5},
  pages = {55},
  issn = {2504-284X},
  doi = {10/gjd55s},
  file = {/home/justin/Zotero/storage/MT4VAJKH/Hjärne and Lyrén - 2020 - Group Differences in the Value of Subscores A Fai.pdf}
}

@book{hoffman2020,
  title = {Methods for {{Network Analysis}}},
  author = {Hoffman, Mark},
  year = {2020},
  file = {/home/justin/Zotero/storage/9QFM6CKV/centrality.html}
}

@article{hogarty2005,
  title = {The {{Quality}} of {{Factor Solutions}} in {{Exploratory Factor Analysis}}: {{The Influence}} of {{Sample Size}}, {{Communality}}, and {{Overdetermination}}},
  shorttitle = {The {{Quality}} of {{Factor Solutions}} in {{Exploratory Factor Analysis}}},
  author = {Hogarty, Kristine Y. and Hines, Constance V. and Kromrey, Jeffrey D. and Ferron, John M. and Mumford, Karen R.},
  year = {2005},
  month = apr,
  journal = {Educational and Psychological Measurement},
  volume = {65},
  number = {2},
  pages = {202--226},
  issn = {0013-1644, 1552-3888},
  doi = {10/frfnwf},
  langid = {english},
  file = {/home/justin/Zotero/storage/C5CQP7TJ/Hogarty et al. - 2005 - The Quality of Factor Solutions in Exploratory Fac.pdf}
}

@article{holgado-tello2010,
  title = {Polychoric versus {{Pearson}} Correlations in Exploratory and Confirmatory Factor Analysis of Ordinal Variables},
  author = {{Holgado{\textendash}Tello}, Francisco Pablo and {Chac{\'o}n{\textendash}Moscoso}, Salvador and {Barbero{\textendash}Garc{\'i}a}, Isabel and {Vila{\textendash}Abad}, Enrique},
  year = {2010},
  journal = {Quality \& Quantity},
  volume = {44},
  number = {1},
  pages = {153--166},
  issn = {0033-5177, 1573-7845},
  doi = {10/bztdzc},
  langid = {english}
}

@book{holland1975,
  title = {Adaptation in Natural and Artificial Systems: {{An}} Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence},
  author = {Holland, J. H.},
  year = {1975},
  publisher = {{University of Michigan Press}},
  address = {{Ann Arbor}},
  isbn = {0-472-08460-7},
  keywords = {Adaptation (Biology) -- Mathematical models,Adaptation; Biological,Models; Theoretical},
  file = {/home/justin/Zotero/storage/FJMU9955/Holland_1975_Adaptation in natural and artificial systems.pdf}
}

@incollection{holtom2018,
  title = {Job {{Embeddedness Theory}} as a {{Tool}} for {{Improving Employee Retention}}},
  booktitle = {Psychology of {{Retention}}: {{Theory}}, {{Research}} and {{Practice}}},
  author = {Holtom, Brooks C. and Darabi, Tiffany},
  editor = {Coetzee, Melinde and Potgieter, Ingrid L. and Ferreira, Nadia},
  year = {2018},
  pages = {95--117},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-98920-4_5},
  abstract = {In 2001, job embeddedness theory was introduced as a theory explaining why employees stay in organizations. The accumulated empirical results summarized in a compelling meta-analysis point to the predictive value of the theory. Across many contexts (e.g., for profit as well as not for profit, US and international), researchers have found that job embeddedness predicts staying as well as other positive work outcomes such as in-role and extra-role performance. Further, they have found that those who are more embedded are less likely to be absent or engage in counterproductive work behaviors. Recent theoretical elaborations identifying additional antecedents, moderators and consequences of embeddedness, have enriched both researcher and practitioner perspectives on staying. Based on theory and investigation, many practical implications for organizations seeking to enhance job embeddedness and its associated outcomes are advanced.},
  isbn = {978-3-319-98920-4},
  langid = {english},
  keywords = {Counterproductive work behaviors,Job embeddedness,Organizational citizenship behaviors,Performance,Retention,Turnover},
  file = {/home/justin/Zotero/storage/BJ4F7J7S/Holtom_Darabi_2018_Job Embeddedness Theory as a Tool for Improving Employee Retention.pdf}
}

@article{homer1988,
  title = {Using {{LISREL}} Models with Crude Rank Category Measures},
  author = {Homer, Pamela and O'Brien, Robert M.},
  year = {1988},
  journal = {Quality and Quantity},
  volume = {22},
  number = {2},
  pages = {191--201},
  issn = {1573-7845},
  doi = {10/ffrthf},
  abstract = {Recent versions of LISREL (Joresk\"og and Sorbom, 1983) contain procedures for estimating polyserial and polychoric correlations from crude rank category measures. In this paper, the accuracy of these procedures for estimating the relationship between constructs in both a single and a multiple indicator model is compared to that of using Pearsonian correlations based on equal distance scoring of the rank categories. In these comparisons, a variety of multivariate nonnormal distributions were simulated and the average bias and average absolute error of the estimated relationships between underlying constructs were calculated. These estimates were affected by the number of rank categories for each measure, the correlations among measures, their skewness and kurtosis, and the correlation between underlying constructs. The most important finding is that although the polychoric procedure can be helpful in estimating the correlation between unobserved variables in single indicator models, it does not improve estimates based on Pearsonian correlations in the multiple indicator model.},
  langid = {english},
  file = {/home/justin/Zotero/storage/52D3JTQV/Homer_O'Brien_1988_Using LISREL models with crude rank category measures.pdf}
}

@article{hong1999,
  title = {Generating Correlation Matrices with Model Error for Simulation Studies in Factor Analysis: {{A}} Combination of the {{Tucker-Koopman-Linn}} Model and {{Wijsman}}'s Algorithm},
  author = {Hong, Sehee},
  year = {1999},
  journal = {Behavior Research Methods, Instruments, \& Computers},
  volume = {31},
  number = {4},
  pages = {727--730},
  publisher = {{Springer}},
  doi = {10/dsjcnm},
  date-added = {2020-02-20 12:39:15 -0600},
  date-modified = {2020-02-20 12:39:33 -0600},
  file = {/home/justin/Zotero/storage/9CFD9P6F/Hong_1999_Generating correlation matrices with model error for simulation studies in.pdf}
}

@article{hoogland1998,
  title = {Robustness Studies in Covariance Structure Modeling: {{An}} Overview and a Meta-Analysis},
  shorttitle = {Robustness Studies in Covariance Structure Modeling},
  author = {Hoogland, Jeffery J. and Boomsma, Anne},
  year = {1998},
  month = feb,
  journal = {Sociological Methods \& Research},
  volume = {26},
  number = {3},
  pages = {329--367},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10/cbp8p8},
  abstract = {In covariance structure modeling, several estimation methods are available. The robustness of an estimator against specific violations of assumptions can be determined empirically by means of a Monte Carlo study. Many such studies in covariance structure analysis have been published, but the conclusions frequently seem to contradict each other. An overview of robustness studies in covariance structure analysis is given, and an attempt is made to generalize findings. Robustness studies are described and distinguished from each other systematically by means of certain characteristics. These characteristics serve as explanatory variables in a meta-analysis concerning the behavior of parameter estimators, standard error estimators, and goodness-of-fit statistics when the model is correctly specified.},
  langid = {english},
  file = {/home/justin/Zotero/storage/B5QSYXIX/HOOGLAND_BOOMSMA_1998_Robustness Studies in Covariance Structure Modeling.pdf}
}

@article{horn1965,
  title = {A Rationale and Test for the Number of Factors in Factor Analysis},
  author = {Horn, John L.},
  year = {1965},
  journal = {Psychometrika},
  volume = {30},
  number = {2},
  pages = {179--185},
  publisher = {{Springer}},
  issn = {1860-0980},
  doi = {10/b7684f},
  abstract = {It is suggested that if Guttman's latent-root-one lower bound estimate for the rank of a correlation matrix is accepted as a psychometric upper bound, following the proofs and arguments of Kaiser and Dickman, then the rank for a sample matrix should be estimated by subtracting out the component in the latent roots which can be attributed to sampling error, and least-squares ``capitalization'' on this error, in the calculation of the correlations and the roots. A procedure based on the generation of random variables is given for estimating the component which needs to be subtracted.},
  date-added = {2019-12-12 11:35:37 -0600},
  date-modified = {2019-12-12 11:35:37 -0600},
  langid = {english},
  file = {/home/justin/Zotero/storage/P52QCDXZ/Horn - 1965 - A rationale and test for the number of factors in .pdf}
}

@techreport{howe1955,
  type = {Technical {{Report}}},
  title = {Some Contributions to Factor Analysis},
  author = {Howe, W G},
  year = {1955},
  month = aug,
  number = {ONRL-1919},
  address = {{Oak Ridge National Lab., Tenn.}}
}

@article{hsu2015,
  title = {Detecting Misspecified Multilevel Structural Equation Models with Common Fit Indices: {{A Monte Carlo}} Study},
  shorttitle = {Detecting Misspecified Multilevel Structural Equation Models with Common Fit Indices},
  author = {Hsu, Hsien-Yuan and Kwok, Oi-man and Lin, Jr Huang and Acosta, Sandra},
  year = {2015},
  month = mar,
  journal = {Multivariate Behavioral Research},
  volume = {50},
  number = {2},
  pages = {197--215},
  issn = {0027-3171, 1532-7906},
  doi = {10/gg5fk7},
  langid = {english},
  file = {/home/justin/Zotero/storage/YJFDSBTM/Hsu et al_2015_Detecting Misspecified Multilevel Structural Equation Models with Common Fit.pdf}
}

@article{hu1999,
  title = {Cutoff Criteria for Fit Indexes in Covariance Structure Analysis: {{Conventional}} Criteria versus New Alternatives},
  shorttitle = {Cutoff Criteria for Fit Indexes in Covariance Structure Analysis},
  author = {Hu, Li-tze and Bentler, Peter M.},
  year = {1999},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {6},
  number = {1},
  pages = {1--55},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/dbt},
  abstract = {This article examines the adequacy of the ``rules of thumb'' conventional cutoff criteria and several new alternatives for various fit indexes used to evaluate model fit in practice. Using a 2-index presentation strategy, which includes using the maximum likelihood (ML)-based standardized root mean squared residual (SRMR) and supplementing it with either Tucker-Lewis Index (TLI), Bollen's (1989) Fit Index (BL89), Relative Noncentrality Index (RNI), Comparative Fit Index (CFI), Gamma Hat, McDonald's Centrality Index (Mc), or root mean squared error of approximation (RMSEA), various combinations of cutoff values from selected ranges of cutoff criteria for the ML-based SRMR and a given supplemental fit index were used to calculate rejection rates for various types of true-population and misspecified models; that is, models with misspecified factor covariance(s) and models with misspecified factor loading(s). The results suggest that, for the ML method, a cutoff value close to .95 for TLI, BL89, CFI, RNI, and Gamma Hat; a cutoff value close to .90 for Mc; a cutoff value close to .08 for SRMR; and a cutoff value close to .06 for RMSEA are needed before we can conclude that there is a relatively good fit between the hypothesized model and the observed data. Furthermore, the 2-index presentation strategy is required to reject reasonable proportions of various types of true-population and misspecified models. Finally, using the proposed cutoff criteria, the ML-based TLI, Mc, and RMSEA tend to overreject true-population models at small sample size and thus are less preferable when sample size is small.},
  annotation = {\_eprint: https://doi.org/10.1080/10705519909540118},
  file = {/home/justin/Zotero/storage/RE9CGE4S/Hu and Bentler - 1999 - Cutoff criteria for fit indexes in covariance stru.pdf;/home/justin/Zotero/storage/D8UB42QL/10705519909540118.html}
}

@article{huang2020,
  title = {Penalized {{Least Squares}} for {{Structural Equation Modeling}} with {{Ordinal Responses}}},
  author = {Huang, Po-Hsien},
  year = {2020},
  month = sep,
  journal = {Multivariate Behavioral Research},
  volume = {0},
  number = {0},
  pages = {1--19},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/gkrp6d},
  abstract = {Statistical modeling with sparsity has become an active research topic in the fields of statistics and machine learning. Because the true sparsity pattern of a model is generally unknown aforehand, it is often explored by a sparse estimation procedure, like least absolute shrinkage and selection operator (lasso). In this study, a penalized least squares (PLS) method for structural equation modeling (SEM) with ordinal data is developed. PLS describes data generation by an underlying response approach, and uses a least squares (LS) fitting function to construct a penalized estimation criterion. A numerical simulation was used to compare PLS with existing penalized likelihood (PL) in terms of averaged mean square error, absolute bias, and the correctness of the model. Based on these empirical findings, a hybrid PLS was also proposed to improve both PL and PLS. The hybrid PLS first chooses an optimal sparsity pattern by PL, then estimates model parameters by an unpenalized LS under the model selected by PL. We also extended PLS to cases of mixed type data and multi-group analysis. All proposed methods could be realized in the R package lslx.},
  pmid = {32990059},
  keywords = {factor analysis,lasso,penalized least squares,polychoric correlation,Structural equation modeling},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2020.1820309}
}

@article{huber1976,
  title = {Ideal Point Models of Preference},
  author = {Huber, Joel},
  year = {1976},
  journal = {ACR North American Advances},
  keywords = {⛔ No DOI found}
}

@article{humphreys1973,
  title = {A Comparison of Squared Multiples and Iterated Diagonals as Communality Estimates},
  author = {Humphreys, Lloyd G. and Taber, Thomas},
  year = {1973},
  journal = {Educational and Psychological Measurement},
  doi = {10/d9vsxs},
  langid = {english},
  keywords = {Comparative Analysis,Factor Analysis,Mathematical Applications,Measurement Techniques,Research Methodology,Statistical Analysis},
  file = {/home/justin/Zotero/storage/58SYZDPH/eric.ed.gov.html}
}

@article{humphreys1975,
  title = {An Investigation of the Parallel Analysis Criterion for Determining the Number of Common Factors},
  author = {Humphreys, Lloyd G. and Jr, Richard G. Montanelli},
  year = {1975},
  month = apr,
  journal = {Multivariate Behavioral Research},
  volume = {10},
  number = {2},
  pages = {193--205},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/b7rvtp},
  abstract = {Then correlation matrices based upon real and random data with squared multiple correlations in the diagonals are factored, the hypothesis is that the point at which the curves of the latent roots cross indicates the number of common factors. Sampling studies confirm the hypothesis when the common factor model provides a good fit to the data. When small overlapping, nonrandom factors are introduced, the expected value of the number of common factors can still be the number of major factor in the population when the nonrandom "noise" is small compared to sampling error "noise." This criterion for the number of common factors, furthermore, is more accurate than the method of maximum likelihood.},
  annotation = {\_eprint: https://doi.org/10.1207/s15327906mbr1002\_5},
  file = {/home/justin/Zotero/storage/2MVQ59JT/Humphreys and Jr - 1975 - An Investigation of the Parallel Analysis Criterio.pdf;/home/justin/Zotero/storage/SB6HBETE/s15327906mbr1002_5.html}
}

@article{hunter2000,
  title = {Racial and Gender Bias in Ability and Achievement Tests: {{Resolving}} the Apparent Paradox},
  shorttitle = {Racial and Gender Bias in Ability and Achievement Tests},
  author = {Hunter, John E. and Schmidt, Frank L.},
  year = {2000},
  journal = {Psychology, Public Policy, and Law},
  volume = {6},
  number = {1},
  pages = {151--158},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1528(Electronic),1076-8971(Print)},
  doi = {10/dqbsk7},
  abstract = {The study of potential racial and gender bias in individual test items is a major research area today. The fact that research has established that total scores on ability and achievement tests are predictively unbiased raises the question of whether there is in fact any real bias at the item level. No theoretical rationale for expecting such bias has been advanced. It appears that findings of item bias (differential item functioning; DIF) can be explained by three factors: failure to control for measurement error in ability estimates, violations of the unidimensionality assumption required by DIF detection methods, and reliance on significance testing (causing tiny artifactual DIF effects to be statistically significant because sample sizes are very large). After taking into account these artifacts, there appears to be no evidence that items on currently used tests function differently in different racial and gender groups. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Academic Achievement,Achievement Measures,Cognitive Ability,Human Sex Differences,Item Analysis (Test),Racial and Ethnic Differences,Test Bias},
  file = {/home/justin/Zotero/storage/5KQ94M5A/Hunter_Schmidt_2000_Racial and gender bias in ability and achievement tests.pdf;/home/justin/Zotero/storage/HH9DQN8E/2000-02413-013.html}
}

@inproceedings{hutchinson2019,
  ids = {hutchinson50YearsTest2019a},
  title = {50 {{Years}} of {{Test}} ({{Un}})Fairness: {{Lessons}} for {{Machine Learning}}},
  shorttitle = {50 {{Years}} of {{Test}} ({{Un}})Fairness},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Hutchinson, Ben and Mitchell, Margaret},
  year = {2019},
  month = jan,
  series = {{{FAT}}* '19},
  pages = {49--58},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10/gftmq2},
  abstract = {Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.},
  isbn = {978-1-4503-6125-5},
  keywords = {fairness,history,ML fairness,psychometrics,test fairness},
  file = {/home/justin/Zotero/storage/7EQQ9CT2/Hutchinson and Mitchell - 2019 - 50 Years of Test (Un)fairness Lessons for Machine.pdf}
}

@article{jackel1999,
  title = {The Most General Methodology to Create a Valid Correlation Matrix for Risk Management and Option Pricing Purposes},
  author = {J{\"a}ckel, Peter and Rebonato, Riccardo},
  year = {1999},
  journal = {Journal of Risk},
  volume = {2},
  number = {2},
  keywords = {⛔ No DOI found,correlation,matrix smooth,smooth matrix}
}

@article{jackson2001,
  title = {Sample {{Size}} and {{Number}} of {{Parameter Estimates}} in {{Maximum Likelihood Confirmatory Factor Analysis}}: {{A Monte Carlo Investigation}}},
  shorttitle = {Sample {{Size}} and {{Number}} of {{Parameter Estimates}} in {{Maximum Likelihood Confirmatory Factor Analysis}}},
  author = {Jackson, Dennis L.},
  year = {2001},
  month = apr,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {8},
  number = {2},
  pages = {205--223},
  issn = {1070-5511, 1532-8007},
  doi = {10/cfkg4w},
  langid = {english},
  file = {/home/justin/Zotero/storage/32ZH6T25/Jackson - 2001 - Sample Size and Number of Parameter Estimates in M.pdf}
}

@article{jackson2009,
  title = {Reporting Practices in Confirmatory Factor Analysis: {{An}} Overview and Some Recommendations.},
  shorttitle = {Reporting Practices in Confirmatory Factor Analysis},
  author = {Jackson, Dennis L.},
  year = {2009},
  journal = {Psychological Methods},
  volume = {14},
  number = {1},
  pages = {6},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1463},
  doi = {10/cbd6zd},
  abstract = {APA PsycNet FullTextHTML page},
  langid = {english},
  file = {/home/justin/Zotero/storage/PRFLKTLI/2009-02702-005.html}
}

@article{jacobs2019,
  title = {Measurement and {{Fairness}}},
  author = {Jacobs, Abigail Z. and Wallach, Hanna},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.05511 [cs]},
  eprint = {1912.05511},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce the language of measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as "creditworthiness," "teacher quality," or "risk to society," that cannot be measured directly and must instead be inferred from observable properties thought to be related to them---i.e., operationalized via a measurement model. This process introduces the potential for mismatch between the theoretical understanding of the construct purported to be measured and its operationalization. Indeed, we argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. Further complicating these discussions is the fact that fairness itself is an unobservable theoretical construct. Moreover, it is an essentially contested construct---i.e., it has many different theoretical understandings depending on the context. We argue that this contestedness underlies recent debates about fairness definitions: disagreements that appear to be about contradictory operationalizations are, in fact, disagreements about different theoretical understandings of the construct itself. By introducing the language of measurement modeling, we provide the computer science community with a process for making explicit and testing assumptions about unobservable theoretical constructs, thereby making it easier to identify, characterize, and even mitigate fairness-related harms.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/justin/Zotero/storage/XV4APD6C/Jacobs and Wallach - 2019 - Measurement and Fairness.pdf;/home/justin/Zotero/storage/76YU5DAC/1912.html}
}

@article{jacqmin2007,
  title = {Robustness of the Linear Mixed Model to Misspecified Error Distribution},
  author = {{Jacqmin-Gadda}, H{\'e}l{\`e}ne and Sibillot, Solenne and Proust, C{\'e}cile and Molina, Jean-Michel and Thi{\'e}baut, Rodolphe},
  year = {2007},
  journal = {Computational Statistics \& Data Analysis},
  volume = {51},
  number = {10},
  pages = {5142--5154},
  publisher = {{Elsevier}},
  doi = {10/b88tvn}
}

@article{jamshidian1998,
  ids = {jamshidianQuasiNewtonMethodMinimum1998},
  title = {A Quasi-{{Newton}} Method for Minimum Trace Factor Analysis},
  author = {Jamshidian, Mortaza and Bentler, Peter M.},
  year = {1998},
  journal = {Journal of Statistical Computation and Simulation},
  volume = {62},
  number = {1-2},
  pages = {73--89},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10/c5zxj3},
  abstract = {In the past several algorithms have been given to solve the minimum trace factor analysis (MTFA) and the constrained minimum trace factor analysis (CMTFA) problems. Some of these algorithms, depending on the initial value, may converge to points that are not the solution to the above problems, some converge linearly, and some are quadratically convergent but are somewhat difficult to implement. In this paper we propose modified Han\textendash Powell algorithms to solve the MTFA and CMTFA problems. The modifications deal with the problem of multiple eigenvalues. The proposed algorithms are globally convergent and their speed is locally superlinear. We also give a modified Han\textendash Powell algorithm to solve the weighted minimum trace factor analysis (WMTFA) problem. This method is also locally superlinear and is simpler to implement as compared to methods proposed earlier. Four examples are given to show the performance of the proposed algorithms. More generally, our experience with these algorithms shows that, starting at arbitrary points, they converge to the solution in a small number of iterations and reasonable time.},
  date-added = {2020-01-02 12:45:20 -0600},
  date-modified = {2020-01-02 12:45:27 -0600},
  keywords = {constrained minimum trace factor analysis,Han–Powell algorithm,matrix smooth,Minimum trace factor analysis,reliability,weighted minimum trace factor analysis},
  annotation = {\_eprint: https://doi.org/10.1080/00949659808811925},
  file = {/home/justin/Zotero/storage/H74Z9YZ5/00949659808811925.html}
}

@article{jennrich2002,
  title = {A Simple General Method for Oblique Rotation},
  author = {Jennrich, Robert I.},
  year = {2002},
  journal = {Psychometrika},
  volume = {67},
  number = {1},
  pages = {7--19},
  issn = {1860-0980},
  doi = {10/bpmpvk},
  abstract = {A simple and very general algorithm for oblique rotation is identified. While motivated by the rotation problem in factor analysis, it may be used to minimize almost any function of a not necessarily square matrix whose columns are restricted to have unit length. The algorithm has two steps. The first is to compute the gradient of the rotation criterion and the second is to project this onto a manifold of matrices with unit length columns. For this reason it is called a gradient projection algorithm. Because the projection step is very simple, implementation of the algorithm involves little more than computing the gradient of the rotation criterion which for many applications is very simple. It is proven that the algorithm is strictly monotone, that is as long as it is not already at a stationary point, each step will decrease the value of the criterion. Examples from a variety of areas are used to demonstrate the algorithm, including oblimin rotation, target rotation, simplimax rotation, and rotation to similarity and simplicity simultaneously. While it may be, the algorithm is not intended for use as a standard algorithm for well established problems, but rather as a tool for investigating new methods where its generality and simplicity may save an investigator substantial effort.},
  abstractnote = {A simple and very general algorithm for oblique rotation is identified. While motivated by the rotation problem in factor analysis, it may be used to minimize almost any function of a not necessarily square matrix whose columns are restricted to have unit length. The algorithm has two steps. The first is to compute the gradient of the rotation criterion and the second is to project this onto a manifold of matrices with unit length columns. For this reason it is called a gradient projection algorithm. Because the projection step is very simple, implementation of the algorithm involves little more than computing the gradient of the rotation criterion which for many applications is very simple. It is proven that the algorithm is strictly monotone, that is as long as it is not already at a stationary point, each step will decrease the value of the criterion. Examples from a variety of areas are used to demonstrate the algorithm, including oblimin rotation, target rotation, simplimax rotation, and rotation to similarity and simplicity simultaneously. While it may be, the algorithm is not intended for use as a standard algorithm for well established problems, but rather as a tool for investigating new methods where its generality and simplicity may save an investigator substantial effort.},
  doi2 = {10.1007/BF02294706},
  langid = {english},
  file = {/home/justin/Zotero/storage/39CMS8XU/Jennrich - 2002 - A simple general method for oblique rotation.pdf}
}

@article{jennrich2006,
  title = {Rotation to {{Simple Loadings Using Component Loss Functions}}: {{The Oblique Case}}},
  shorttitle = {Rotation to {{Simple Loadings Using Component Loss Functions}}},
  author = {Jennrich, Robert I.},
  year = {2006},
  month = mar,
  journal = {Psychometrika},
  volume = {71},
  number = {1},
  pages = {173--191},
  issn = {1860-0980},
  doi = {10/bzhbsc},
  abstract = {Component loss functions (CLFs) similar to those used in orthogonal rotation are introduced to define criteria for oblique rotation in factor analysis. It is shown how the shape of the CLF affects the performance of the criterion it defines. For example, it is shown that monotone concave CLFs give criteria that are minimized by loadings with perfect simple structure when such loadings exist. Moreover, if the CLFs are strictly concave, minimizing must produce perfect simple structure whenever it exists. Examples show that methods defined by concave CLFs perform well much more generally. While it appears important to use a concave CLF, the specific CLF used is less important. For example, the very simple linear CLF gives a rotation method that can easily outperform the most popular oblique rotation methods promax and quartimin and is competitive with the more complex simplimax and geomin methods.},
  langid = {english},
  file = {/home/justin/Zotero/storage/2J8L32TB/Jennrich - 2006 - Rotation to Simple Loadings Using Component Loss F.pdf}
}

@article{jiang2012,
  title = {When and How Is Job Embeddedness Predictive of Turnover? {{A}} Meta-Analytic Investigation.},
  shorttitle = {When and How Is Job Embeddedness Predictive of Turnover?},
  author = {Jiang, Kaifeng and Liu, Dong and McKay, Patrick F. and Lee, Thomas W. and Mitchell, Terence R.},
  year = {2012},
  journal = {Journal of Applied Psychology},
  volume = {97},
  number = {5},
  pages = {1077},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1854},
  doi = {10/f383k6},
  file = {/home/justin/Zotero/storage/XE56KVJU/Jiang et al_2012_When and how is job embeddedness predictive of turnover.pdf}
}

@incollection{john1990,
  title = {The "{{Big Five}}" Factor Taxonomy: {{Dimensions}} of Personality in the Natural Language and in Questionnaires},
  booktitle = {Handbook of Personality: {{Theory}} and Research},
  author = {John, Oliver P},
  editor = {Pervin, Lawrence A},
  year = {1990},
  pages = {66--100},
  publisher = {{Guilford Press}},
  address = {{New York}},
  isbn = {978-0-89862-430-4 978-0-89862-593-6},
  langid = {english},
  annotation = {OCLC: 21593110}
}

@article{jones2020,
  title = {The {{Invariance Paradox}}: {{Using Optimal Test Design}} to {{Minimize Bias}}},
  shorttitle = {The {{Invariance Paradox}}},
  author = {Jones, Andrew T. and Kopp, Jason P. and Ong, Thai Q.},
  year = {2020},
  month = jun,
  journal = {Educational Measurement: Issues and Practice},
  volume = {39},
  number = {2},
  pages = {48--57},
  issn = {0731-1745, 1745-3992},
  doi = {10/gjrkdg},
  langid = {english},
  file = {/home/justin/Zotero/storage/AMLA2GSE/Jones et al. - 2020 - The Invariance Paradox Using Optimal Test Design .pdf}
}

@article{joreskog1969,
  title = {A General Approach to Confirmatory Maximum Likelihood Factor Analysis},
  author = {J{\"o}reskog, K. G.},
  year = {1969},
  journal = {Psychometrika},
  volume = {34},
  number = {2, Pt.1},
  pages = {183--202},
  publisher = {{Springer}},
  address = {{Germany}},
  issn = {1860-0980(Electronic),0033-3123(Print)},
  doi = {10/cs6df7},
  abstract = {Describes a general procedure by which any number of parameters of the factor analytic model can be held fixed at any values and the remaining free parameters estimated by the maximum likelihood method. By choosing the fixed parameters appropriately, factors can be defined to have desired properties and make subsequent rotation unnecessary. The goodness of fit of the maximum likelihood solution under the hypothesis represented by the fixed parameters is tested by a large sample CHI2 test based on the likelihood ratio technique. Ss were 145 7th and 8th graders. From an estimate of the variance-covariance matrix of the estimated parameters, approximate confidence intervals for the parameters can be obtained. Several examples illustrating the usefulness of the procedure are given. (22 ref.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Factor Analysis,Methodology,Statistical Analysis},
  file = {/home/justin/Zotero/storage/9Q86S7NP/1970-03001-001.html}
}

@article{joreskog1981,
  title = {Analysis of Linear Structural Relationships by Maximum Likelihood and Least Squares Methods},
  author = {J{\"o}reskog, K. G. and S{\"o}rbom, Dag},
  year = {1981},
  journal = {International Educational Services. Chicago, IL},
  keywords = {⛔ No DOI found}
}

@article{joreskog1982,
  title = {Recent Developments in Structural Equation Modeling},
  author = {J{\"o}reskog, K. G. and S{\"o}rbom, Dag},
  year = {1982},
  month = nov,
  journal = {Journal of Marketing Research},
  volume = {19},
  number = {4},
  pages = {404--416},
  publisher = {{SAGE Publications Inc}},
  issn = {0022-2437},
  doi = {10/gjg82s},
  abstract = {The authors describe new developments in structural equation modeling as incorporated in LISREL V. The procedures are illustrated on data from three previous studies.},
  langid = {english},
  file = {/home/justin/Zotero/storage/B36U5DU4/Jöreskog_Sörbom_1982_Recent Developments in Structural Equation Modeling.pdf}
}

@article{joreskog1990,
  title = {New Developments in {{LISREL}}: Analysis of Ordinal Variables Using Polychoric Correlations and Weighted Least Squares},
  author = {J{\"o}reskog, K. G.},
  year = {1990},
  journal = {Quality and Quantity},
  doi = {10/bk8xqb}
}

@article{joreskog2001,
  title = {Factor Analysis of Ordinal Variables: {{A}} Comparison of Three Approaches},
  author = {J{\"o}reskog, K. G.},
  year = {2001},
  journal = {Multivariate Behavioral Research},
  doi = {10/bcmqtf}
}

@article{joseph2010,
  title = {Emotional Intelligence: An Integrative Meta-Analysis and Cascading Model.},
  author = {Joseph, Dana L and Newman, Daniel A},
  year = {2010},
  journal = {Journal of Applied Psychology},
  volume = {95},
  number = {1},
  pages = {54},
  doi = {10/bmn4tr},
  keywords = {matrix smooth}
}

@article{Jung_2011,
  title = {Exploratory Factor Analysis for Small Samples},
  author = {Jung, Sunho and Lee, Soonmook},
  year = {2011},
  journal = {Behavior Research Methods},
  volume = {43},
  number = {3},
  pages = {701--709},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {1554-3528},
  doi = {10/brhw58},
  date-added = {2019-11-27 11:58:52 -0600},
  date-modified = {2019-12-12 15:00:40 -0600}
}

@article{jung2008regularized,
  title = {Regularized Common Factor Analysis},
  author = {Jung, Sunho and Takane, Yoshio},
  year = {2008},
  journal = {New trends in psychometrics},
  pages = {141--149},
  publisher = {{Tokyo: Universal Academy Press}},
  keywords = {⛔ No DOI found},
  file = {/home/justin/Zotero/storage/MAGRD59R/Jung_Takane_2008_Regularized common factor analysis.pdf}
}

@phdthesis{jurich2014,
  title = {Assessing Model Fit of Multidimensional Item Response Theory and Diagnostic Classification Models Using Limited-Information Statistics},
  author = {Jurich, Daniel P.},
  year = {2014},
  address = {{Harrisonburg, VA}},
  abstract = {Educational assessments have been constructed predominately to measure broad unidimensional constructs, limiting the amount of formative information gained from the assessments. This has led various stakeholders to call for increased application of multidimensional assessments that can be used diagnostically to address students' strengths and weaknesses. Multidimensional item response theory (MIRT) and diagnostic classification models (DCMs) have received considerable attention as statistical models that can address this call. However, assessment of model fit has posed an issue for these models as common full-information statistics fail to approximate the appropriate distribution for typical test lengths. This dissertation explored a recently proposed limited-information framework for full-information algorithms that alleviates issues presented by full-information fit statistics. Separate studies were conducted to investigate the limited-information fit statistics under MIRT models and DCMs. The first study investigated the performance of a bivariate limited-information test statistic, termed M2, with MIRT models. This study particularly focused on the root mean square error of approximation (RMSEA) index computed from M2 that quantifies the degree of model misspecification. Simulations were used to examine the RMSEA under a variety of model misspecifications and conditions in order to provide practitioners empirical guidelines for interpreting the index. Results showed the RMSEA provides a useful indicator to evaluate degree of model fit, with cut-offs around .04 appearing to be reasonable guidelines for determining a moderate misspecification. However, cut-offs necessary to reject misspecified models showed some dependence on the type of misspecification. The second study extended the M2 and RMSEA indices to the log-linear cognitive diagnostic model, a generalized DCM. Results showed that the M2 followed the appropriate theoretical chi-squared distribution and RMSEA appropriately distinguished between various degrees of misspecification. Discussions highlight how the limited-information framework provides practitioners a pragmatic set of tools for evaluating the fit of multidimensional assessments and how the framework can be used to guide development of future assessments. Limitations and future research to address these issues are also presented.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9781303909382},
  langid = {english},
  school = {James Madison University},
  keywords = {Diagnostic classification models,Education,Item response theory,Model fit,Psychology,Pure sciences,Validity evidence},
  file = {/home/justin/Zotero/storage/IDTVNW53/Jurich_2014_Assessing Model Fit of Multidimensional Item Response Theory and Diagnostic.pdf}
}

@book{jurs1993,
  title = {Using Linear Regression to Determine the Number of Factors to Retain in Factor Analysis and the Number of Issues to Retain in Delphi Studies and Other Surveys},
  author = {Jurs, Stephen and And Others},
  year = {1993},
  month = apr,
  abstract = {The scree test and its linear regression technique are reviewed, and results of its use in factor analysis and Delphi data sets are described. The scree test was originally a visual approach for making judgments about eigenvalues, which considered the relationships of the eigenvalues to one another as well as their actual values. The graph that is plotted resembles a mountainside where a base pile of rubble, or scree, is formed. The analysis determines which eigenvalues are salient (mountainside) and which are rubble (scree). A multiple linear regression (MLR) approach has been proposed that would include more data points than the usual scree test and could yield better results. The MLR test provides the same decision as does the visual scree test, but can be easily programed,  using an approach in which the ordered eigenvalues are thought of as points in a scatterplot. Examples are presented of the use of the scree test and the MLR approach with Delphi technique data to help decide how many items or issues to retain in a Delphi study or survey. The MLR approach appears to be an effective analytical procedure for the scree test. It usually produces the same number or fewer factors than the visual scree test in factor analysis, but yields more items than the visual scree in factor analysis. Three tables and six figures illustrate the discussion. (Contains 15 references.) (SLD)},
  langid = {english},
  keywords = {Delphi Technique,Equations (Mathematics),Factor Analysis,Graphs,Mathematical Models,Regression (Statistics),Research Methodology,Surveys},
  file = {/home/justin/Zotero/storage/QPTV54KJ/Jurs_And Others_1993_Using Linear Regression To Determine the Number of Factors To Retain in Factor.pdf;/home/justin/Zotero/storage/98NYLWRR/eric.ed.gov.html}
}

@article{kamata2008,
  ids = {kamataNoteRelationFactor2008},
  title = {A {{Note}} on the {{Relation Between Factor Analytic}} and {{Item Response Theory Models}}},
  author = {Kamata, Akihito and Bauer, Daniel J.},
  year = {2008},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {15},
  number = {1},
  pages = {136--153},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/c29wz5},
  abstract = {The relations among several alternative parameterizations of the binary factor analysis model and the 2-parameter item response theory model are discussed. It is pointed out that different parameterizations of factor analysis model parameters can be transformed into item response model theory parameters, and general formulas are provided. Illustrative data analysis is provided to demonstrate the transformations.},
  annotation = {\_eprint: https://doi.org/10.1080/10705510701758406},
  file = {/home/justin/Zotero/storage/QCYPZUW7/Kamata and Bauer - 2008 - A Note on the Relation Between Factor Analytic and.pdf;/home/justin/Zotero/storage/PJ8RLVWD/10705510701758406.html}
}

@phdthesis{kanyongo2004,
  title = {A {{Monte Carlo}} Study of the Influence of Reliability on Four Rules for Determining the Number of Components to Retain from a Principal Components Analysis},
  author = {Kanyongo, Gibbs Yanai},
  year = {2004},
  address = {{Athens, OH}},
  abstract = {This study seeks to investigate how reliability influences the number of components extracted by different procedures under varying conditions of sample size (n), component loading (aij) and variable-to-component ratio (p:m). It compares four procedures, Kaiser rule, scree plot, Horn's parallel analysis procedure and modified Horn's parallel analysis procedure. To investigate this issue, a Monte Carlo study was conducted. The underlying population correlation matrices were generated for each possible p, p:m and aij combination and the components upon which these population correlation matrices were based were independent of each other. The population correlation matrices were generated by specifying the component pattern matrix based on the combination of values for p:m and aij. The program was run four times to yield four different population correlation matrices, one correlation matrix for each combination of conditions. Ten samples were created for each combination of p:m, aij, {$\rho$}xx{${'}$} and n. For each sample, the four extraction methods were compared for bias and RMSE. Based on the results of this study, it has been shown that reliability has influence under certain conditions but not under other conditions. Modified Horn's parallel analysis procedure has the best overall performance, followed by Horn's parallel analysis procedure although the two procedures are quite close in performance. Both procedures display slight bias at low reliability values, with HPA overestimating, and MHPA underestimating the number of components. The scree plot is also highly accurate, in most cases, it is comparable to MHPA and HPA, and in some cases, it is even better. The Kaiser rule as expected is very poor in most situations. In all the situations, it is wrong; it overestimates the correct number of components. Overall, all the procedures perform worst at the reliability value of .60 and are best at the reliability value of 1.00.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9780496763900},
  langid = {english},
  school = {Ohio University},
  keywords = {Education,Parallel analysis,Principal components analysis,Reliability},
  file = {/home/justin/Zotero/storage/98RCGIU7/Kanyongo_2004_A Monte Carlo study of the influence of reliability on four rules for.pdf}
}

@article{kasy,
  title = {Fairness, {{Equality}}, and {{Power}} in {{Algorithmic Decision-Making}}},
  author = {Kasy, Maximilian and Abebe, Rediet},
  pages = {14},
  abstract = {Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same ``merit." Drawing on the theory of justice, we argue that such leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by ``merit;" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider only between-group and not within-group differences.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/justin/Zotero/storage/V6YXL3HS/Kasy and Abebe - Fairness, Equality, and Power in Algorithmic Decis.pdf}
}

@phdthesis{kay2004,
  title = {A Comparison of Traditional and {{IRT}} Factor Analysis},
  author = {Kay, Cheryl Ann},
  year = {2004},
  address = {{United States -- Texas}},
  abstract = {This study investigated the item parameter recovery of two methods of factor analysis. The methods researched were a traditional factor analysis of tetrachoric correlation coefficients and an IRT approach to factor analysis which utilizes marginal maximum likelihood estimation using an EM algorithm (MMLE-EM). Dichotomous item response data was generated under the 2-parameter normal ogive model (2PNOM) using PARDSIM software. Examinee abilities were sampled from both the standard normal and uniform distributions. True item discrimination, a, was normal with a mean of .75 and a standard deviation of .10. True b, item difficulty, was specified as uniform [-2, 2]. The two distributions of abilities were completely crossed with three test lengths (n = 30, 60, and 100) and three sample sizes (N = 50, 500, and 1000). Each of the 18 conditions was replicated 5 times, resulting in 90 datasets. PRELIS software was used to conduct a traditional factor analysis on the tetrachoric correlations. The IRT approach to factor analysis was conducted using BILOG 3 software. Parameter recovery was evaluated in terms of root mean square error, average signed bias, and Pearson correlations between estimated and true item parameters. ANOVAs were conducted to identify systematic differences in error indices. Based on many of the indices, it appears the IRT approach to factor analysis recovers item parameters better than the traditional approach studied. Future research should compare other methods of factor analysis to MMLE-EM under various non-normal distributions of abilities.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9780496900053},
  langid = {english},
  school = {University of North Texas},
  keywords = {Education,Item response,Psychology,Tetrachoric correlations,Traditional factor analysis},
  file = {/home/justin/Zotero/storage/YKJNZQGS/Kay_2004_A comparison of traditional and IRT factor analysis.pdf}
}

@article{kelley2011,
  title = {Accuracy in Parameter Estimation for the Root Mean Square Error of Approximation: {{Sample}} Size Planning for Narrow Confidence Intervals},
  shorttitle = {Accuracy in Parameter Estimation for the Root Mean Square Error of Approximation},
  author = {Kelley, Ken and Lai, Keke},
  year = {2011},
  month = feb,
  journal = {Multivariate Behavioral Research},
  volume = {46},
  number = {1},
  pages = {1--32},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/bc9tqb},
  abstract = {The root mean square error of approximation (RMSEA) is one of the most widely reported measures of misfit/fit in applications of structural equation modeling. When the RMSEA is of interest, so too should be the accompanying confidence interval. A narrow confidence interval reveals that the plausible parameter values are confined to a relatively small range at the specified level of confidence. The accuracy in parameter estimation approach to sample size planning is developed for the RMSEA so that the confidence interval for the population RMSEA will have a width whose expectation is sufficiently narrow. Analytic developments are shown to work well with a Monte Carlo simulation study. Freely available computer software is developed so that the methods discussed can be implemented. The methods are demonstrated for a repeated measures design where the way in which social relationships and initial depression influence coping strategies and later depression are examined.},
  pmid = {26771579},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2011.543027},
  file = {/home/justin/Zotero/storage/WFDSWRWR/Kelley_Lai_2011_Accuracy in Parameter Estimation for the Root Mean Square Error of Approximation.pdf;/home/justin/Zotero/storage/MTP8FC6B/00273171.2011.html}
}

@article{kelley2016,
  title = {Confidence Intervals for Population Reliability Coefficients: {{Evaluation}} of Methods, Recommendations, and Software for Composite Measures},
  shorttitle = {Confidence Intervals for Population Reliability Coefficients},
  author = {Kelley, Ken and Pornprasertmanit, Sunthud},
  year = {2016},
  journal = {Psychological Methods},
  volume = {21},
  number = {1},
  pages = {69--92},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10/f8fq9j},
  abstract = {A composite score is the sum of a set of components. For example, a total test score can be defined as the sum of the individual items. The reliability of composite scores is of interest in a wide variety of contexts due to their widespread use and applicability to many disciplines. The psychometric literature has devoted considerable time to discussing how to best estimate the population reliability value. However, all point estimates of a reliability coefficient fail to convey the uncertainty associated with the estimate as it estimates the population value. Correspondingly, a confidence interval is recommended to convey the uncertainty with which the population value of the reliability coefficient has been estimated. However, many confidence interval methods for bracketing the population reliability coefficient exist and it is not clear which method is most appropriate in general or in a variety of specific circumstances. We evaluate these confidence interval methods for 4 reliability coefficients (coefficient alpha, coefficient omega, hierarchical omega, and categorical omega) under a variety of conditions with 3 large-scale Monte Carlo simulation studies. Our findings lead us to generally recommend bootstrap confidence intervals for hierarchical omega for continuous items and categorical omega for categorical items. All of the methods we discuss are implemented in the freely available R language and environment via the MBESS package. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Computer Software,Confidence Limits (Statistics),Measurement,Methodology,Statistical Correlation,Statistical Reliability,Test Scores},
  file = {/home/justin/Zotero/storage/VBRQKAFS/Kelley_Pornprasertmanit_2016_Confidence intervals for population reliability coefficients.pdf;/home/justin/Zotero/storage/SXEF4RES/2016-11566-002.html}
}

@article{kenny2003,
  title = {Effect of the {{Number}} of {{Variables}} on {{Measures}} of {{Fit}} in {{Structural Equation Modeling}}},
  author = {Kenny, David A. and McCoach, D. Betsy},
  year = {2003},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {10},
  number = {3},
  pages = {333--351},
  issn = {1070-5511, 1532-8007},
  doi = {10/bmnt6z},
  langid = {english}
}

@misc{kenny2020,
  title = {{{SEM}}: {{Fit}} ({{David A}}. {{Kenny}})},
  author = {Kenny, David A.},
  year = {2020},
  month = jun,
  journal = {Measuring Model Fit},
  howpublished = {https://davidakenny.net/cm/fit.htm},
  file = {/home/justin/Zotero/storage/TZJYE4EY/fit.html}
}

@article{khamaru2019,
  title = {Computation of the Maximum Likelihood Estimator in Low-Rank Factor Analysis},
  author = {Khamaru, Koulik and Mazumder, Rahul},
  year = {2019},
  month = jul,
  journal = {Mathematical Programming},
  volume = {176},
  number = {1-2},
  pages = {279--310},
  issn = {0025-5610, 1436-4646},
  doi = {10/ggmmvn},
  langid = {english},
  file = {/home/justin/Zotero/storage/3YVLLZDW/Khamaru and Mazumder - 2019 - Computation of the maximum likelihood estimator in.pdf}
}

@book{khine2013,
  title = {{Application of structural equation modelling in educational research and practice.}},
  author = {Khine, Myint Swe and Afari, Ernest and Aldridge, Jill and Bergen, Theo and DiStefano, Christine and Fraser, Barry J and In'nami, Yo and Kamp, Marcel and Khine, Myint Swe and Koizumi, Rie and Lee, Wincy W. S and Leung, Doris Y. P and Liem, Gregory Arief D and Liu, Liyan and Lo, Kenneth C. H and Manderson, Jill and Martin, Andrew J and Mindrila, Diana and Monrad, Diane M and Rugutt, John K and Seker, Hasan and Sheehan, Lorn and Sundararajan, Binod and Sundararajan, Malavika and Teo, Timothy and Tsai, Liang Ting and Velayutham, Sunitadevi and {Vrijnsen-de Corte}, Marjan and Yang, Chih-Chien and Den Brok, Perry},
  year = {2013},
  abstract = {Structural Equation Modeling (SEM) is a statistical approach to testing hypotheses about relationships among observed and latent variables. In recent years the use of SEM in research has increased in psychology, sociology, and economics. In particular educational researchers try to obtain a complete image of the process of education through the measurement of personality differences, learning environments, motivation levels and a host of other variables that affect the teaching and learning process. With the use of survey instruments and interviews with students, teachers and other stakeholders as a lens, educators can assess and gain valuable information about the social ecology of the classrooms that could help in improving instructional approach, classroom management and learning organisations. SEM is becoming a powerful analytical tool and making methodological advances in multivariate analysis. This book presents works on concepts, methodologies and applications of SEM in educational research and practice. [Back cover, ed].},
  isbn = {978-94-6209-330-0 978-94-6209-331-7},
  langid = {In English.},
  annotation = {OCLC: 927059984},
  file = {/home/justin/Zotero/storage/EXCWHAYS/Khine et al_2013_Application of structural equation modelling in educational research and.pdf}
}

@article{kim2011,
  title = {Testing Measurement Invariance: {{A}} Comparison of Multiple-Group Categorical {{CFA}} and {{IRT}}},
  shorttitle = {Testing Measurement Invariance},
  author = {Kim, Eun Sook and Yoon, Myeongsun},
  year = {2011},
  month = apr,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {18},
  number = {2},
  pages = {212--228},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/bkpp6w},
  abstract = {This study investigated two major approaches in testing measurement invariance for ordinal measures: multiple-group categorical confirmatory factor analysis (MCCFA) and item response theory (IRT). Unlike the ordinary linear factor analysis, MCCFA can appropriately model the ordered-categorical measures with a threshold structure. A simulation study under various conditions was conducted for the comparison of MCCFA and IRT with respect to the power to detect the lack of invariance across groups. Both MCCFA and IRT showed reasonable power to identify the noninvariant item when differential item functioning (DIF) was large. The false positive rates were relatively high in both methods, however. The adjustment of critical values improved the performance of MCCFA by reducing false positive rates substantially and yet yielding adequate power. Alternative model fit indexes of MCCFA were also examined and they were found to be reliable to detect DIF, in general.},
  keywords = {categorical analysis,CFA (confirmatory factor analysis),DIF (differential item functioning),IRT (item response theory),measurement invariance},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2011.557337},
  file = {/home/justin/Zotero/storage/MAG39WMW/Kim and Yoon - 2011 - Testing Measurement Invariance A Comparison of Mu.pdf;/home/justin/Zotero/storage/I835ET6Z/10705511.2011.html}
}

@article{kim2012,
  title = {Testing {{Measurement Invariance Using MIMIC}}: {{Likelihood Ratio Test With}} a {{Critical Value Adjustment}}},
  shorttitle = {Testing {{Measurement Invariance Using MIMIC}}},
  author = {Kim, Eun Sook and Yoon, Myeongsun and Lee, Taehun},
  year = {2012},
  month = jun,
  journal = {Educational and Psychological Measurement},
  volume = {72},
  number = {3},
  pages = {469--492},
  issn = {0013-1644, 1552-3888},
  doi = {10/fpk6bz},
  langid = {english},
  file = {/home/justin/Zotero/storage/IQLXV2TJ/Kim et al. - 2012 - Testing Measurement Invariance Using MIMIC Likeli.pdf}
}

@misc{kipf2016,
  title = {How Powerful Are {{Graph Convolutional Networks}}?},
  author = {Kipf, Thomas},
  year = {2016},
  abstract = {Many important real-world datasets come in the form of graphs or networks: social networks, knowledge graphs, protein-interaction networks, the World Wide Web, etc. (just to name a few). Yet, until recently, very little attention has been devoted to the generalization of neural...},
  file = {/home/justin/Zotero/storage/G5HM93AP/graph-convolutional-networks.html}
}

@article{kipf2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  journal = {arXiv:1609.02907 [cs, stat]},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/justin/Zotero/storage/S5DM5E44/Kipf_Welling_2017_Semi-Supervised Classification with Graph Convolutional Networks.pdf;/home/justin/Zotero/storage/NDUV59E9/1609.html}
}

@book{kline2011,
  title = {Principles and Practice of Structural Equation Modeling},
  author = {Kline, Rex B.},
  year = {2011},
  series = {Methodology in the Social Sciences},
  edition = {3rd ed},
  publisher = {{Guilford Press}},
  address = {{New York}},
  abstract = {"This bestselling text provides a balance between the technical and practical aspects of structural equation modeling (SEM). Using clear and accessible language, Rex B. Kline covers core techniques, potential pitfalls, and applications across the behavioral and social sciences. Some more advanced topics are also covered, including estimation of interactive effects of latent variables and multilevel SEM. The companion Web page offers downloadable syntax, data, and output files for each detailed example for EQS, LISREL, and Mplus, allowing readers to view the results of the same analysis generated by three different computer tools."--pub. desc},
  isbn = {978-1-60623-877-6 978-1-60623-876-9},
  langid = {english},
  lccn = {QA278 .K585 2011},
  keywords = {Social sciences,Statistical methods Data processing,Structural equation modeling},
  annotation = {OCLC: ocn636917444},
  file = {/home/justin/Zotero/storage/B7DDM3ZM/Kline - 2011 - Principles and practice of structural equation mod.pdf}
}

@article{kline2013,
  title = {Assessing Statistical Aspects of Test Fairness with Structural Equation Modelling},
  author = {Kline, Rex B.},
  year = {2013},
  month = apr,
  journal = {Educational Research and Evaluation},
  volume = {19},
  number = {2-3},
  pages = {204--222},
  publisher = {{Routledge}},
  issn = {1380-3611},
  doi = {10/gg8sct},
  abstract = {Test fairness and test bias are not synonymous concepts. Test bias refers to statistical evidence that the psychometrics or interpretation of test scores depend on group membership, such as gender or race, when such differences are not expected. A test that is grossly biased may be judged to be unfair, but test fairness concerns the broader, more subjective evaluation of assessment outcomes from perspectives of social justice. Thus, the determination of test fairness is not solely a matter of statistics, but statistical evidence is important when evaluating test fairness. This work introduces the use of the structural equation modelling technique of multiple-group confirmatory factor analysis (MGCFA) to evaluate hypotheses of measurement invariance, or whether a set of observed variables measures the same factors with the same precision over different populations. An example of testing for measurement invariance with MGCFA in an actual, downloadable data set is also demonstrated.},
  keywords = {measurement invariance,multiple-group confirmatory factor analysis,structural equation modelling,test bias,test fairness},
  annotation = {\_eprint: https://doi.org/10.1080/13803611.2013.767624},
  file = {/home/justin/Zotero/storage/I2AVGK9A/Kline - 2013 - Assessing statistical aspects of test fairness wit.pdf;/home/justin/Zotero/storage/73FG9ENU/13803611.2013.html}
}

@article{knol1989,
  ids = {knolLeastsquaresApproximationImproper1989},
  title = {Least-{{Squares Approximation}} of an {{Improper Correlation Matrix}} by a {{Proper One}}},
  author = {Knol, Dl and {ten Berge}, Jos M. F.},
  year = {1989},
  month = mar,
  journal = {Psychometrika},
  volume = {54},
  number = {1},
  pages = {53--61},
  publisher = {{Psychometric Soc}},
  address = {{Williamsburg}},
  issn = {0033-3123},
  doi = {10/bpcxh9},
  langid = {english},
  keywords = {fungibleR,matrix smooth,SEM},
  annotation = {WOS:A1989U856500003},
  file = {/home/justin/Zotero/storage/N2EGFUZL/Knol and Tenberge - 1989 - Least-Squares Approximation of an Improper Correla.pdf}
}

@article{knol1991,
  title = {Empirical Comparison between Factor Analysis and Multidimensional Item Response Models},
  author = {Knol, Dirk L. and Berger, Martijn P. F.},
  year = {1991},
  journal = {Multivariate Behavioral Research},
  volume = {26},
  number = {3},
  pages = {457--477},
  publisher = {{Taylor \& Francis}},
  issn = {0027-3171},
  doi = {10/dwdmhg},
  abstract = {Many factor analysis and multidimensional item response models for dichotomous variables have been proposed in literature. The models and various methods for estimating the item parameters are reviewed briefly. In a simulation study these methods are compared with respect to their estimates of the item parameters both in terms of an item response theory formulation and in terms of a factor analysis formulation. It is concluded that for multidimensional data a common factor analysis on the matrix of tetrachoric correlations performs at least as well as the theoretically appropriate multidimensional item response models.},
  date-added = {2019-12-11 11:45:58 -0600},
  date-modified = {2019-12-11 11:45:58 -0600},
  pmid = {26776713},
  annotation = {\_eprint: https://doi.org/10.1207/s15327906mbr2603\_5},
  file = {/home/justin/Zotero/storage/KET3DTKI/Knol and Berger - 1991 - Empirical Comparison Between Factor Analysis and M.pdf;/home/justin/Zotero/storage/5W8QSD3G/s15327906mbr2603_5.html}
}

@techreport{knolLeastSquaresApproximationImproper1987,
  title = {Least-{{Squares Approximation}} of an {{Improper}} by a {{Proper Correlation Matrix Using}} a {{Semi-Infinite Convex Program}}. {{Research Report}} 87-7.},
  author = {Knol, Dirk L and {ten Berge}, Jos MF},
  year = {1987},
  institution = {{ERIC}},
  keywords = {matrix smooth}
}

@book{kolaczyk2020,
  title = {Statistical {{Analysis}} of {{Network Data}} with {{R}}},
  author = {Kolaczyk, Eric D and {Kolaczyk} and {Briskman}},
  year = {2020},
  publisher = {{Springer International Publishing AG}},
  address = {{S.l.}},
  isbn = {978-3-030-44128-9},
  langid = {english},
  annotation = {OCLC: 1192346319},
  file = {/home/justin/Zotero/storage/C9ZUFW3K/Kolaczyk-Csárdi2020_Book_StatisticalAnalysisOfNetworkDa.epub}
}

@article{kolter2008,
  title = {Convex Optimization Overview},
  author = {Kolter, Zico},
  year = {2008},
  keywords = {⛔ No DOI found}
}

@article{kracht2018,
  title = {A {{Comparison}} of {{Matrix Smoothing Algorithms}}.},
  author = {Kracht, Justin and Waller, Niels G},
  year = {2018},
  journal = {Multivariate behavioral research},
  volume = {53},
  number = {1},
  pages = {136--137},
  doi = {10/gdqck6},
  copyright = {All rights reserved}
}

@article{kracht2020,
  title = {Assessing Dimensionality in Non-Positive Definite Tetrachoric Correlation Matrices: {{Does}} Matrix Smoothing Help?},
  shorttitle = {Assessing {{Dimensionality}} in {{Non-Positive Definite Tetrachoric Correlation Matrices}}},
  author = {Kracht, Justin D. and Waller, Niels G.},
  year = {2020},
  journal = {Multivariate Behavioral Research},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/ghq7p7},
  abstract = {We performed two simulation studies that investigated dimensionality recovery in NPD tetrachoric correlation matrices using parallel analysis. In each study, the NPD matrices were rehabilitated by three smoothing algorithms. In Study 1, we replicated the work by Debelak and Tran on the assessment of dimensionality in one- or two-dimensional common factor models. In Study 2, we extended the Debelak and Tran design in three important ways. Specifically, we investigated: (a) a wider range of factors; (b) models with varying amounts of model error; and (c) models generated from more realistic population item parameters. Our results indicated that matrix smoothing of NPD tetrachoric correlation matrices improves the performance of parallel analysis with binary data. However, these improvements were modest and often of trivial size. To demonstrate the effect of matrix smoothing on an empirical data set, we applied parallel analysis and factor analysis to Adjective Checklist data from the California Twin Registry.},
  copyright = {All rights reserved},
  keywords = {binary items,dimensionality recovery,Matrix smoothing,non-positive definite,parallel analysis},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2020.1859350},
  file = {/home/justin/Zotero/storage/33ZJ339X/Kracht_Waller_2020_Assessing dimensionality in non-positive definite tetrachoric correlation.pdf;/home/justin/Zotero/storage/6YAVHS34/00273171.2020.html}
}

@article{kruyen2014,
  title = {Assessing Individual Change Using Short Tests and Questionnaires},
  author = {Kruyen, Peter M and Emons, Wilco HM and Sijtsma, Klaas},
  year = {2014},
  journal = {Applied Psychological Measurement},
  volume = {38},
  number = {3},
  pages = {201--216},
  doi = {10/gjrkcw}
}

@article{kshirsagar1959,
  ids = {kshirsagar1959bartlett},
  title = {Bartlett Decomposition and {{Wishart}} Distribution},
  author = {Kshirsagar, A. M.},
  year = {1959},
  journal = {The Annals of Mathematical Statistics},
  volume = {30},
  number = {1},
  pages = {239--241},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  doi = {10/d75tsv},
  x-fetchedfrom = {Google Scholar}
}

@article{kuncel2001,
  title = {A Comprehensive Meta-Analysis of the Predictive Validity of the {{Graduate Record Examinations}}: {{Implications}} for Graduate Student Selection and Performance},
  shorttitle = {A Comprehensive Meta-Analysis of the Predictive Validity of the {{Graduate Record Examinations}}},
  author = {Kuncel, Nathan R. and Hezlett, Sarah A. and Ones, Deniz S.},
  year = {2001},
  journal = {Psychological Bulletin},
  volume = {127},
  number = {1},
  pages = {162--181},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10/cffkhc},
  abstract = {This meta-analysis examined the validity of the Graduate Record Examinations (GRE) and undergraduate grade point average (UGPA) as predictors of graduate school performance. The study included samples from multiple disciplines, considered different criterion measures, and corrected for statistical artifacts. Data from 1,753 independent samples were included in the meta-analysis, yielding 6,589 correlations for 8 different criteria and 82,659 graduate students. The results indicated that the GRE and UGPA are generalizably valid predictors of graduate grade point average, 1st-year graduate grade point average, comprehensive examination scores, publication citation counts, and faculty ratings. GRE correlations with degree attainment and research productivity were consistently positive; however, some lower 90\% credibility intervals included 0. Subject Tests tended to be better predictors than the Verbal, Quantitative, and Analytical tests. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Academic Achievement Prediction,College Academic Achievement,Graduate Record Examination,Graduate Students,Predictive Validity,Test Scores},
  file = {/home/justin/Zotero/storage/VKTBSMAQ/Kuncel et al. - 2001 - A comprehensive meta-analysis of the predictive va.pdf;/home/justin/Zotero/storage/DB76S7NJ/2001-16276-008.html}
}

@book{kuncel2012,
  title = {Predictive {{Bias}} in {{Work}} and {{Educational Settings}}},
  author = {Kuncel, Nathan R. and Klieger, David M.},
  year = {2012},
  month = mar,
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780199732579.013.0020},
  file = {/home/justin/Zotero/storage/JKAVKX5K/Kuncel and Klieger - 2012 - Predictive Bias in Work and Educational Settings.pdf}
}

@article{kuncel2020,
  title = {Decoy {{Effects Improve Diversity Hiring}}},
  author = {Kuncel, Nathan and Dahlke, Jeffrey},
  year = {2020},
  month = jul,
  journal = {Personnel Assessment and Decisions},
  volume = {6},
  number = {2},
  issn = {23778822},
  doi = {10/gjrkdh},
  file = {/home/justin/Zotero/storage/CZ44INYM/Kuncel and Dahlke - 2020 - Decoy Effects Improve Diversity Hiring.pdf}
}

@article{kusan2010,
  title = {The Use of Fuzzy Logic in Predicting House Selling Price},
  author = {Ku{\c s}an, Hakan and Aytekin, Osman and {\"O}zdemir, {\.I}lker},
  year = {2010},
  month = mar,
  journal = {Expert Systems with Applications},
  volume = {37},
  number = {3},
  pages = {1808--1813},
  issn = {0957-4174},
  doi = {10/d3zr9j},
  abstract = {In this paper, a new grading model has been developed for prediction of the selling price of house-building. Fuzzy logic systems, considering the city plans, the nearness to cultural, medical, training and educational buildings, the public transportations systems, the other environmental factors and the increased technological upgrading deals with information about construction, have been employed in order to construct the model and achieve the aim. Such factors are used as the inputs. Besides, a questionnaire application including these factors has been applied to determine the values of fuzzy training and testing sets. In this way, the constructed model has been applied to the prediction of selling prices of houses located in different regions of Eski\c{s}ehir city in Turkey. The predicted values and real selling prices determined by selling market have been compared with each other. Consequently, real selling price of house-building has shown variety with respect to the regional aspects and salesmen. The suggested fuzzy logic model can be capable and usable for similar applications.},
  langid = {english},
  keywords = {Fuzzy logic,House price,Local aspects,Prediction model,Real estate market},
  file = {/home/justin/Zotero/storage/CENPLTZN/S0957417409006885.html}
}

@article{kwon2017,
  title = {A {{Social Embeddedness Perspective}} on {{Turnover Intention}}: {{The Role}} of {{Informal Networks}} and {{Social Identity Evidence From South Korea}}},
  shorttitle = {A {{Social Embeddedness Perspective}} on {{Turnover Intention}}},
  author = {Kwon, Hyang Won},
  year = {2017},
  journal = {Public Personnel Management},
  volume = {46},
  number = {3},
  pages = {263--287},
  publisher = {{SAGE Publications Inc}},
  issn = {0091-0260},
  doi = {10/gj733p},
  abstract = {This study examines turnover intention through a social embeddedness perspective proposing that turnover intention may be a function of the degree to which an organization's members are attached to one another in terms of relational ties and emotional bonds. Drawing on network theory and social identity theory, it was hypothesized that peripheral positions in informal networks (solidarity ties and instrumental ties) and marginal identity in the workplace may influence higher turnover intention. Sequential mixed methods design was utilized to explore the context-specific bases upon which informal networks and social identities can form and to test the generality of the link between the explored bases and turnover intention against larger samples using Ordered Logistic Model. The results showed that (a) peripherally positioned individuals in informal networks will likely have high turnover intention and (b) individuals with marginal identity in the workplace will likely have high turnover intention. The study results suggest that the social factors accrued from informal networks and social identities deserve enhanced attention in both theorization and personnel management.},
  langid = {english},
  keywords = {informal networks,mixed methods design,social embeddedness,social identity,turnover intention},
  file = {/home/justin/Zotero/storage/L2RITK5I/Kwon_2017_A Social Embeddedness Perspective on Turnover Intention.pdf}
}

@article{lai2016,
  title = {The Problem with Having Two Watches: {{Assessment}} of Fit When {{RMSEA}} and {{CFI}} Disagree},
  shorttitle = {The Problem with Having Two Watches},
  author = {Lai, Keke and Green, Samuel B.},
  year = {2016},
  journal = {Multivariate Behavioral Research},
  volume = {51},
  number = {2-3},
  pages = {220--239},
  issn = {0027-3171, 1532-7906},
  doi = {10.1080/00273171.2015.1134306},
  langid = {english},
  file = {/home/justin/Zotero/storage/HG593VW9/Lai_Green_2016_The Problem with Having Two Watches.pdf}
}

@article{lai2017,
  title = {Standardized Parameters in Misspecified Structural Equation Models: {{Empirical}} Performance in Point Estimates, Standard Errors, and Confidence Intervals},
  shorttitle = {Standardized Parameters in Misspecified Structural Equation Models},
  author = {Lai, Keke and Zhang, Xiaoguang},
  year = {2017},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {24},
  number = {4},
  pages = {571--584},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/gcph63},
  abstract = {In practice, models always have misfit, and it is not well known in what situations methods that provide point estimates, standard errors (SEs), or confidence intervals (CIs) of standardized structural equation modeling (SEM) parameters are trustworthy. In this article we carried out simulations to evaluate the empirical performance of currently available methods. We studied maximum likelihood point estimates, as well as SE estimators based on the delta method, nonparametric bootstrap (NP-B), and semiparametric bootstrap (SP-B). For CIs we studied Wald CI based on delta, and percentile and BCa intervals based on NP-B and SP-B. We conducted simulation studies using both confirmatory factor analysis and SEM models. Depending on (a) whether point estimate, SE, or CI is of interest; (b) amount of model misfit; (c) sample size; and (d) model complexity, different methods can be the one that renders best performance. Based on the simulation results, we discuss how to choose proper methods in practice.},
  keywords = {confidence interval,misspecified model,standard error,standardized SEM parameters},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2016.1263155},
  file = {/home/justin/Zotero/storage/DMKAJNMV/Lai_Zhang_2017_Standardized Parameters in Misspecified Structural Equation Models.pdf;/home/justin/Zotero/storage/3XA26NPN/10705511.2016.html}
}

@article{lai2017a,
  ids = {lai2017},
  title = {Understanding the {{Impact}} of {{Partial Factorial Invariance}} on {{Selection Accuracy}}: {{An R Script}}},
  shorttitle = {Understanding the {{Impact}} of {{Partial Factorial Invariance}} on {{Selection Accuracy}}},
  author = {Lai, Mark H. C. and Kwok, Oi-man and Yoon, Myeongsun and Hsiao, Yu-Yu},
  year = {2017},
  month = sep,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {24},
  number = {5},
  pages = {783--799},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/gcph7f},
  abstract = {Much of the previous literature on partial measurement invariance has focused on (a) statistically detecting noninvariance, and (b) modeling partial invariance to obtain correct inferences for latent mean comparisons across groups in a single research study. However, very little guidance is provided on the practical implications of partial invariance on the instrument itself in the context of selection. In a frequently cited paper, Millsap and Kwok (2004) provided a framework for evaluating the impact of partial invariance by quantifying the magnitude of noninvariance on the efficacy of the test for selection purposes, yet our literature review found that only a few of the citations have fully captured the essence of Millsap and Kwok's method. In this article, we briefly review the selection accuracy analysis for partial invariance and provide a user-friendly R script (also available as a Web application) that takes parameter estimates as input, automatically produces summary statistics for evaluating selection accuracy, and generates a graph for visualizing the results. Hypothetical and real data examples are provided to illustrate the use of the R script. The goal of this article is to help readers understand Millsap and Kwok's framework of evaluating the impact of partial invariance through an accessible computer program and step-by-step demonstrations of the selection accuracy analysis.},
  keywords = {factorial invariance,measurement invariance,partial invariance,selection accuracy},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2017.1318703},
  file = {/home/justin/Zotero/storage/PNP5FP8W/10705511.2017.html}
}

@article{lai2017b,
  title = {Graphical Displays for Understanding {{SEM}} Model Similarity},
  author = {Lai, Keke and Green, Samuel B. and Levy, Roy},
  year = {2017},
  month = nov,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {24},
  number = {6},
  pages = {803--818},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/ggjrqn},
  abstract = {Relationships between structural equation models are almost exclusively characterized as categorical. However, a simple yes\textendash no answer as to whether 2 models are equivalent or nested (or other categorical descriptors of relationships) cannot address whether the models tend to fit similarly across a wide range of data, or how the characteristics of models and data affect the differences in fit. Therefore, such simple answers have limited value for model selection and evaluation. In this article we proposed a quantitative framework that allows for graphical display of results, incorporates the categorical distinctions in model similarity, is applicable to any model pairs, and offers a new method to generate data for studying model similarity. The framework is straightforward to implement and does not require complex analyses. The information this framework provides is diagnostic and can help researchers identify and explain how key characteristics in data and model parameters lead to (dis)similarity in fit.},
  keywords = {equivalent models,model comparison,model similarity,near-equivalent models},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2017.1334206},
  file = {/home/justin/Zotero/storage/WZ9SP2D9/Lai et al_2017_Graphical Displays for Understanding SEM Model Similarity.pdf;/home/justin/Zotero/storage/Y6P6XXE3/10705511.2017.html}
}

@article{lai2018,
  title = {Estimating Standardized {{SEM}} Parameters given Nonnormal Data and Incorrect Model: {{Methods}} and Comparison},
  shorttitle = {Estimating Standardized Sem Parameters given Nonnormal Data and Incorrect Model},
  author = {Lai, Keke},
  year = {2018},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {25},
  number = {4},
  pages = {600--620},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/gc4pjh},
  abstract = {When both model misspecifications and nonnormal data are present, it is unknown how trustworthy various point estimates, standard errors (SEs), and confidence intervals (CIs) are for standardized structural equation modeling parameters. We conducted simulations to evaluate maximum likelihood (ML), conventional robust SE estimator (MLM), Huber\textendash White robust SE estimator (MLR), and the bootstrap (BS). We found (a) ML point estimates can sometimes be quite biased at finite sample sizes if misfit and nonnormality are serious; (b) ML and MLM generally give egregiously biased SEs and CIs regardless of the degree of misfit and nonnormality; (c) MLR and BS provide trustworthy SEs and CIs given medium misfit and nonnormality, but BS is better; and (d) given severe misfit and nonnormality, MLR tends to break down and BS begins to struggle.},
  keywords = {incorrect model,nonnormal data,robust methods,standard errors,standardized model parameters},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2017.1392248},
  file = {/home/justin/Zotero/storage/59MNC2MX/Lai_2018_Estimating Standardized SEM Parameters Given Nonnormal Data and Incorrect Model.pdf}
}

@article{lai2019,
  ids = {laiCreatingMisspecifiedModels2019},
  title = {Creating Misspecified Models in Moment Structure Analysis},
  author = {Lai, Keke},
  year = {2019},
  month = sep,
  journal = {Psychometrika},
  volume = {84},
  number = {3},
  pages = {781--801},
  issn = {1860-0980},
  doi = {10/gjrkdb},
  abstract = {To understand how SEM methods perform in practice where models always have misfit, simulation studies often involve incorrect models. To create a wrong model, traditionally one specifies a perfect model first and then removes some paths. This approach becomes difficult or even impossible to implement in moment structure analysis and fails to control the amounts of misfit separately and precisely for the mean and covariance parts. Most importantly, this approach assumes a perfect model exists and wrong models can eventually be made perfect, whereas in practice models are all implausible if taken literally and at best provide approximations of the real world. To improve the traditional approach, we propose a more realistic and flexible way to create model misfit for multiple group moment structure analysis. Given (a) the model \$\$\textbackslash varvec\{\{\{\textbackslash upmu \}\}\} (\textbackslash cdot ) \$\${$\mu$}({$\cdot$}) and \$\$\textbackslash varvec\{\{\textbackslash Sigma \}\} (\textbackslash cdot ) \$\${$\Sigma$}({$\cdot$}), (b) population model parameters \$\$\textbackslash varvec\{\{\{\textbackslash uptheta \}\}\} \_0\$\$\texttheta 0, and (c) \$\$F\_1\$\$F1 and \$\$F\_2\$\$F2 specified by the researcher, our method creates \$\$\textbackslash varvec\{\{\{\textbackslash upmu \}\}\} \^*\$\${$\mu{_\ast}$} and \$\$\textbackslash varvec\{\{\textbackslash Sigma \}\} \^*\$\${$\Sigma{_\ast}$} to simultaneously satisfy (a) \$\$\textbackslash varvec\{\{\{\textbackslash uptheta \}\}\} \_0 = \textbackslash arg \textbackslash min F[\textbackslash varvec\{\{\{\textbackslash upmu \}\}\} \^*, \textbackslash varvec\{\{\textbackslash Sigma \}\} \^*; \textbackslash varvec\{\{\{\textbackslash upmu \}\}\} (\textbackslash cdot ), \textbackslash varvec\{\{\textbackslash Sigma \}\} (\textbackslash cdot )]\$\$\texttheta 0=argminF[{$\mu{_\ast}$},{$\Sigma{_\ast}$};{$\mu$}({$\cdot$}),{$\Sigma$}({$\cdot$})], (b) the mean structure's misfit equals \$\$F\_1\$\$F1, and (c) the covariance structure's misfit equals \$\$F\_2\$\$F2.},
  langid = {english},
  file = {/home/justin/Zotero/storage/UXQIDHEF/Lai - 2019 - Creating Misspecified Models in Moment Structure A.pdf}
}

@article{lai2019a,
  title = {A Simple Analytic Confidence Interval for {{CFI}} given Nonnormal Data},
  author = {Lai, Keke},
  year = {2019},
  month = sep,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {26},
  number = {5},
  pages = {757--777},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/gh7r8d},
  abstract = {The comparative fit index (CFI) is one of the most widely-used fit indices in structural equation modeling (SEM). When applying the CFI to model evaluation, although it is universally recognized that the focus should be the population fit, in practice one often considers only the CFI value within a sample and neglects the uncertainty in point estimation. Confidence interval (CI) methods for CFI appeared only recently, but these methods assume multivariate normality, which often fails to hold in practice. In addition, the current methods are applications of the bootstrap and are thus computationally intensive. To better handle nonnormal data and simplify CI construction, in this paper we propose an analytic CI method for CFI without assuming normality. We then carry out simulation studies to compare the new and current methods at various levels of model misfit and nonnormality. Simulation results verify the effectiveness and advantages of the new method.},
  keywords = {confidence interval,model misspecification,nonnormal data,The comparative fit index},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2018.1562351},
  file = {/home/justin/Zotero/storage/JQBC4STW/Lai_2019_A Simple Analytic Confidence Interval for CFI Given Nonnormal Data.pdf;/home/justin/Zotero/storage/RY4UWLZP/10705511.2018.html}
}

@article{lai2020,
  title = {Better Confidence Intervals for {{RMSEA}} in Growth Models given Nonnormal Data},
  author = {Lai, Keke},
  year = {2020},
  month = mar,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {27},
  number = {2},
  pages = {255--274},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/gh7r8k},
  abstract = {Currently, the best confidence interval (CI) for RMSEA in covariance structure analysis given nonnormal data is proposed by Brosseau-Liard, Savalei, and Li (BSL). A key assumption for the BSL CI often overlooked is that all the nonzero eigenvalues are equal in a matrix related to the model and data nonnormality. This assumption rarely holds in practice, especially for mean and covariance structure analysis, and violating this assumption can entail serious mistakes when the model's degrees of freedom are small. One important application of moment structure analysis with small degrees of freedom is growth models. In this paper, we propose a new CI method for RMSEA in growth models given nonnormal data, without assuming equal eigenvalues. Although we focus on growth models, our method applies to any other models in moment structure analysis. Simulation results verify the new method is trustworthy and better than all the current methods.},
  keywords = {confidence interval,nonnormal data,robust method,The root mean square error of approximation},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2019.1643246},
  file = {/home/justin/Zotero/storage/3DMCFTGK/Lai_2020_Better Confidence Intervals for RMSEA in Growth Models given Nonnormal Data.pdf}
}

@article{lai2020a,
  title = {Correct Point Estimator and Confidence Interval for {{RMSEA}} given Categorical Data},
  author = {Lai, Keke},
  year = {2020},
  month = sep,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {27},
  number = {5},
  pages = {678--695},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/gh7r82},
  abstract = {RMSEA estimation given nonnormal continuous data is usually based on the mean-adjusted ( T M ) or mean-variance-adjusted ( T M V ) chi-square statistic, but a plain application of these statistics has poor performance. Savalei and colleagues gave a better way (the BSL method) to infer RMSEA using T M or T M V . However, the BSL method is applicable to continuous data only. For categorical data, currently RMSEA inference is still based on a plain application of T M or T M V , but such practice is already problematic under continuous data. In this paper, we first show that it is more meaningful to define RMSEA under unweighted least squares (ULS) than under weighted least squares (WLS) or diagonally weighted least squares (DWLS). Then, we propose a correct point estimator and confidence interval for RMSEA given categorical data and ULS. Simulation results show our methods perform well while all the traditional methods break down.},
  keywords = {Categorical data,confidence interval,the root mean square error of approximation,weighted least squares},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2019.1687302},
  file = {/home/justin/Zotero/storage/4YRHE6L4/Lai_2020_Correct Point Estimator and Confidence Interval for RMSEA Given Categorical Data.pdf;/home/justin/Zotero/storage/666G7DKE/10705511.2019.html}
}

@article{lai2020b,
  title = {Correct Estimation Methods for {{RMSEA}} under Missing Data},
  author = {Lai, Keke},
  year = {2020},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {0},
  number = {0},
  pages = {1--12},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/gh7r9d},
  abstract = {The full information maximum likelihood (FIML) and the two-stage (TS) procedure are two popular likelihood-based approaches to estimating structural equation model (SEM) parameters with missing data. Although both FIML and TS can give consistent model parameter estimates and standard errors, the RMSEA point estimate and confidence interval (CI) based on these two procedures are problematic. In this paper, we propose a correct point estimator and a correct CI for the population RMSEA, given missing (completely) at random normal data. We first explain with theoretical derivations why the new methods are correct but the traditional methods fail. Then we conduct three simulation studies to compare the methods using various model types (CFA; SEM; growth), missing mechanisms (MAR; MCAR; mixture of MAR and MCAR), missing data amounts, and sample sizes that are typical in the social sciences. Results indicate that the new methods perform well whereas all the traditional methods break down.},
  keywords = {confidence interval,missing data,point estimation,RMSEA},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2020.1755864},
  file = {/home/justin/Zotero/storage/PLVEXWRZ/Lai_2020_Correct Estimation Methods for RMSEA Under Missing Data.pdf}
}

@article{lai2020c,
  title = {Fit Difference between Nonnested Models given Categorical Data: {{Measures}} and Estimation},
  shorttitle = {Fit Difference between Nonnested Models given Categorical Data},
  author = {Lai, Keke},
  year = {2020},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {0},
  number = {0},
  pages = {1--22},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/gh7r9f},
  abstract = {In structural equation modeling (SEM) studies with categorical data, researchers often use the root mean square error of approximation (RMSEA), comparative fit index (CFI), or standardized root mean squared residual (SRMR) to compare rival models. Model selection based on {$\Delta$} R M S E A , {$\Delta$} C F I , or {$\Delta$} S R M R is meaningful because (a) finding a better model is more scientific and easier than establishing a ``good'' model, (b) it avoids the problems with cutoffs for fit indices, (c) one is less likely to overlook other equally substantively plausible models, and (d) information criteria (e.g., AIC, BIC) are not applicable to categorical data SEM. In this paper, we propose point estimators and confidence intervals (CIs) for {$\Delta$} R M S E A , {$\Delta$} C F I , and {$\Delta$} S R M R under categorical data. Our methods are applicable to nonnested models and do not need a true model. Simulation results show our point estimators and CIs are all trustworthy, whereas the bias is large when estimating {$\Delta$} R M S E A ( {$\Delta$} C F I , {$\Delta$} S R M R ) based on the common estimators in the current literature for RMSEA (CFI, SRMR).},
  keywords = {categorical data,confidence interval,Fit index,nonnested models},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2020.1763802},
  file = {/home/justin/Zotero/storage/N9KH2CDZ/Lai_2020_Fit Difference Between Nonnested Models Given Categorical Data.pdf}
}

@article{lam2016,
  title = {Nonparametric Eigenvalue-Regularized Precision or Covariance Matrix Estimator},
  author = {Lam, Clifford},
  year = {2016},
  month = jun,
  journal = {Annals of Statistics},
  volume = {44},
  number = {3},
  pages = {928--953},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10/gjrkc5},
  abstract = {We introduce nonparametric regularization of the eigenvalues of a sample covariance matrix through splitting of the data (NERCOME), and prove that NERCOME enjoys asymptotic optimal nonlinear shrinkage of eigenvalues with respect to the Frobenius norm. One advantage of NERCOME is its computational speed when the dimension is not too large. We prove that NERCOME is positive definite almost surely, as long as the true covariance matrix is so, even when the dimension is larger than the sample size. With respect to the Stein's loss function, the inverse of our estimator is asymptotically the optimal precision matrix estimator. Asymptotic efficiency loss is defined through comparison with an ideal estimator, which assumed the knowledge of the true covariance matrix. We show that the asymptotic efficiency loss of NERCOME is almost surely 0 with a suitable split location of the data. We also show that all the aforementioned optimality holds for data with a factor structure. Our method avoids the need to first estimate any unknowns from a factor model, and directly gives the covariance or precision matrix estimator, which can be useful when factor analysis is not the ultimate goal. We compare the performance of our estimators with other methods through extensive simulations and real data analysis.},
  langid = {english},
  mrnumber = {MR3485949},
  zmnumber = {1341.62124},
  keywords = {covariance matrix,data splitting,factor model,High dimensional data analysis,nonlinear shrinkage,Stieltjes transform},
  file = {/home/justin/Zotero/storage/ZSXY4TNV/Lam - 2016 - Nonparametric eigenvalue-regularized precision or .pdf;/home/justin/Zotero/storage/UCN7WQXS/1460381682.html}
}

@article{laughlin1979,
  title = {A {{Bayesian}} Alternative to Least Squares and Equal Weighting Coefficients in Regression},
  author = {Laughlin, James E.},
  year = {1979},
  month = sep,
  journal = {Psychometrika},
  volume = {44},
  number = {3},
  pages = {271--288},
  issn = {1860-0980},
  doi = {10/dt5t4j},
  abstract = {This paper details a Bayesian alternative to the use of least squares and equal weighting coefficients in regression. An equal weight prior distribution for the linear regression parameters is described with regard to the conditional normal regression model, and resulting posterior distributions for these parameters are detailed. Some interesting connections between this Bayesian procedure and several other methods for estimating optimal weighting coefficients are discussed. In addition, results are presented of a Monte Carlo investigation which compared the effectiveness of the Bayesian procedure relative to least squares, equal weight, ridge, and Bayesian exchangeability estimations.},
  langid = {english},
  file = {/home/justin/Zotero/storage/YBWJZ2W6/Laughlin_1979_A bayesian alternative to least squares and equal weighting coefficients in.pdf}
}

@article{laurent1995,
  title = {On a Positive Semidefinite Relaxation of the Cut Polytope},
  author = {Laurent, Monique and Poljak, Svatopluk},
  year = {1995},
  journal = {Linear Algebra and its Applications},
  volume = {223},
  pages = {439--461},
  doi = {10/dsm2qf}
}

@article{laurent1996,
  title = {On the Facial Structure of the Set of Correlation Matrices},
  author = {Laurent, Monique and Poljak, Svatopluk},
  year = {1996},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {17},
  number = {3},
  pages = {530--547},
  doi = {10/br452f}
}

@article{lee1994,
  title = {Covariance and Correlation Structure Analyses with Continuous and Polytomous Variables},
  author = {Lee, Sik-Yum and Poon, Wai-Yin and Bentler, Peter M.},
  year = {1994},
  journal = {Lecture Notes-Monograph Series},
  volume = {24},
  pages = {347--358},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0749-2170},
  doi = {10/dwxfsx},
  abstract = {The main purpose of this paper is to develop two-stage methods for covariance and correlation structure analyses with continuous and polytomous variables. A full maximum likelihood approach and a partition maximum likelihood approach are used to attain the first stage estimates of the thresholds and the underlying covariance or correlation matrix. Then based on the joint asymptotic distributions of the first stage estimators and appropriate weight matrices, the generalized least squares approach is employed to estimate the structures parameters in the covariance or the correlation structures. Asymptotic properties of the estimates are derived. A simulation study is reported to give some ideas about the accuracy and the asymptotic behaviors of the method.},
  abstractnote = {The main purpose of this paper is to develop two-stage methods for covariance and correlation structure analyses with continuous and polytomous variables. A full maximum likelihood approach and a partition maximum likelihood approach are used to attain the first stage estimates of the thresholds and the underlying covariance or correlation matrix. Then based on the joint asymptotic distributions of the first stage estimators and appropriate weight matrices, the generalized least squares approach is employed to estimate the structures parameters in the covariance or the correlation structures. Asymptotic properties of the estimates are derived. A simulation study is reported to give some ideas about the accuracy and the asymptotic behaviors of the method.},
  file = {/home/justin/Zotero/storage/6XU2574R/Lee et al. - 1994 - Covariance and Correlation Structure Analyses with.pdf}
}

@article{lee2012,
  title = {Ordinary Least Squares Estimation of Parameters in Exploratory Factor Analysis with Ordinal Data},
  author = {{Lee}},
  year = {2012},
  journal = {Multivariate Behavioral Research},
  doi = {10/ghx8x2}
}

@phdthesis{lee2013,
  ids = {lee2013a},
  title = {An Empirical Evaluation of Three Procedures of Confirmatory Factor Analysis with Ordinal Data},
  author = {Lee, Chun-Ting},
  year = {2013},
  address = {{Notre Dame, IN}},
  abstract = {Confirmatory factor analysis (CFA) is routinely applied to ordinal variables like questionnaire items. Treating ordinal manifest variables as if they were continuous in CFA leads to less-than-optimal results. Three procedures have been proposed to properly fit CFA models with ordinal manifest variables. These three procedures are the polychoric correlation approach (J\"oreskog, 1990, 1994; Muth\'en, 1984), the underlying bivariate normal approach (J\"oreskog and Moustaki, 2001; Xi, 2011), and the item response theory approach (Bock et al., 1988; Cai, 2010a,b). Statistical properties of the polychoric correlation approach under correctly specified models have been well studied. The properties of the underlying bivariate normal approach and the item response theory approach have not been studied and contrasted. The goal of the present dissertation is to empirically evaluate the performances of three procedures under a variety of conditions. To serve the purpose of comparison, the normal theory maximum likelihood approach which ignoring the characteristics of ordinal manifest variables is also examined in the current study. Of particular interests are their respective performances under misspecified CFA models. To examine the performances of different approaches under misspecified CFA models, different levels of model error are introduced into population covariance matrices. The levels of model error are controlled to have RMSEA index with 0, .05, and .08, which falls in the range from perfect fit, close fit and fair fit. The model errors are introduced to the population covariance matrices by the procedure described in Cudeck and Browne (1992). The independent variables manipulated in the simulation studies include the level of model misspecification, sample size, model size, number of response categories, and the distribution of ordinal variables. The outcome variables include point estimates, standard error estimates, and confidence intervals. The chi-square test statistics and fit measures are also examined to understand the influences of the factors on model fit. Guidelines for substantive researchers are provided. The results indicate that across the manipulated conditions, the composite likelihood function approach provides satisfactory performances in terms of the relative bias of point and standard error estimates, empirical coverage rates of confidence intervals, and empirical rejection rates of overall model fit test statistic. The item response theory approach provides similar results as the composite likelihood function approach. The polychoric correlation approach can also provides unbiased point estimates for model parameters. And the normal theory maximum likelihood approach is not recommended to apply with the ordinal manifest variables.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9781303775864},
  langid = {english},
  school = {University of Notre Dame},
  keywords = {Confirmatory factor analysis,Empirical evaluation,Ordinal data,Procedures,Psychology},
  file = {/home/justin/Zotero/storage/C8UX6RRE/Lee_2013_An empirical evaluation of three procedures of confirmatory factor analysis.pdf}
}

@article{lee2016,
  title = {An Overview of the {{Normal Ogive Harmonic Analysis Robust Method}} ({{NOHARM}}) Approach to Item Response Theory},
  author = {Lee, J. J. and Lee, M. K.},
  year = {2016},
  journal = {The Quantitative Methods for Psychology},
  volume = {12},
  number = {1},
  pages = {1--8},
  issn = {2292-1354},
  doi = {10/ggfnxs},
  abstract = {Here we provide a description of the IRT estimation method known as Normal Ogive Harmonic Analysis Robust Method (NOHARM). Although in some ways this method has been superseded by new computer programs that also adopt a specifically factor-analytic approach, its fundamental principles remain useful in certain applications, which include calculating the residual covariance matrix and rescaling the distribution of the common factor (latent trait). These principles can be applied to parameter estimates obtained by any method.},
  langid = {english},
  file = {/home/justin/Zotero/storage/2I9UL3R6/Lee and Lee - 2016 - An overview of the Normal Ogive Harmonic Analysis .pdf}
}

@article{lewis2002,
  title = {An {{Empirical Analysis}} of {{Stressors}} for {{Gay Men}} and {{Lesbians}}},
  author = {Lewis, Robin J. and Derlega, Valerian J. and Berndt, Andrea and Morris, Lynn M. and Rose, Suzanna},
  year = {2002},
  month = mar,
  journal = {Journal of Homosexuality},
  volume = {42},
  number = {1},
  pages = {63--88},
  publisher = {{Routledge}},
  issn = {0091-8369},
  doi = {10/cjscr9},
  abstract = {This research describes the empirical classification of stressors for gay men and lesbians. Volunteer respondents were recruited through a free local gay and lesbian newspaper, through gay and lesbian student organizations nationwide, through gay and lesbian bookstores nationwide, and at a gay festival in St. Louis. Nine hundred seventy-nine (979) participants completed a 70-item measure with stressors that had been identified in previous qualitative research. Participants were asked to indicate the degree to which they had experienced stress associated with a variety of experiences. Participants also completed a measure of dysphoria (CES-D), responded about their degree of openness regarding sexual orientation, and provided information about their relationship status and involvement with gay groups and activities. Using confirmatory factor analysis, a six-factor model was predicted to account for the data. One-factor, six-factor, and ten-factor models were tested. The ten-factor model yielded the best fit with the data and accounted for 63.5\% of the variance. The factor structure remained stable when gay men were compared to lesbians, when those endorsing a predominantly gay versus exclusively gay orientation were compared, and when those in a relationship were compared to those who were not in a relationship. Increased gay stress was associated with more dysphoria. Implications of these findings are discussed and directions for future research are considered.},
  keywords = {Gay stress,gays,sexual orientation,stress},
  annotation = {\_eprint: https://doi.org/10.1300/J082v42n01\_04},
  file = {/home/justin/Zotero/storage/GC82DA7L/PhD et al. - 2002 - An Empirical Analysis of Stressors for Gay Men and.pdf;/home/justin/Zotero/storage/L98G52LD/J082v42n01_04.html}
}

@article{li1994,
  title = {A Note on Extreme Correlation Matrices},
  author = {Li, Chi-Kwong and Tam, Bit-Shun},
  year = {1994},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {15},
  number = {3},
  pages = {903--908},
  doi = {10/fsfcmp}
}

@inproceedings{li2009,
  title = {A {{SVR}} Based Forecasting Approach for Real Estate Price Prediction},
  booktitle = {2009 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  author = {Li, Da-Ying and Xu, Wei and Zhao, Hong and {Rong-Qiu Chen}},
  year = {2009},
  month = jul,
  volume = {2},
  pages = {970--974},
  issn = {2160-1348},
  doi = {10/fwmgzk},
  abstract = {The support vector machine (SVM), proposed by Vapnik (1995), has been successfully applied to classification, cluster, and forecast. This study proposes support vector regression (SVR) to forecast real estate prices in China. The aim of this paper is to examine the feasibility of SVR in real estate price prediction. To achieve the aim, five indicators are selected as the input variables and real estate price is used as output variable of the SVR. The quarterly data during 1998-2008 are employed as the data set to construct the SVR model. With the scenarios, real estate prices in future are forecasted and analyzed. The forecasting performance of SVR model was also compared with BPNN model. The experimental results demonstrate that based on the mean absolute error (MAE), the mean absolute percentage error (MAPE) and the root mean squared error (RMSE), the SVR model outperforms the BPNN model and the SVR based approach was an efficient tool to forecast real estate prices.},
  keywords = {Cybernetics,Economic forecasting,Economic indicators,Employment,forecast,forecasting theory,Loans and mortgages,Machine learning,Macroeconomics,mean absolute error,mean absolute percentage error,mean square error methods,Predictive models,pricing,real estate data processing,real estate price,real estate price prediction,regression analysis,root mean squared error,support vector machine,Support vector machine (SVM),support vector machines,Support vector machines,support vector regression,SVM,SVR based forecasting,Technology forecasting},
  file = {/home/justin/Zotero/storage/J2U74EEK/5212389.html}
}

@article{li2010,
  title = {Newton's Method for Computing the Nearest Correlation Matrix with a Simple Upper Bound},
  author = {Li, Qingna and Li, Donghui and Qi, Houduo},
  year = {2010},
  journal = {Journal of optimization theory and applications},
  volume = {147},
  number = {3},
  pages = {546--568},
  publisher = {{Springer}},
  doi = {10/bqztq8},
  date-added = {2019-12-11 11:58:09 -0600},
  date-modified = {2019-12-11 11:58:09 -0600}
}

@article{li2010a,
  ids = {li2010},
  title = {Newton's {{Method}} for {{Computing}} the {{Nearest Correlation Matrix}} with a {{Simple Upper Bound}}},
  author = {Li, Qingna and Li, Donghui and Qi, Houduo},
  year = {2010},
  month = dec,
  journal = {Journal of Optimization Theory and Applications},
  volume = {147},
  number = {3},
  pages = {546--568},
  publisher = {{Springer}},
  issn = {1573-2878},
  doi = {10/bqztq8},
  abstract = {The standard nearest correlation matrix can be efficiently computed by exploiting a recent development of Newton's method (Qi and Sun in SIAM J. Matrix Anal. Appl. 28:360\textendash 385, 2006). Two key mathematical properties, that ensure the efficiency of the method, are the strong semismoothness of the projection operator onto the positive semidefinite cone and constraint nondegeneracy at every feasible point. In the case where a simple upper bound is enforced in the nearest correlation matrix in order to improve its condition number, it is shown, among other things, that constraint nondegeneracy does not always hold, meaning Newton's method may lose its quadratic convergence. Despite this, the numerical results show that Newton's method is still extremely efficient even for large scale problems. Through regularization, the developed method is applied to semidefinite programming problems with simple bounds.},
  date-added = {2019-12-11 11:58:09 -0600},
  date-modified = {2019-12-11 11:58:09 -0600},
  langid = {english},
  keywords = {matrix smooth},
  file = {/home/justin/Zotero/storage/LZHQYVWY/Li et al. - 2010 - Newton’s Method for Computing the Nearest Correlat.pdf}
}

@article{li2011,
  title = {A Sequential Semismooth {{Newton}} Method for the Nearest Low-Rank Correlation Matrix Problem},
  author = {Li, Qingna and Qi, Hou-duo},
  year = {2011},
  journal = {SIAM Journal on Optimization},
  volume = {21},
  number = {4},
  pages = {1641--1666},
  doi = {10/b2n4n8},
  keywords = {matrix smooth}
}

@article{li2016,
  title = {The Performance of {{ML}}, {{DWLS}}, and {{ULS}} Estimation with Robust Corrections in Structural Equation Models with Ordinal Variables},
  author = {Li, Cheng-Hsien},
  year = {2016},
  journal = {Psychological Methods},
  volume = {21},
  number = {3},
  pages = {369--387},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10/gf895p},
  abstract = {Three estimation methods with robust corrections\textemdash maximum likelihood (ML) using the sample covariance matrix, unweighted least squares (ULS) using a polychoric correlation matrix, and diagonally weighted least squares (DWLS) using a polychoric correlation matrix\textemdash have been proposed in the literature, and are considered to be superior to normal theory-based maximum likelihood when observed variables in latent variable models are ordinal. A Monte Carlo simulation study was carried out to compare the performance of ML, DWLS, and ULS in estimating model parameters, and their robust corrections to standard errors, and chi-square statistics in a structural equation model with ordinal observed variables. Eighty-four conditions, characterized by different ordinal observed distribution shapes, numbers of response categories, and sample sizes were investigated. Results reveal that (a) DWLS and ULS yield more accurate factor loading estimates than ML across all conditions; (b) DWLS and ULS produce more accurate interfactor correlation estimates than ML in almost every condition; (c) structural coefficient estimates from DWLS and ULS outperform ML estimates in nearly all asymmetric data conditions; (d) robust standard errors of parameter estimates obtained with robust ML are more accurate than those produced by DWLS and ULS across most conditions; and (e) regarding robust chi-square statistics, robust ML is inferior to DWLS and ULS in controlling for Type I error in almost every condition, unless a large sample is used (N = 1,000). Finally, implications of the findings are discussed, as are the limitations of this study as well as potential directions for future research. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Least Squares,Maximum Likelihood,Statistical Estimation},
  file = {/home/justin/Zotero/storage/IANJ95MZ/2016-41158-001.html}
}

@article{li2016a,
  title = {Confirmatory Factor Analysis with Ordinal Data: {{Comparing}} Robust Maximum Likelihood and Diagonally Weighted Least Squares},
  shorttitle = {Confirmatory Factor Analysis with Ordinal Data},
  author = {Li, Cheng-Hsien},
  year = {2016},
  month = sep,
  journal = {Behavior Research Methods},
  volume = {48},
  number = {3},
  pages = {936--949},
  issn = {1554-3528},
  doi = {10/f83h6m},
  langid = {english},
  file = {/home/justin/Zotero/storage/BD26H544/Li - 2016 - Confirmatory factor analysis with ordinal data Co.pdf}
}

@article{li2016b,
  title = {Confirmatory Factor Analysis with Ordinal Data: {{Comparing}} Robust Maximum Likelihood and Diagonally Weighted Least Squares},
  shorttitle = {Confirmatory Factor Analysis with Ordinal Data},
  author = {Li, Cheng-Hsien},
  year = {2016},
  month = sep,
  journal = {Behavior Research Methods},
  volume = {48},
  number = {3},
  pages = {936--949},
  issn = {1554-3528},
  doi = {10/f83h6m},
  langid = {english},
  file = {/home/justin/Zotero/storage/QVDUBQB7/Li_2016_Confirmatory factor analysis with ordinal data.pdf}
}

@article{lievens2019,
  title = {Constructed Response Formats and Their Effects on Minority\textendash Majority Differences and Validity.},
  author = {Lievens, Filip and Sackett, Paul R. and Dahlke, Jeffrey A. and Oostrom, Janneke K. and De Soete, Britt},
  year = {2019},
  month = may,
  journal = {Journal of Applied Psychology},
  volume = {104},
  number = {5},
  pages = {715--726},
  issn = {1939-1854, 0021-9010},
  doi = {10/gfkbmq},
  langid = {english},
  file = {/home/justin/Zotero/storage/SHXLYFMW/Lievens et al. - 2019 - Constructed response formats and their effects on .pdf}
}

@article{lim2019,
  title = {Determining the Number of Factors Using Parallel Analysis and Its Recent Variants},
  author = {Lim, Sangdon and Jahng, Seungmin},
  year = {2019},
  journal = {Psychological Methods},
  volume = {24},
  number = {4},
  pages = {452--467},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10/gf4jsz},
  abstract = {Parallel analysis (PA) is recommended as one of the best procedures to determine the number of factors but its theoretical justification has long been questioned. The current study discussed theoretical issues on the use of eigenvalues for dimensionality assessment and reviewed the development of PA and its recent variants proposed to address the issues. The performances of 13 different PAs including PA with minimum rank factor analysis, revised PA, and comparison data method were investigated through a Monte Carlo simulation under a wide range of factor structures that produce small factor-representing and nonrepresenting eigenvalues for different types of measurement scales. Results showed that the traditional PA using full correlation matrices performed best in most of the conditions, especially when population error was involved. However, the overall accuracy of PA was not satisfactory when factor-representing eigenvalues were small, that is, when factor loadings were low and factor correlations were high. From these results, we suggest that the original PA be used to determine the number of factors but the estimated number should not be taken as a fixed estimate. The number of factors within {$\pm$}1 range of the estimate can be considered as viable candidates and interpretational validity of the competing models should be consulted for the decision. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Experimental Methods,Factor Analysis,Factor Structure,Measurement,Models,Simulation,Test Construction},
  file = {/home/justin/Zotero/storage/QJX92EYQ/2019-31747-001.html}
}

@article{linn1968,
  title = {A {{Monte Carlo}} Approach to the Number of Factors Problem},
  author = {Linn, Robert L.},
  year = {1968},
  month = mar,
  journal = {Psychometrika},
  volume = {33},
  number = {1},
  pages = {37--71},
  issn = {0033-3123, 1860-0980},
  doi = {10/fg9frw},
  langid = {english},
  file = {/home/justin/Zotero/storage/LPD8FA4Q/Linn_1968_A monte carlo approach to the number of factors problem.pdf}
}

@inproceedings{little1986,
  title = {Missing Data in Census Bureau Surveys},
  booktitle = {Proceedings of the Second Annual {{Census Bureau}} Research Conference},
  author = {Little, Roderick JA},
  year = {1986},
  pages = {442--454},
  keywords = {⛔ No DOI found}
}

@book{lord1968,
  title = {Statistical Theories of Mental Test Scores},
  author = {Lord, F.M. and Novick, M.R.},
  year = {1968},
  series = {Addison-{{Wesley}} Series in Behavioral Sciences: {{Quantitative}} Methods},
  publisher = {{Addison-Wesley Publishing Company}},
  lccn = {gb68009588}
}

@article{lorenzo-seva2000,
  title = {The Weighted Oblimin Rotation},
  author = {{Lorenzo-Seva}, Urbano},
  year = {2000},
  month = sep,
  journal = {Psychometrika},
  volume = {65},
  number = {3},
  pages = {301--318},
  issn = {0033-3123, 1860-0980},
  doi = {10/c6r6th},
  langid = {english}
}

@article{lorenzo-seva2011,
  title = {The {{Hull}} Method for Selecting the Number of Common Factors},
  author = {{Lorenzo-Seva}, Urbano and Timmerman, Marieke E. and Kiers, Henk A. L.},
  year = {2011},
  month = apr,
  journal = {Multivariate Behavioral Research},
  volume = {46},
  number = {2},
  pages = {340--364},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/cr9t2q},
  abstract = {A common problem in exploratory factor analysis is how many factors need to be extracted from a particular data set. We propose a new method for selecting the number of major common factors: the Hull method, which aims to find a model with an optimal balance between model fit and number of parameters. We examine the performance of the method in an extensive simulation study in which the simulated data are based on major and minor factors. The study compares the method with four other methods such as parallel analysis and the minimum average partial test, which were selected because they have been proven to perform well and/or they are frequently used in applied research. The Hull method outperformed all four methods at recovering the correct number of major factors. Its usefulness was further illustrated by its assessment of the dimensionality of the Five-Factor Personality Inventory (Hendriks, Hofstee, \& De Raad, 1999). This inventory has 100 items, and the typical methods for assessing dimensionality prove to be useless: the large number of factors they suggest has no theoretical justification. The Hull method, however, suggested retaining the number of factors that the theoretical background to the inventory actually proposes.},
  pmid = {26741331},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2011.564527},
  file = {/home/justin/Zotero/storage/T43DQSKT/Lorenzo-Seva et al_2011_The Hull Method for Selecting the Number of Common Factors.pdf}
}

@article{lorenzo-seva2012,
  title = {{{TETRA-COM}}: {{A}} Comprehensive {{SPSS}} Program for Estimating the Tetrachoric Correlation},
  shorttitle = {{{TETRA-COM}}},
  author = {{Lorenzo-Seva}, Urbano and Ferrando, Pere J.},
  year = {2012},
  journal = {Behavior Research Methods},
  volume = {44},
  number = {4},
  pages = {1191--1196},
  issn = {1554-3528},
  doi = {10/f4h6rr},
  abstract = {We provide an SPSS program that implements descriptive and inferential procedures for estimating tetrachoric correlations. These procedures have two main purposes: (1) bivariate estimation in contingency tables and (2) constructing a correlation matrix to be used as input for factor analysis (in particular, the SPSS FACTOR procedure). In both cases, the program computes accurate point estimates, as well as standard errors and confidence intervals that are correct for any population value. For purpose (1), the program computes the contingency table together with five other measures of association. For purpose (2), the program checks the positive definiteness of the matrix, and if it is found not to be Gramian, performs a nonlinear smoothing procedure at the user's request. The SPSS syntax, a short manual, and data files related to this article are available as supplemental materials from brm.psychonomic-journals.org/content/supplemental.},
  langid = {english},
  file = {/home/justin/Zotero/storage/F39C4SLW/Lorenzo-Seva_Ferrando_2012_TETRA-COM.pdf}
}

@article{lorenzo-seva2016,
  title = {Multiple Imputation of Missing Values in Exploratory Factor Analysis of Multidimensional Scales: Estimating Latent Trait Scores},
  shorttitle = {Multiple {{Imputation}} of Missing Values in Exploratory Factor Analysis of Multidimensional Scales},
  author = {{Lorenzo-Seva}, Urbano and Ginkel, Joost R. Van},
  year = {2016},
  month = apr,
  journal = {Anales de Psicolog\'ia / Annals of Psychology},
  volume = {32},
  number = {2},
  pages = {596--608},
  issn = {1695-2294},
  doi = {10/dvrt},
  copyright = {Derechos de autor},
  langid = {english},
  keywords = {Análisis factorial exploratorio},
  file = {/home/justin/Zotero/storage/LEDS5SZA/Lorenzo-Seva_Ginkel_2016_Multiple Imputation of missing values in exploratory factor analysis of.pdf;/home/justin/Zotero/storage/H8MS4TZD/analesps.32.2.html}
}

@article{lorenzo-seva2020,
  title = {Unrestricted Factor Analysis of Multidimensional Test Items Based on an Objectively Refined Target Matrix},
  author = {{Lorenzo-Seva}, Urbano and Ferrando, Pere J.},
  year = {2020},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {52},
  number = {1},
  pages = {116--130},
  issn = {1554-3528},
  doi = {10/gh64j4},
  langid = {english},
  file = {/home/justin/Zotero/storage/5CDQVARR/Lorenzo-Seva_Ferrando_2020_Unrestricted factor analysis of multidimensional test items based on an.pdf;/home/justin/Zotero/storage/8E678HMJ/Lorenzo-Seva and Ferrando - 2020 - Unrestricted factor analysis of multidimensional t.pdf}
}

@article{lorenzo-seva2020a,
  ids = {lorenzoseva2020},
  title = {Not Positive Definite Correlation Matrices in Exploratory Item Factor Analysis: {{Causes}}, Consequences and a Proposed Solution},
  shorttitle = {Not {{Positive Definite Correlation Matrices}} in {{Exploratory Item Factor Analysis}}},
  author = {{Lorenzo-Seva}, Urbano and Ferrando, Pere J.},
  year = {2020},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  pages = {1--10},
  issn = {1070-5511, 1532-8007},
  doi = {10/gg9czw},
  doi2 = {10.1080/10705511.2020.1735393},
  langid = {english},
  file = {/home/justin/Zotero/storage/4UAPES2G/Lorenzo-Seva and Ferrando - 2020 - Not Positive Definite Correlation Matrices in Expl.pdf}
}

@techreport{lorenzo-sevaHowReportPercentage2013,
  type = {Technical {{Report}}},
  title = {How to Report the Percentage of Explained Common Variance in Exploratory Factor Analysis},
  author = {{Lorenzo-Seva}, U.},
  year = {2013},
  institution = {{Department of Psychology, Universitat Rovira i Vergili, Tarragona}},
  file = {/home/justin/Zotero/storage/8QB8UPA9/Lorenzo-Seva - 2013 - How to report the percentage of explained common v.pdf}
}

@book{lovaszBackgroundMaterial,
  title = {Background {{Material}}},
  author = {Lovasz, Laszlo; Katalin Vesztergombi}
}

@article{lubbe2019,
  ids = {Lubbe_2019},
  title = {Parallel Analysis with Categorical Variables: {{Impact}} of Category Probability Proportions on Dimensionality Assessment Accuracy},
  shorttitle = {Parallel Analysis with Categorical Variables},
  author = {Lubbe, Dirk},
  year = {2019},
  journal = {Psychological Methods},
  volume = {24},
  number = {3},
  pages = {339--351},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10/ghdxxs},
  abstract = {Parallel analysis (PA) is regarded as one of the most accurate methods to determine the number of factors underlying a set of variables. Commonly, PA is performed on the basis of the variables' product-moment correlation matrix. To improve dimensionality assessments for dichotomous or ordered categorical variables, it has been proposed to replace product-moment correlations with more appropriate coefficients, such as tetrachoric or polychoric correlations. While similar modifications have proven useful for various factor analytic approaches, PA results were not consistently improved. The present article outlines a main reason for this result. Specifically, it explains the dependency of PA results on differing proportions of category probabilities when using tetrachoric or polychoric correlations and shows how to adjust for it by generating appropriate reference eigenvalues. The accuracy of dimensionality assessments of PA accounting for category probability proportions versus not accounting for them is investigated using simulation studies. The results show that the category probability adjusted approach distinctly improves dimensionality assessments. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  pmid = {29745684},
  keywords = {Data Interpretation; Statistical,Exploratory Factor Analysis,Factor Analysis,Factor Analysis; Statistical,Humans,Methodology,Models; Statistical,Principal Component Analysis,Probability,Psychology,Psychometrics,Statistical Correlation,Statistical Probability},
  file = {/home/justin/Zotero/storage/6S3YDEJR/2018-20955-001.html}
}

@article{lubke2003,
  title = {On the Relationship between Sources of Within- and between-Group Differences and Measurement Invariance in the Common Factor Model},
  author = {Lubke, Gitta H. and Dolan, Conor V. and Kelderman, Henk and Mellenbergh, Gideon J.},
  year = {2003},
  month = nov,
  journal = {Intelligence},
  volume = {31},
  number = {6},
  pages = {543--566},
  issn = {0160-2896},
  doi = {10/c9h5s5},
  abstract = {Investigating sources of within- and between-group differences and measurement invariance (MI) across groups is fundamental to any meaningful group comparison based on observed test scores. It is shown that by placing certain restrictions on the multigroup confirmatory factor model, it is possible to investigate the hypothesis that within- and between-group differences are due to the same factors. Moreover, the modeling approach clarifies that absence of measurement bias implies common sources of within- and between-group variation. It is shown how the influence of background variables can be incorporated in the model. The advantages of the modeling approach as compared with other commonly used methods for group comparisons is discussed and illustrated by means of an analysis of empirical data.},
  langid = {english},
  keywords = {Between-group differences,Common factor model,Confirmatory factor analysis,Measurement invariance,Within-group differences},
  file = {/home/justin/Zotero/storage/YVI5DLFA/Lubke et al_2003_On the relationship between sources of within- and between-group differences.pdf;/home/justin/Zotero/storage/NSM5XNUN/S0160289603000515.html}
}

@mastersthesis{lucas2001,
  ids = {lucasComputingNearestCovariance2001},
  title = {Computing {{Nearest Covariance}} and {{Correlation Matrices}}},
  author = {Lucas, Craig},
  year = {2001},
  month = oct,
  abstract = {We look at two matrix nearness problems posed by a finance company, where nearness is measured in the Frobenius norm. Correlation and covariance matrices are computed from sampled stock data with missing entries by a technique that produces matrices that are not positive semidefinite. In the first problem we find the nearest correlation matrix that is positive semidefinite and preserves any correlations known to be exact. In the second problem we investigate how the missing elements in the data should be chosen in order to generate the nearest covariance matrix to the indefinite matrix  from the completed set of data. We show how the former problem can be solved using an alternating projections  algorithm and how the latter problem can be investigated using a multi-directional search optimization method.},
  langid = {english},
  school = {Manchester Institute for Mathematical Sciences, The University of Manchester},
  keywords = {alternating projection methods,correlation,MAP,matrix smooth},
  file = {/home/justin/Zotero/storage/VWGVHK3G/Lucas - 2001 - Computing Nearest Covariance and Correlation Matri.pdf;/home/justin/Zotero/storage/KUKK5FXR/2311.html}
}

@article{lurie1998,
  ids = {lurieApproximateMethodSampling1998},
  title = {An {{Approximate Method}} for {{Sampling Correlated Random Variables}} from {{Partially-Specified Distributions}}},
  author = {Lurie, Philip M. and Goldberg, Matthew S.},
  year = {1998},
  journal = {Management Science},
  volume = {44},
  number = {2},
  pages = {203--218},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  doi = {10/cj7sbc},
  abstract = {This paper presents an algorithm for generating correlated vectors of random numbers. The user need not fully specify the joint distribution function; instead, the user ``partially specifies'' only the marginal distributions and the correlation matrix. The algorithm may be applied to any set of continuous, strictly increasing distribution functions; the marginal distributions need not all be of the same functional form. The correlation matrix is first checked for mathematical consistency (positive semi-definiteness), and adjusted if necessary. Then the correlated random vectors are generated using a combination of Cholesky decomposition and Gauss-Newton iteration. Applications are made to cost analysis, where correlations are often present between cost elements in a work breakdown structure.},
  date-added = {2019-12-11 11:58:25 -0600},
  date-modified = {2019-12-11 11:58:25 -0600},
  keywords = {matrix smooth},
  file = {/home/justin/Zotero/storage/WEC7KTJX/Lurie and Goldberg - 1998 - An Approximate Method for Sampling Correlated Rand.pdf;/home/justin/Zotero/storage/WK5LQ2HA/mnsc.44.2.html}
}

@article{lykken1968,
  title = {Statistical Significance in Psychological Research.},
  author = {Lykken, David T.},
  year = {1968},
  journal = {Psychological Bulletin},
  volume = {70},
  number = {3, Pt.1},
  pages = {151--159},
  issn = {1939-1455, 0033-2909},
  doi = {10/c6942g},
  langid = {english},
  file = {/home/justin/Zotero/storage/J9BYID3B/Lykken - 1968 - Statistical significance in psychological research.pdf}
}

@article{maccallum1991,
  ids = {maccallum1991representing},
  title = {Representing Sources of Error in the Common-Factor Model: {{Implications}} for Theory and Practice.},
  shorttitle = {Representing Sources of Error in the Common-Factor Model},
  author = {MacCallum, Robert C. and Tucker, Ledyard R.},
  year = {1991},
  journal = {Psychological Bulletin},
  volume = {109},
  number = {3},
  pages = {502--511},
  publisher = {{American Psychological Association}},
  issn = {1939-1455, 0033-2909},
  doi = {10/cgsqhv},
  date-added = {2019-12-12 14:59:12 -0600},
  date-modified = {2019-12-12 14:59:12 -0600},
  langid = {english},
  file = {/home/justin/Zotero/storage/L66NXHPG/MacCallum_Tucker_1991_Representing sources of error in the common-factor model.pdf}
}

@article{maccallum1999sample,
  title = {Sample Size in Factor Analysis.},
  author = {MacCallum, Robert C and Widaman, Keith F and Zhang, Shaobo and Hong, Sehee},
  year = {1999},
  journal = {Psychological methods},
  volume = {4},
  number = {1},
  pages = {84},
  publisher = {{American Psychological Association}},
  doi = {10/gjrkcs},
  file = {/home/justin/Zotero/storage/KY4SH3MJ/MacCallum et al. - 1999 - Sample size in factor analysis..pdf}
}

@article{maccallum2000,
  title = {Applications of {{Structural Equation Modeling}} in {{Psychological Research}}},
  author = {MacCallum, Robert C. and Austin, James T.},
  year = {2000},
  journal = {Annual Review of Psychology},
  volume = {51},
  number = {1},
  pages = {201--226},
  doi = {10/bntmh2},
  abstract = {This chapter presents a review of applications of structural equation modeling (SEM) published in psychological research journals in recent years. We focus first on the variety of research designs and substantive issues to which SEM can be applied productively. We then discuss a number of methodological problems and issues of concern that characterize some of this literature. Although it is clear that SEM is a powerful tool that is being used to great benefit in psychological research, it is also clear that the applied SEM literature is characterized by some chronic problems and that this literature can be considerably improved by greater attention to these issues.},
  pmid = {10751970},
  annotation = {\_eprint: https://doi.org/10.1146/annurev.psych.51.1.201},
  file = {/home/justin/Zotero/storage/YLVM5LJ6/MacCallum_Austin_2000_Applications of Structural Equation Modeling in Psychological Research.pdf}
}

@article{maccallum2001,
  title = {Sample Size in Factor Analysis: {{The}} Role of Model Error},
  shorttitle = {Sample Size in Factor Analysis},
  author = {MacCallum, Robert C. and Widaman, Keith F. and Preacher, Kristopher J. and Hong, Sehee},
  year = {2001},
  journal = {Multivariate Behavioral Research},
  volume = {36},
  number = {4},
  pages = {611--637},
  publisher = {{LAWRENCE ERLBAUM ASSOC INC}},
  address = {{10 INDUSTRIAL AVE, MAHWAH, NJ 07430-2262 USA}},
  issn = {0027-3171},
  doi = {10/b3v4p6},
  abstract = {This article examines effects of sample size and other design features on correspondence between factors obtained from analysis of sample data and those present in the population from which the samples were drawn. We extend earlier work on this question by examining these phenomena in the situation in which the common factor model does not hold exactly in the population. We present a theoretical framework for representing such lack of fit and examine its implications in the population and sample. Based on this approach we hypothesize that lack of fit of the model in the population will not, on the average, influence recovery of population factors in analysis of sample data, regardless of degree of model error and regardless of sample size. Rather, such recovery will be affected only by phenomena related to sampling error which have been studied previously. These hypotheses are investigated and verified in two sampling studies, one using artificial data and one using empirical data.},
  date-added = {2019-11-21 12:07:50 -0600},
  date-modified = {2019-12-12 15:00:28 -0600},
  pmid = {26822184},
  times-cited = {357},
  wok-uid = {WOS:000173914200006},
  annotation = {\_eprint: https://doi.org/10.1207/S15327906MBR3604\_06},
  file = {/home/justin/Zotero/storage/3WCXG8IH/MacCallum et al. - 2001 - Sample Size in Factor Analysis The Role of Model .pdf;/home/justin/Zotero/storage/VUJVDCXC/MacCallum et al_2001_Sample Size in Factor Analysis.pdf;/home/justin/Zotero/storage/B6H4K6ZG/S15327906MBR3604_06.html}
}

@article{maccallum2003,
  title = {2001 {{Presidential Address}}: {{Working}} with Imperfect Models},
  shorttitle = {2001 Presidential Address},
  author = {MacCallum, Robert C.},
  year = {2003},
  journal = {Multivariate Behavioral Research},
  volume = {38},
  number = {1},
  pages = {113--139},
  publisher = {{Taylor \& Francis}},
  issn = {0027-3171},
  doi = {10/bg7vtz},
  abstract = {Since the early years of psychological research, investigators in psychology have made use of mathematical models of psychological phenomena. Models are now routinely used to represent and study cognitive processes, the structure of psychological measurements, the structure of correlational relationships among variables, the nature of change over time, and many other topics and phenomena of interest. All of these models, in their attempt to provide a parsimonious representation of psychological phenomena, are wrong to some degree and are thus implausible if taken literally. Such models simply cannot fully represent the complexities of the phenomena of interest and at best provide an approximation of the real world. This imperfection has implications for how we specify, estimate, and evaluate models, and how we interpret results of fitting models to data. Using factor analysis and structural equation models as a context, I examine some implications of model imperfection for our use of models, focusing on formal specification of models; the nature of parameters and parameter estimates; the relevance of discrepancy functions; the issue of sample size; the evaluation, development, and selection of models; and the conduct of simulation studies. The overall perspective is that our use and study of models should be guided by an understanding that our models are imperfect and cannot be made to be exactly correct.},
  date-added = {2019-12-12 14:59:47 -0600},
  date-modified = {2019-12-12 15:00:22 -0600},
  pmid = {26771126},
  annotation = {\_eprint: https://doi.org/10.1207/S15327906MBR3801\_5},
  file = {/home/justin/Zotero/storage/USED6J8J/MacCallum - 2003 - 2001 Presidential Address Working with Imperfect .pdf;/home/justin/Zotero/storage/GSZE2L6U/S15327906MBR3801_5.html}
}

@article{maccallum2010,
  title = {The Issue of Isopower in Power Analysis for Tests of Structural Equation Models},
  author = {MacCallum, Robert and Lee, Taehun and Browne, Michael W.},
  year = {2010},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {17},
  number = {1},
  pages = {23--41},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/b5szmw},
  abstract = {Two general frameworks have been proposed for evaluating statistical power of tests of model fit in structural equation modeling (SEM). Under the Satorra\textendash Saris (1985) approach, to evaluate the power of the test of fit of Model A, a Model B, within which A is nested, is specified as the alternative hypothesis and considered as the true model. We then determine the power of the test of fit of A when B is true. Under the MacCallum\textendash Browne\textendash Sugawara (1996) approach, power is evaluated with respect to the test of fit of Model A against an alternative hypothesis specifying a true degree of model misfit. We then determine the power of the test of fit of A when a specified degree of misfit is assumed to exist as the alternative hypothesis. In both approaches the phenomenon of isopower is present, which means that different alternative hypotheses (in the Satorra\textendash Saris approach) or combinations of alternative hypotheses and other factors (in the MacCallum\textendash Browne\textendash Sugawara approach) yield the same level of power. We show how these isopower alternatives can be defined and identified in both frameworks, and we discuss implications of isopower for understanding the results of power analysis in applications of SEM.},
  annotation = {\_eprint: https://doi.org/10.1080/10705510903438906},
  file = {/home/justin/Zotero/storage/H4WV2ZEX/MacCallum et al_2010_The Issue of Isopower in Power Analysis for Tests of Structural Equation Models.pdf;/home/justin/Zotero/storage/NGFLYN43/10705510903438906.html}
}

@article{maccallum2015,
  title = {Advances in Modeling Model Discrepancy: {{Comment}} on {{Wu}} and {{Browne}} (2015)},
  shorttitle = {Advances in Modeling Model Discrepancy},
  author = {MacCallum, Robert C. and O'Hagan, Anthony},
  year = {2015},
  month = sep,
  journal = {Psychometrika},
  volume = {80},
  number = {3},
  pages = {601--607},
  issn = {1860-0980},
  doi = {10/f7qpj3},
  abstract = {Wu and Browne (Psychometrika, 79, 2015) have proposed an innovative approach to modeling discrepancy between a covariance structure model and the population that the model is intended to represent. Their contribution is related to ongoing developments in the field of Uncertainty Quantification (UQ) on modeling and quantifying effects of model discrepancy. We provide an overview of basic principles of UQ and some relevant developments and we examine the Wu\textendash Browne work in that context. We view the Wu\textendash Browne contribution as a seminal development providing a foundation for further work on the critical problem of model discrepancy in statistical modeling in psychological research.},
  langid = {english},
  keywords = {model error},
  file = {/home/justin/Zotero/storage/66IDA9QY/MacCallum_O’Hagan_2015_Advances in modeling model discrepancy.pdf}
}

@book{magnus2019matrix,
  title = {Matrix Differential Calculus with Applications in Statistics and Econometrics},
  author = {Magnus, Jan R and Neudecker, Heinz},
  year = {2019},
  publisher = {{John Wiley \& Sons}},
  doi = {10.1002/9781119541219},
  file = {/home/justin/Zotero/storage/GRX5HG7W/Magnus_Neudecker_2019_Matrix differential calculus with applications in statistics and econometrics.pdf}
}

@inproceedings{malick2010,
  title = {Analysis and Geometry of Alternating Projections Algorithms},
  author = {Malick, Jerome},
  year = {2010},
  keywords = {⛔ No DOI found}
}

@inproceedings{marcinkowski2020,
  title = {Implications of {{AI}} (Un-)Fairness in Higher Education Admissions: The Effects of Perceived {{AI}} (Un-)Fairness on Exit, Voice and Organizational Reputation},
  shorttitle = {Implications of {{AI}} (Un-)Fairness in Higher Education Admissions},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Marcinkowski, Frank and Kieslich, Kimon and Starke, Christopher and L{\"u}nich, Marco},
  year = {2020},
  month = jan,
  series = {{{FAT}}* '20},
  pages = {122--130},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10/ghd5fx},
  abstract = {Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.},
  isbn = {978-1-4503-6936-7},
  keywords = {algorithmic decision making,artificial intelligence,distributive fairness,exit,higher education systems,procedural fairness,reputation,voice},
  file = {/home/justin/Zotero/storage/7BYZQLD8/Marcinkowski et al. - 2020 - Implications of AI (un-)fairness in higher educati.pdf}
}

@article{marcoulides2017a,
  title = {New {{Ways}} to {{Evaluate Goodness}} of {{Fit}}: {{A Note}} on {{Using Equivalence Testing}} to {{Assess Structural Equation Models}}},
  shorttitle = {New {{Ways}} to {{Evaluate Goodness}} of {{Fit}}},
  author = {Marcoulides, Katerina M. and Yuan, Ke-Hai},
  year = {2017},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {24},
  number = {1},
  pages = {148--153},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2016.1225260},
  abstract = {Structural equation models are typically evaluated on the basis of goodness-of-fit indexes. Despite their popularity, agreeing what value these indexes should attain to confidently decide between the acceptance and rejection of a model has been greatly debated. A recently proposed approach by means of equivalence testing has been recommended as a superior way to evaluate the goodness of fit of models. The approach has also been proposed as providing a necessary vehicle that can be used to advance the inferential nature of structural equation modeling as a confirmatory tool. The purpose of this article is to introduce readers to key ideas in equivalence testing and illustrate its use for conducting model\textendash data fit assessments. Two confirmatory factor analysis models in which a priori specified latent variable models with known structure and tested against data are used as examples. It is advocated that whenever the goodness of fit of a model is to be assessed researchers should always examine the resulting values obtained via the equivalence testing approach.},
  langid = {english},
  file = {/home/justin/Zotero/storage/U74CI4Q7/Marcoulides and Yuan - 2017 - New Ways to Evaluate Goodness of Fit A Note on Us.pdf}
}

@phdthesis{maree2012,
  type = {{{BSc Thesis}}},
  title = {Correcting {{Non Positive Definite Correlation Matrices}}},
  author = {Mar{\'e}e, Stef C. and Grzelak, L. and Oosterlee, C.W.},
  year = {2012},
  school = {Delft University of Technology}
}

@article{marsaglia1984,
  title = {Generating Correlation Matrices},
  author = {Marsaglia, George and Olkin, Ingram},
  year = {1984},
  month = jun,
  journal = {SIAM Journal on Scientific and Statistical Computing},
  volume = {5},
  number = {2},
  pages = {470--475},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0196-5204},
  doi = {10/bgv7hw},
  abstract = {This paper describes a variety of methods for generating random correlation matrices, with emphasis on choice of random variables and distributions so as to provide matrices with given structure, expected values or eigenvalues.},
  file = {/home/justin/Zotero/storage/YY6S8DZR/Marsaglia_Olkin_1984_Generating Correlation Matrices.pdf;/home/justin/Zotero/storage/QQJLELJN/0905034.html}
}

@article{marsh1998,
  title = {Is {{More Ever Too Much}}? {{The Number}} of {{Indicators}} per {{Factor}} in {{Confirmatory Factor Analysis}}},
  shorttitle = {Is {{More Ever Too Much}}?},
  author = {Marsh, Herbert W. and Hau, Kit-Tai and Balla, John R. and Grayson, David},
  year = {1998},
  month = apr,
  journal = {Multivariate Behavioral Research},
  volume = {33},
  number = {2},
  pages = {181--220},
  issn = {0027-3171, 1532-7906},
  doi = {10/b33fcr},
  langid = {english},
  file = {/home/justin/Zotero/storage/QMU37ZUF/Marsh et al. - 1998 - Is More Ever Too Much The Number of Indicators pe.pdf}
}

@article{marsh1998a,
  title = {Is More Ever Too Much? {{The}} Number of Indicators per Factor in Confirmatory Factor Analysis},
  shorttitle = {Is More Ever Too Much?},
  author = {Marsh, Herbert W. and Hau, Kit-Tai and Balla, John R. and Grayson, David},
  year = {1998},
  month = apr,
  journal = {Multivariate Behavioral Research},
  volume = {33},
  number = {2},
  pages = {181--220},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/b33fcr},
  abstract = {We evaluated whether "more is ever too much" for the number of indicators (p) per factor (p/f) in confirmatory factor analysis by varying sample size (N = 50-1000) and p/f (2-12 items per factor) in 35,000 Monte Carlo solutions. For all N's, solution behavior steadily improved (more proper solutions, more accurate parameter estimates, greater reliability) with increasing p/f. There was a compensatory relation between N and p/f: large p/f compensated for small N and large N compensated for small p/f, but large-N and large-p/f was best. A bias in the behavior of the {$\chi$}2 was also demonstrated where apparent goodness of fit declined with increasing p/f ratios even though approximating models were "true". Fit was similar for proper and improper solutions, as were parameter estimates form improper solutions not involving offending estimates. We also used the 12-p/f data to construct 2, 3, 4, or 6 parcels of items (e.g., two parcels of 6 items per factor, three parcels of 4 items per factor, etc.), but the 12-indicator (nonparceled) solutions were somewhat better behaved. At least for conditions in our simulation study, traditional "rules" implying fewer indicators should be used for smaller N may be inappropriate and researchers should consider using more indicators per factor that is evident in current practice.},
  pmid = {26771883},
  annotation = {\_eprint: https://doi.org/10.1207/s15327906mbr3302\_1},
  file = {/home/justin/Zotero/storage/EHXDXK7T/Marsh et al_1998_Is More Ever Too Much.pdf;/home/justin/Zotero/storage/VNHKN7ZM/s15327906mbr3302_1.html}
}

@article{marsh2004,
  title = {In Search of Golden Rules: {{Comment}} on Hypothesis-Testing Approaches to Setting Cutoff Values for Fit Indexes and Dangers in Overgeneralizing {{Hu}} and {{Bentler}}'s (1999) Findings},
  shorttitle = {In Search of Golden Rules},
  author = {Marsh, Herbert W. and Hau, Kit-Tai and Wen, Zhonglin},
  year = {2004},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {11},
  number = {3},
  pages = {320--341},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/dp39b9},
  abstract = {Goodness-of-fit (GOF) indexes provide "rules of thumb"\textemdash recommended cutoff values for assessing fit in structural equation modeling. Hu and Bentler (1999) proposed a more rigorous approach to evaluating decision rules based on GOF indexes and, on this basis, proposed new and more stringent cutoff values for many indexes. This article discusses potential problems underlying the hypothesis-testing rationale of their research, which is more appropriate to testing statistical significance than evaluating GOF. Many of their misspecified models resulted in a fit that should have been deemed acceptable according to even their new, more demanding criteria. Hence, rejection of these acceptable-misspecified models should have constituted a Type 1 error (incorrect rejection of an "acceptable" model), leading to the seemingly paradoxical results whereby the probability of correctly rejecting misspecified models decreased substantially with increasing N. In contrast to the application of cutoff values to evaluate each solution in isolation, all the GOF indexes were more effective at identifying differences in misspecification based on nested models. Whereas Hu and Bentler (1999) offered cautions about the use of GOF indexes, current practice seems to have incorporated their new guidelines without sufficient attention to the limitations noted by Hu and Bentler (1999).},
  annotation = {\_eprint: https://doi.org/10.1207/s15328007sem1103\_2},
  file = {/home/justin/Zotero/storage/8EU6Z4R2/Marsh et al. - 2004 - In Search of Golden Rules Comment on Hypothesis-T.pdf;/home/justin/Zotero/storage/5Z3L8CDC/s15328007sem1103_2.html}
}

@incollection{marsh2005,
  title = {Goodness of Fit in Structural Equation Models.},
  booktitle = {Contemporary Psychometrics: {{A}} Festschrift for {{Roderick P}}. {{McDonald}}.},
  author = {Marsh, Herbert W. and Hau, Kit-Tai and Grayson, David},
  year = {2005},
  series = {Multivariate Applications Book Series.},
  pages = {275--340},
  publisher = {{Lawrence Erlbaum Associates Publishers}},
  address = {{Mahwah,  NJ,  US}},
  abstract = {Following the introduction, we divide the discussion of goodness of fit (GOF) into three broad sections. In the first and the most substantial (in terms of length) section, we provide a technical summary of the GOF literature. In this section we take a reasonably uncritical perspective on the role of GOF testing, providing an almost encyclopedic summary of GOF indices and their behavior in relation to a variety of criteria. Then we introduce some complications related to GOF testing that have not been adequately resolved and may require further research. In the final section, we place the role of GOF within the broader context of model evaluation. Taking the role of devil's advocate, we challenge the appropriateness of current GOF practice, arguing that current practice is leading structural equation modeling (SEM) research into counterproductive directions that run the risk of undermining good science and marginalizing the usefulness of SEM as a research tool. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  isbn = {0-8058-4608-5 (Hardcover)},
  keywords = {*Goodness of Fit,Structural Equation Modeling},
  file = {/home/justin/Zotero/storage/PR4ZXUSW/Marsh et al_2005_Goodness of fit in structural equation models.pdf}
}

@book{mathoverflowDerivativeCholeskyFactor,
  title = {The Derivative of the Cholesky Factor},
  author = {{mathoverflow}},
  keywords = {matrix smooth}
}

@book{matlab,
  title = {{{MATLAB}} and Statistics Toolbox R2019a},
  author = {The Mathworks, Inc.},
  year = {2019},
  address = {{Natick, Massachusetts, United States}},
  date-added = {2019-04-11 09:24:34 -0500},
  date-modified = {2019-04-11 09:28:39 -0500}
}

@book{matrixpackageauthorsNearPDCorHelpPage2013,
  title = {{{NearPDCor R Help}} Page ({{Matrix Library}})},
  author = {{MatrixPackageAuthors}},
  year = {2013},
  keywords = {matrix smooth}
}

@article{maydeu-olivares2017,
  title = {Assessing the Size of Model Misfit in Structural Equation Models},
  author = {{Maydeu-Olivares}, Alberto},
  year = {2017},
  month = sep,
  journal = {Psychometrika},
  volume = {82},
  number = {3},
  pages = {533--558},
  issn = {0033-3123, 1860-0980},
  doi = {10/gbthc8},
  langid = {english},
  file = {/home/justin/Zotero/storage/KBB3PZAT/Maydeu-Olivares_2017_Assessing the Size of Model Misfit in Structural Equation Models.pdf}
}

@article{maydeu-olivares2017a,
  title = {Goodness of Fit in Item Factor Analysis: {{Effect}} of the Number of Response Alternatives},
  shorttitle = {Goodness of Fit in Item Factor Analysis},
  author = {{Maydeu-Olivares}, Alberto and Fairchild, Amanda J. and Hall, Alexander G.},
  year = {2017},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {24},
  number = {4},
  pages = {495--505},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/gcph66},
  abstract = {The power of the chi-square test statistic used in structural equation modeling decreases as the absolute value of excess kurtosis of the observed data increases. Excess kurtosis is more likely the smaller the number of item response categories. As a result, fit is likely to improve as the number of item response categories decreases, regardless of the true underlying factor structure or {$\chi$}2-based fit index used to examine model fit. Equivalently, given a target value of approximate fit (e.g., root mean square error of approximation {$\leq$} .05) a model with more factors is needed to reach it as the number of categories increases. This is true regardless of whether the data are treated as continuous (common factor analysis) or as discrete (ordinal factor analysis). We recommend using a large number of response alternatives ({$\geq$} 5) to increase the power to detect incorrect substantive models.},
  keywords = {difficulty factors,model fit},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2017.1289816},
  file = {/home/justin/Zotero/storage/YWLSE882/Maydeu-Olivares et al_2017_Goodness of Fit in Item Factor Analysis.pdf;/home/justin/Zotero/storage/KSIG6J4L/10705511.2017.html}
}

@article{mazza2011,
  title = {Package `{{KernSmoothIRT}}'},
  author = {Mazza, Angelo and Punzo, Antonio and McGuire, Brian and McGuire, Maintainer Brian and Rcpp, Depends and Rcpp, LinkingTo},
  year = {2011},
  keywords = {⛔ No DOI found,nonparametric IRT}
}

@article{mazza2012,
  title = {{{KernSmoothIRT}}: {{An R Package}} for {{Kernel Smoothing}} in {{Item Response Theory}}},
  author = {Mazza, Angelo and Punzo, Antonio and McGuire, Brian},
  year = {2012},
  journal = {arXiv preprint arXiv:1211.1183},
  eprint = {1211.1183},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,nonparametric IRT}
}

@article{mcardle1980,
  title = {Causal Modeling Applied to Psychonomic Systems Simulation},
  author = {McArdle, J. Jack},
  year = {1980},
  month = mar,
  journal = {Behavior Research Methods \& Instrumentation},
  volume = {12},
  number = {2},
  pages = {193--209},
  issn = {1554-3528},
  doi = {10/dv36qw},
  abstract = {Psychometric principles in structural equation model building (e.g., LISREL) are applied to the psychonomic problems in simulating causal systems. A new approach is described (i.e., RAM) that permits the complete and concise description of causal logic by nomographic-diagrammatic representation. Several simple examples based on traditional data-analytic models are causally explicated. Relevant numerical algorithms for multivariable simulation in an on-line environment are described. As an illustration of complete structure-process modeling, a substantive example of ``human ability systems'' is presented. Finally, a broader view of both practical and theoretical applications is offered.},
  langid = {english},
  file = {/home/justin/Zotero/storage/EHURFGCF/McArdle_1980_Causal modeling applied to psychonomic systems simulation.pdf}
}

@article{mccrae1992,
  title = {Discriminant {{Validity}} of {{NEO-PIR Facet Scales}}},
  author = {McCrae, Robert R. and Costa, Paul T.},
  year = {1992},
  month = mar,
  journal = {Educational and Psychological Measurement},
  volume = {52},
  number = {1},
  pages = {229--237},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10/bvvc7z},
  abstract = {Two analyses were conducted to examine the discriminant validity of 30 facet scales from the Revised NEO Personality Inventory (NEO-PIR). To examine cross-observer validity of specific variance in the facet scales, partial correlations between self-reports and peer (N = 250) and spouse (N = 68) ratings on the facets were calculated, controlling for the five common factors. All 60 convergent partial correlations were positive, and 48 (80\%) were significant. In the second analysis, Adjective Check List (Gough and Heilbrun, 1983) correlates of the facet scales were identified in a sample of 305 adults. Judges correctly matched scales to correlates in most cases, providing additional evidence for discriminant validity.},
  langid = {english},
  file = {/home/justin/Zotero/storage/GZ7ZEE4L/McCrae and Costa - 1992 - Discriminant Validity of NEO-PIR Facet Scales.pdf}
}

@article{mcdonald1969,
  title = {The Common Factor Analysis of Multicategory Data},
  author = {McDonald, Roderick P.},
  year = {1969},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {22},
  number = {2},
  pages = {165--175},
  issn = {2044-8317},
  doi = {10/fkvgcc},
  abstract = {An algorithm is described that serves to fit a linear latent structure model to multicategory data. The model has the formal properties of a (generalized) common factor model, in contrast to principal component analysis. An empirical example is given.},
  copyright = {1969 The British Psychological Society},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1969.tb00428.x},
  file = {/home/justin/Zotero/storage/HU9ID2AT/McDonald_1969_The Common Factor Analysis of Multicategory Data.pdf}
}

@article{mcdonald1974,
  title = {Difficulty Factors in Binary Data},
  author = {McDonald, Roderick P. and Ahlawat, Kapur S.},
  year = {1974},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {27},
  number = {1},
  pages = {82--99},
  issn = {2044-8317},
  doi = {10/fpm6jf},
  abstract = {A number of writers have regarded difficulty factors as arising from the misbehaviour, in some sense, of correlation coefficients for binary data when the binary variables have varying difficulty levels. McDonald (1965) argued that difficulty factors are due to non-linear item characteristic curves rather than difficulty per se, but did not give an example using binary data. An artificial experiment is described in which non-linear item characteristic curves yield a difficulty factor, whereas linear item characteristic curves do not, even when in the latter case the items differ markedly in difficulty and phi coefficients are employed.},
  copyright = {1974 The British Psychological Society},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1974.tb00530.x},
  file = {/home/justin/Zotero/storage/YHLBS3QV/McDonald_Ahlawat_1974_Difficulty Factors in Binary Data.pdf}
}

@book{mcdonald2014,
  title = {Factor {{Analysis}} and {{Related Methods}}},
  author = {McDonald, Roderick P.},
  year = {2014},
  month = jan,
  publisher = {{Psychology Press}},
  doi = {10.4324/9781315802510},
  abstract = {Factor Analysis is a genetic term for a somewhat vaguely delimited set of techniques for data processing, mainly applicable to the social and biological},
  isbn = {978-1-315-80251-0},
  langid = {english},
  file = {/home/justin/Zotero/storage/M25US5B2/McDonald - 2014 - Factor Analysis and Related Methods.pdf;/home/justin/Zotero/storage/2TAS7WGZ/9781315802510.html}
}

@phdthesis{mcguireKernSmoothIRTPackageAllowing2012,
  title = {{{KernSmoothIRT}}: {{An R Package}} Allowing for {{Kernel Smoothing}} in {{Item Response Theory}}.},
  author = {McGuire, Brian},
  year = {2012},
  school = {Montana State University},
  keywords = {nonparametric IRT}
}

@article{mckinley1983,
  title = {{{MAXLOG}}: {{A}} Computer Program for the Estimation of the Parameters of a Multidimensional Logistic Model},
  shorttitle = {{{MAXLOG}}},
  author = {McKinley, Robert L. and Reckase, Mark D.},
  year = {1983},
  journal = {Behavior Research Methods \& Instrumentation},
  volume = {15},
  number = {3},
  pages = {389--390},
  issn = {1554-351X, 1554-3528},
  doi = {10/c72mc3},
  langid = {english},
  file = {/home/justin/Zotero/storage/ZXMMQ5PN/McKinley and Reckase - 1983 - MAXLOG A computer program for the estimation of t.pdf}
}

@article{mcneish2017,
  title = {Clustered Data with Small Sample Sizes: {{Comparing}} the Performance of Model-Based and Design-Based Approaches},
  shorttitle = {Clustered Data with Small Sample Sizes},
  author = {McNeish, Daniel M. and Harring, Jeffery R.},
  year = {2017},
  month = feb,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {46},
  number = {2},
  pages = {855--869},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10/ghmbms},
  abstract = {Two classes of methods properly account for clustering of data: design-based methods and model-based methods. Estimates from both methods have been shown to be approximately equal with large samples. However, both classes are known to produce biased standard error estimates with small samples. This paper compares the bias of standard errors and statistical power of marginal effects for generalized estimating equations (a design-based method) and generalized/linear mixed effects models (model-based methods) with small sample sizes via a simulation study. Provided that the distributional assumptions are met, model-based methods produced the least-biased standard error estimates and greater relative statistical power.},
  keywords = {62J05,62J12,GEE,Kenward-Roger,Mixed model,Multilevel model,Small sample size},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2014.983648},
  file = {/home/justin/Zotero/storage/J8R92PMW/McNeish_Harring_2017_Clustered data with small sample sizes.pdf;/home/justin/Zotero/storage/D5QN4NF8/03610918.2014.html}
}

@article{mcneish2017a,
  title = {Clustered Data with Small Sample Sizes: {{Comparing}} the Performance of Model-Based and Design-Based Approaches},
  shorttitle = {Clustered Data with Small Sample Sizes},
  author = {McNeish, Daniel M. and Harring, Jeffery R.},
  year = {2017},
  month = feb,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {46},
  number = {2},
  pages = {855--869},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10/ghmbms},
  abstract = {Two classes of methods properly account for clustering of data: design-based methods and model-based methods. Estimates from both methods have been shown to be approximately equal with large samples. However, both classes are known to produce biased standard error estimates with small samples. This paper compares the bias of standard errors and statistical power of marginal effects for generalized estimating equations (a design-based method) and generalized/linear mixed effects models (model-based methods) with small sample sizes via a simulation study. Provided that the distributional assumptions are met, model-based methods produced the least-biased standard error estimates and greater relative statistical power.},
  keywords = {62J05,62J12,GEE,Kenward-Roger,Mixed model,Multilevel model,Small sample size},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2014.983648},
  file = {/home/justin/Zotero/storage/L33AHPBC/McNeish_Harring_2017_Clustered data with small sample sizes.pdf;/home/justin/Zotero/storage/PDWZALEZ/03610918.2014.html}
}

@article{mcneish2017exploratory,
  title = {Exploratory Factor Analysis with Small Samples and Missing Data},
  author = {McNeish, Daniel},
  year = {2017},
  journal = {Journal of Personality Assessment},
  volume = {99},
  number = {6},
  pages = {637--652},
  publisher = {{Taylor \& Francis}},
  doi = {10/gfvqgr},
  date-added = {2020-02-07 11:02:27 -0600},
  date-modified = {2020-02-07 11:02:27 -0600}
}

@article{meade2004,
  title = {A {{Monte-Carlo}} Study of Confirmatory Factor Analytic Tests of Measurement Equivalence/Invariance},
  author = {Meade, Adam W. and Lautenschlager, Gary J.},
  year = {2004},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {11},
  number = {1},
  pages = {60--72},
  issn = {1070-5511, 1532-8007},
  doi = {10/cqtgqt},
  langid = {english},
  file = {/home/justin/Zotero/storage/B634D2XE/Meade and Lautenschlager - 2004 - A Monte-Carlo Study of Confirmatory Factor Analyti.pdf}
}

@article{meade2004a,
  title = {A Comparison of {{Item Response Theory}} and Confirmatory Factor Analytic Methodologies for Establishing Measurement Equivalence/Invariance},
  author = {Meade, Adam W. and Lautenschlager, Gary J.},
  year = {2004},
  month = oct,
  journal = {Organizational Research Methods},
  volume = {7},
  number = {4},
  pages = {361--388},
  publisher = {{SAGE Publications Inc}},
  issn = {1094-4281},
  doi = {10/dxv9mr},
  abstract = {Recently, there has been increased interest in tests of measurement equivalence/ invariance (ME/I). This study uses simulated data with known properties to assess the appropriateness, similarities, and differences between confirmatory factor analysis and item response theory methods of assessing ME/I. Results indicate that although neither approach is without flaw, the item response theory\textendash based approach seems to be better suited for some types of ME/I analyses.},
  langid = {english},
  file = {/home/justin/Zotero/storage/IQMPWLFJ/Meade and Lautenschlager - 2004 - A Comparison of Item Response Theory and Confirmat.pdf}
}

@article{meade2008,
  title = {Power and Sensitivity of Alternative Fit Indices in Tests of Measurement Invariance},
  author = {Meade, Adam W. and Johnson, Emily C. and Braddy, Phillip W.},
  year = {2008},
  journal = {Journal of Applied Psychology},
  volume = {93},
  number = {3},
  pages = {568--592},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1854(Electronic),0021-9010(Print)},
  doi = {10/b3t945},
  abstract = {Confirmatory factor analytic tests of measurement invariance (MI) based on the chi-square statistic are known to be highly sensitive to sample size. For this reason, G. W. Cheung and R. B. Rensvold (2002) recommended using alternative fit indices (AFIs) in MI investigations. In this article, the authors investigated the performance of AFIs with simulated data known to not be invariant. The results indicate that AFIs are much less sensitive to sample size and are more sensitive to a lack of invariance than chi-square-based tests of MI. The authors suggest reporting differences in comparative fit index (CFI) and R. P. McDonald's (1989) noncentrality index (NCI) to evaluate whether MI exists. Although a general value of change in CFI (.002) seemed to perform well in the analyses, condition specific change in McDonald's NCI values exhibited better performance than a single change in McDonald's NCI value. Tables of these values are provided as are recommendations for best practices in MI testing. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Chi Square Test,Measurement,Measurement Invariance,Statistics,Testing},
  file = {/home/justin/Zotero/storage/Q2DPVXBS/Meade et al. - 2008 - Power and sensitivity of alternative fit indices i.pdf;/home/justin/Zotero/storage/SIX7PKII/2008-05281-006.html}
}

@article{meade2009,
  title = {Test Bias, Differential Prediction, and a Revised Approach for Determining the Suitability of a Predictor in a Selection Context},
  shorttitle = {Test Bias, Differential Prediction, and a Revised Approach for Determining the Suitability of a Predictor in a Selection Context},
  author = {Meade, Adam W. and Fetzer, Michael},
  year = {2009},
  month = feb,
  journal = {Organizational Research Methods},
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  doi = {10/bfg83f},
  abstract = {The most commonly used and accepted model of assessing bias in a selection context is that proposed by Cleary in which predictor-criterion regression lines are ...},
  copyright = {\textcopyright{} 2009 SAGE Publications},
  langid = {english},
  file = {/home/justin/Zotero/storage/PSNSSGAH/1094428109331487.html}
}

@article{meehl1955,
  title = {Antecedent Probability and the Efficiency of Psychometric Signs, Patterns, or Cutting Scores.},
  author = {Meehl, Paul E. and Rosen, Albert},
  year = {1955},
  journal = {Psychological Bulletin},
  volume = {52},
  number = {3},
  pages = {194--216},
  issn = {1939-1455, 0033-2909},
  doi = {10/fbd6c6},
  langid = {english},
  file = {/home/justin/Zotero/storage/HEY52CLI/Meehl and Rosen - 1955 - Antecedent probability and the efficiency of psych.pdf}
}

@article{meehl2002,
  title = {The Path Analysis Controversy: {{A}} New Statistical Approach to Strong Appraisal of Verisimilitude},
  shorttitle = {The Path Analysis Controversy},
  author = {Meehl, Paul E. and Waller, Niels G.},
  year = {2002},
  journal = {Psychological Methods},
  volume = {7},
  number = {3},
  pages = {283--300},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10/dbzsmd},
  abstract = {A new approach for using path analysis to appraise the verisimilitude of theories is described. Rather than trying to test a model's truth (correctness), this method corroborates a class of path diagrams by determining how well they predict intradata relations in comparison with other diagrams. The observed correlation matrix is partitioned into disjoint sets. One set is used to estimate the model parameters, and a nonoverlapping set is used to assess the model's verisimilitude. Computer code was written to generate competing models and to test the conjectured model's superiority (relative to the generated set) using diagram combinatorics and is available on the Web. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Models,Path Analysis,Statistical Analysis},
  file = {/home/justin/Zotero/storage/TNKSUV5X/2002-18342-001.html}
}

@article{meehl2006,
  title = {Antecedent {{Probability}} and the {{Efficiency}} of {{Psychometric Signs}}, {{Patterns}}, or {{Cutting Scores}}},
  author = {Meehl, Paul E. and Rosen, Albert},
  year = {2006},
  pages = {227--250},
  publisher = {{Routledge}},
  doi = {10.1037/h0048070},
  abstract = {In clinical practice, psychologists frequently participate in the making of vital decisions concerning the classification, treatment, prognosis, and disposition of individuals. In their attempts to increase the number of correct classifications and predictions, psychologists have developed and applied many psychometric devices, such as patterns of test responses as well as cutting scores for scales, indices, and sign lists. Since diagnostic and prognostic statements can often be made with a high degree of accuracy purely on the basis of actuarial or experience tables (referred to hereinafter as base rates), a psychometric device, to be efficient, must make possible a greater number of correct decisions than could be made in terms of the base rates alone. The efficiency of the great majority of psychometric devices reported in the clinical psychology literature is difficult or impossible to evaluate for the following reasons: 1. Base rates are virtually never reported. It is, therefore, difficult to determine whether or not a given device results in a greater number of correct decisions than would be possible solely on the basis of the rates from previous experience. When, however, the base rates can be estimated, the reported claims of efficiency of psychometric instruments are often seen to be without foundation.}
}

@article{mehrabi2019,
  title = {A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  year = {2019},
  month = sep,
  journal = {arXiv:1908.09635 [cs]},
  eprint = {1908.09635},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning},
  file = {/home/justin/Zotero/storage/G25XW289/Mehrabi et al. - 2019 - A Survey on Bias and Fairness in Machine Learning.pdf;/home/justin/Zotero/storage/S9I5NLN4/1908.html}
}

@article{merolla2012,
  title = {Connecting Here and There: {{A}} Model of Long-Distance Relationship Maintenance},
  shorttitle = {Connecting Here and There},
  author = {Merolla, Andy J.},
  year = {2012},
  journal = {Personal Relationships},
  volume = {19},
  number = {4},
  pages = {775--795},
  issn = {1475-6811},
  doi = {10/fxrp57},
  abstract = {This study examines a model of long-distance relationship maintenance. The model captures the relational cognition and communication partners enact before, during, and after periods of separation to sustain relational quality. Phase 1 of this study, through inductive analysis, identified 178 forms of maintenance. Phases 2 and 3 employed factor analysis and produced support for a 10-factor model. Subsequent tests assessed how these factors predicted intimacy, satisfaction, and stress for long-distance dating partners. Although several of the factors were significant predictors, the effects were not uniform. The strength and directionality of effects depended on the interactional style of partners' maintenance (i.e., intrapersonal, dyadic, or network) and the time of its enactment (i.e., before, during, or after periods of separation).},
  copyright = {Copyright \textcopyright{} 2012 IARR},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-6811.2011.01392.x},
  file = {/home/justin/Zotero/storage/N3TDJ9LZ/Merolla - 2012 - Connecting here and there A model of long-distanc.pdf;/home/justin/Zotero/storage/XRT3GSTK/j.1475-6811.2011.01392.html}
}

@article{meyer2013penalized,
  title = {A Penalized Likelihood Approach to Pooling Estimates of Covariance Components from Analyses by Parts},
  author = {Meyer, Karin},
  year = {2013},
  journal = {Journal of Animal Breeding and Genetics},
  volume = {130},
  number = {4},
  pages = {270--285},
  publisher = {{Wiley Online Library}},
  doi = {10/f45cg9},
  date-added = {2020-02-07 11:17:51 -0600},
  date-modified = {2020-02-07 11:17:51 -0600}
}

@article{miles2007,
  title = {A Time and a Place for Incremental Fit Indices},
  author = {Miles, Jeremy and Shevlin, Mark},
  year = {2007},
  month = may,
  journal = {Personality and Individual Differences},
  series = {Special Issue on {{Structural Equation Modeling}}},
  volume = {42},
  number = {5},
  pages = {869--874},
  issn = {0191-8869},
  doi = {10/fhdt3m},
  abstract = {It is well established that the {$\chi$}2 test is influenced by sample size and will lead to over rejection of models tested using large sample sizes. In this paper it is shown that the population parameter values of a model can also influence the {$\chi$}2 and lead to erroneous decisions about model acceptance/rejection. It is concluded, based on the examination of hypothetical population factor analytic models, that incremental fit indices offer a useful source of information for the analyst to assist in the interpretation of the {$\chi$}2 test.},
  langid = {english},
  keywords = {Confirmatory factor analysis model fit,Incremental fit indices,Power},
  file = {/home/justin/Zotero/storage/NT492677/Miles_Shevlin_2007_A time and a place for incremental fit indices.pdf;/home/justin/Zotero/storage/TF46A62F/S0191886906003874.html}
}

@article{miles2007a,
  title = {A Time and a Place for Incremental Fit Indices},
  author = {Miles, Jeremy and Shevlin, Mark},
  year = {2007},
  month = may,
  journal = {Personality and Individual Differences},
  series = {Special Issue on {{Structural Equation Modeling}}},
  volume = {42},
  number = {5},
  pages = {869--874},
  issn = {0191-8869},
  doi = {10/fhdt3m},
  abstract = {It is well established that the {$\chi$}2 test is influenced by sample size and will lead to over rejection of models tested using large sample sizes. In this paper it is shown that the population parameter values of a model can also influence the {$\chi$}2 and lead to erroneous decisions about model acceptance/rejection. It is concluded, based on the examination of hypothetical population factor analytic models, that incremental fit indices offer a useful source of information for the analyst to assist in the interpretation of the {$\chi$}2 test.},
  langid = {english},
  keywords = {Confirmatory factor analysis model fit,Incremental fit indices,Power},
  file = {/home/justin/Zotero/storage/FACLPSP5/Miles_Shevlin_2007_A time and a place for incremental fit indices.pdf;/home/justin/Zotero/storage/ENT9WTMB/S0191886906003874.html}
}

@article{millsap1995,
  title = {Measurement {{Invariance}}, {{Predictive Invariance}}, and the {{Duality Paradox}}},
  author = {Millsap, Roger E.},
  year = {1995},
  month = oct,
  journal = {Multivariate Behavioral Research},
  volume = {30},
  number = {4},
  pages = {577--605},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/ccstvg},
  abstract = {The statistical literature on bias in psychological testing distinguishes at least two forms of bias: measurement bias and predictive bias. Measurement bias concerns group differences in the relationship between a test and the latent variable to be measured. Predictive bias concerns group differences in the relationship between a test and an external criterion. How are these two forms of bias related? For example. if a test is unbiased in the predictive sense, does this fact support the hypothesis that the test is unbiased in the measurement sense? A theorem is given that describes the conditions under which measurement invariance (lack of bias) is consistent with predictive invariance for the linear case. Paradoxically, these two forms of invariance are shown to be inconsistent under realistic conditions. This duality or inconsistency is illustrated in simulated data. The implications of the duality for group differences research are illustrated in real data involving gender and ethnic differences on the SAT. The phenomenon of duality may force a reinterpretation of common empirical findings of test criterion regression slope invariance. and of invariance in test validities. Other implications are discussed.},
  pmid = {26790049},
  annotation = {\_eprint: https://doi.org/10.1207/s15327906mbr3004\_6},
  file = {/home/justin/Zotero/storage/PF5KHN2V/Millsap - 1995 - Measurement Invariance, Predictive Invariance, and.pdf;/home/justin/Zotero/storage/YDA6BJ5C/s15327906mbr3004_6.html}
}

@article{millsap1997,
  title = {Invariance in Measurement and Prediction: {{Their}} Relationship in the Single-Factor Case.},
  shorttitle = {Invariance in Measurement and Prediction},
  author = {Millsap, Roger E.},
  year = {1997},
  journal = {Psychological Methods},
  volume = {2},
  number = {3},
  pages = {248--260},
  issn = {1939-1463, 1082-989X},
  doi = {10/dxhj24},
  langid = {english},
  file = {/home/justin/Zotero/storage/2ZH9LZVI/Millsap - 1997 - Invariance in measurement and prediction Their re.pdf}
}

@article{millsap1998,
  title = {Group Differences in Regression Intercepts: {{Implications}} for Factorial Invariance},
  shorttitle = {Group Differences in Regression Intercepts},
  author = {Millsap, Roger E.},
  year = {1998},
  month = jul,
  journal = {Multivariate Behavioral Research},
  volume = {33},
  number = {3},
  pages = {403--424},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/b7bwnb},
  abstract = {Studies of differential prediction typically examine group differences in linear regression slopes or intercepts for predicting criterion scores from one or more test scores. When there are no group differences in slopes, what are the implications of differences in regression intercepts for the measurement equivalence of the tests or criterion across groups? Measurement equivalence is here defined as factorial invariance under a single-factor model for the tests and criterion. Two theorems are given that describe conditions under which intercept differences can exist under factorial invariance. In such cases, intercept differences do not result from measurement bias in either the tests or criterion. The conditions of the theorems are testable using multiple-group confirmatory factor analysis. These test procedures are illustrated in real data. The implications of the theorems and the test procedures for studies of differential prediction are discussed.},
  pmid = {26782721},
  annotation = {\_eprint: https://doi.org/10.1207/s15327906mbr3303\_5},
  file = {/home/justin/Zotero/storage/I2SEJGY5/Millsap - 1998 - Group Differences in Regression Intercepts Implic.pdf;/home/justin/Zotero/storage/CU7JTN6M/s15327906mbr3303_5.html}
}

@article{millsap2004,
  title = {Evaluating the Impact of Partial Factorial Invariance on Selection in Two Populations.},
  author = {Millsap, Roger E. and Kwok, Oi-Man},
  year = {2004},
  journal = {Psychological Methods},
  volume = {9},
  number = {1},
  pages = {93--115},
  issn = {1939-1463, 1082-989X},
  doi = {10/fv9ctv},
  langid = {english}
}

@article{millsap2007,
  title = {Invariance in Measurement and Prediction Revisited},
  author = {Millsap, Roger E.},
  year = {2007},
  month = dec,
  journal = {Psychometrika},
  volume = {72},
  number = {4},
  pages = {461--473},
  issn = {0033-3123, 1860-0980},
  doi = {10/bcgn42},
  langid = {english},
  file = {/home/justin/Zotero/storage/L9EXMPF5/Millsap - 2007 - Invariance in Measurement and Prediction Revisited.pdf}
}

@book{millsap2011,
  title = {Statistical Approaches to Measurement Invariance},
  author = {Millsap, Roger Ellis},
  year = {2011},
  publisher = {{Routledge}},
  address = {{New York London}},
  isbn = {978-1-84872-819-6 978-1-84872-818-9},
  langid = {english},
  annotation = {OCLC: 846122484},
  file = {/home/justin/Zotero/storage/M2BS34MS/Millsap - 2011 - Statistical approaches to measurement invariance.pdf}
}

@article{mislevy1986,
  title = {Recent Developments in the Factor Analysis of Categorical Variables},
  author = {Mislevy, Robert J.},
  year = {1986},
  journal = {Journal of Educational Statistics},
  volume = {11},
  number = {1},
  pages = {3--31},
  publisher = {{[Sage Publications, Inc., American Educational Research Association, American Statistical Association]}},
  issn = {0362-9791},
  doi = {10/b7bpxt},
  abstract = {Despite known shortcomings of the procedure, exploratory factor analysis of dichotomous test items has been limited, until recently, to unweighted analyses of matrices of tetrachoric correlations. Superior methods have begun to appear in the literature, in professional symposia, and in computer programs. This paper places these developments in a unified framework, from a review of the classical common factor model for measured variables through generalized least squares and marginal maximum likelihood solutions for dichotomous data. Further extensions of the model are also reported as work in progress.}
}

@book{mitchell1996,
  title = {An Introduction to Genetic Algorithms},
  author = {Mitchell, Melanie},
  editor = {NetLibrary, Inc},
  year = {1996},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass.}},
  abstract = {Genetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural evolutionary systems. This brief, accessible introduction describes some of the most interesting research in the field and also enables readers to implement and experiment with genetic algorithms on their own. It focuses in depth on a small set of important and interesting topics--particularly in machine learning, scientific modeling, and artificial life--and reviews a broad span of research, including the work of Mitchell and her colleagues.The descriptions of applications and modeling projects stretch beyond the strict boundaries of computer science to include dynamical systems theory, game theory, molecular biology, ecology, evolutionary biology, and population genetics, underscoring the exciting "general purpose" nature of genetic algorithms as search methods that can be employed across disciplines.An Introduction to Genetic Algorithms is accessible to students and researchers in any scientific discipline. It includes many thought and computer exercises that build on and reinforce the reader's understanding of the text. The first chapter introduces genetic algorithms and their terminology and describes two provocative applications in detail. The second and third chapters look at the use of genetic algorithms in machine learning (computer programs, data analysis and prediction, neural networks) and in scientific models (interactions among learning, evolution, and culture},
  isbn = {0-262-13316-4},
  keywords = {Algorithms,Algoritmen,Computer simulation,Computer Simulation,Electronic books,Evolutie,Genetic algorithms,Genetica,Genetics -- Computer simulation,Genetics -- Mathematical models,Genetique -- Modeles mathematiques,Génétique -- Modèles mathématiques,Génétique -- Simulation par ordinateur,Models; Genetic,SCIENCE -- Life Sciences -- Genetics \& Genomics},
  file = {/home/justin/Zotero/storage/8SE23KJT/Mitchell_1996_An introduction to genetic algorithms.pdf}
}

@article{mitchell2020,
  title = {Prediction-{{Based Decisions}} and {{Fairness}}: {{A Catalogue}} of {{Choices}}, {{Assumptions}}, and {{Definitions}}},
  shorttitle = {Prediction-{{Based Decisions}} and {{Fairness}}},
  author = {Mitchell, Shira and Potash, Eric and Barocas, Solon and D'Amour, Alexander and Lum, Kristian},
  year = {2020},
  month = apr,
  journal = {arXiv:1811.07867 [stat]},
  eprint = {1811.07867},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {A recent flurry of research activity has attempted to quantitatively define "fairness" for decisions based on statistical and machine learning (ML) predictions. The rapid growth of this new field has led to wildly inconsistent terminology and notation, presenting a serious challenge for cataloguing and comparing definitions. This paper attempts to bring much-needed order. First, we explicate the various choices and assumptions made---often implicitly---to justify the use of prediction-based decisions. Next, we show how such choices and assumptions can raise concerns about fairness and we present a notationally consistent catalogue of fairness definitions from the ML literature. In doing so, we offer a concise reference for thinking through the choices, assumptions, and fairness considerations of prediction-based decision systems.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Statistics - Applications},
  file = {/home/justin/Zotero/storage/2J3WAYMW/Mitchell et al. - 2020 - Prediction-Based Decisions and Fairness A Catalog.pdf;/home/justin/Zotero/storage/94PLLCN7/Mitchell et al. - 2020 - Prediction-Based Decisions and Fairness A Catalog.pdf;/home/justin/Zotero/storage/GASSB92L/1811.html;/home/justin/Zotero/storage/KTDFUZL4/1811.html}
}

@article{molenaar2013,
  title = {The Formalization of Fairness: Issues in Testing for Measurement Invariance Using Subtest Scores},
  shorttitle = {The Formalization of Fairness},
  author = {Molenaar, Dylan and Borsboom, Denny},
  year = {2013},
  month = apr,
  journal = {Educational Research and Evaluation},
  volume = {19},
  number = {2-3},
  pages = {223--244},
  publisher = {{Routledge}},
  issn = {1380-3611},
  doi = {10/gjrkcv},
  abstract = {Measurement invariance is an important prerequisite for the adequate comparison of group differences in test scores. In psychology, measurement invariance is typically investigated by means of linear factor analyses of subtest scores. These subtest scores typically result from summing the item scores. In this paper, we discuss 4 possible problems related to this common practice. Specifically, we discuss (a) nonlinearity of the latent variable to subtest relation; (b) suboptimality of the total score as a proxy for the latent variable measured through the item scores; (c) non-normality of the subtest score; and (d) differences in the nature of the latent variable at the item level as compared to the latent variable at the subtest level. Additionally, we give guidelines to overcome these problems and illustrate the issues by analysing data that pertain to a performal IQ data set.},
  keywords = {differential item functioning,factor analysis,item response theory,measurement invariance,non-normality,sum scores},
  annotation = {\_eprint: https://doi.org/10.1080/13803611.2013.767628},
  file = {/home/justin/Zotero/storage/FW8QY9FN/13803611.2013.html}
}

@article{monroe2015,
  title = {Evaluating Structural Equation Models for Categorical Outcomes: {{A}} New Test Statistic and a Practical Challenge of Interpretation},
  shorttitle = {Evaluating Structural Equation Models for Categorical Outcomes},
  author = {Monroe, Scott and Cai, Li},
  year = {2015},
  month = nov,
  journal = {Multivariate Behavioral Research},
  volume = {50},
  number = {6},
  pages = {569--583},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/ggf4r9},
  abstract = {This research is concerned with two topics in assessing model fit for categorical data analysis. The first topic involves the application of a limited-information overall test, introduced in the item response theory literature, to structural equation modeling (SEM) of categorical outcome variables. Most popular SEM test statistics assess how well the model reproduces estimated polychoric correlations. In contrast, limited-information test statistics assess how well the underlying categorical data are reproduced. Here, the recently introduced C2 statistic of Cai and Monroe (2014) is applied. The second topic concerns how the root mean square error of approximation (RMSEA) fit index can be affected by the number of categories in the outcome variable. This relationship creates challenges for interpreting RMSEA. While the two topics initially appear unrelated, they may conveniently be studied in tandem since RMSEA is based on an overall test statistic, such as C2. The results are illustrated with an empirical application to data from a large-scale educational survey.},
  pmid = {26717119},
  keywords = {limited-information testing,ordinal data,RMSEA,structural equation modeling},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2015.1032398},
  file = {/home/justin/Zotero/storage/LNMITNX2/Monroe_Cai_2015_Evaluating Structural Equation Models for Categorical Outcomes.pdf}
}

@article{monroe2018,
  title = {Contributions to {{Estimation}} of {{Polychoric Correlations}}},
  author = {Monroe, Scott},
  year = {2018},
  month = mar,
  journal = {Multivariate Behavioral Research},
  volume = {53},
  number = {2},
  pages = {247--266},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/ggf4r5},
  abstract = {This research concerns the estimation of polychoric correlations in the context of fitting structural equation models to observed ordinal variables by multistage estimation. The first main contribution of this research is to propose and evaluate a Monte Carlo estimator for the asymptotic covariance matrix (ACM) of the polychoric correlation estimates. In multistage estimation, the ACM plays a prominent role, as overall test statistics, derived fit indices, and parameter standard errors all depend on this quantity. The ACM, however, must itself be estimated. Established approaches to estimating the ACM use a sample-based version, which can yield poor estimates with small samples. A simulation study demonstrates that the proposed Monte Carlo estimator can be more efficient than its sample-based counterpart. This leads to better calibration for established test statistics, in particular with small samples. The second main contribution of this research is a further exploration of the consequences of violating the normality assumption for the underlying response variables. We show the consequences depend on the type of nonnormality, and the number and location of thresholds. The simulation study also demonstrates that overall test statistics have little power to detect the studied forms of nonnormality, regardless of the ACM estimator.},
  pmid = {29377713},
  keywords = {Categorical data,Monte Carlo methods,polychoric correlation,structural equation modeling},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2017.1419851},
  file = {/home/justin/Zotero/storage/MWJ47X6I/Monroe - 2018 - Contributions to Estimation of Polychoric Correlat.pdf;/home/justin/Zotero/storage/RF3FGEPE/00273171.2017.html}
}

@article{montanelli1974,
  title = {The Goodness of Fit of the Maximum-Likelihood Estimation Procedure in Factor Analysis},
  author = {Montanelli, Richard G.},
  year = {1974},
  month = oct,
  journal = {Educational and Psychological Measurement},
  volume = {34},
  number = {3},
  pages = {547--562},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10/fg5hkr},
  abstract = {Several goodness of fit measures for factor analysis were applied to the results of maximum-likelihood factor analyses of data with known factors. Results showed that p was a fairly good measure of the degree to which the factor analytic model held in the population, although some values were large for smaller sample sizes. Two measures, c and r, were used to compare the obtained sample factors with the population factors. These measures could be used in confirmatory studies, when an hypothesized factor matrix could be written down. They showed that fairly good agreement with an hypothesis can be obtained, even when the factor analytic model does not exactly hold in the population. This would require that the investigator have a large sample size, variables with high communalities, and a low ratio of number of factors to number of variables.},
  langid = {english},
  file = {/home/justin/Zotero/storage/3W7JBD7E/Montanelli_1974_The Goodness of Fit of the Maximum-Likelihood Estimation Procedure in Factor.pdf}
}

@article{montero2018,
  title = {Housing Price Prediction: Parametric versus Semi-Parametric Spatial Hedonic Models},
  shorttitle = {Housing Price Prediction},
  author = {Montero, Jos{\'e}-Mar{\'i}a and M{\'i}nguez, Rom{\'a}n and {Fern{\'a}ndez-Avil{\'e}s}, Gema},
  year = {2018},
  month = jan,
  journal = {Journal of Geographical Systems},
  volume = {20},
  number = {1},
  pages = {27--55},
  issn = {1435-5949},
  doi = {10/gcvdbs},
  abstract = {House price prediction is a hot topic in the economic literature. House price prediction has traditionally been approached using a-spatial linear (or intrinsically linear) hedonic models. It has been shown, however, that spatial effects are inherent in house pricing. This article considers parametric and semi-parametric spatial hedonic model variants that account for spatial autocorrelation, spatial heterogeneity and (smooth and nonparametrically specified) nonlinearities using penalized splines methodology. The models are represented as a mixed model that allow for the estimation of the smoothing parameters along with the other parameters of the model. To assess the out-of-sample performance of the models, the paper uses a database containing the price and characteristics of 10,512 homes in Madrid, Spain (Q1 2010). The results obtained suggest that the nonlinear models accounting for spatial heterogeneity and flexible nonlinear relationships between some of the individual or areal characteristics of the houses and their prices are the best strategies for house price prediction.},
  langid = {english},
  keywords = {housing,price prediction},
  file = {/home/justin/Zotero/storage/27BFDIAE/Montero et al. - 2018 - Housing price prediction parametric versus semi-p.pdf}
}

@article{montoya2020,
  title = {The Poor Fit of Model Fit for Selecting Number of Factors in Exploratory Factor Analysis for Scale Evaluation},
  author = {Montoya, Amanda K. and Edwards, Michael C.},
  year = {2020},
  month = aug,
  journal = {Educational and Psychological Measurement},
  pages = {0013164420942899},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10/gh7r86},
  abstract = {Model fit indices are being increasingly recommended and used to select the number of factors in an exploratory factor analysis. Growing evidence suggests that the recommended cutoff values for common model fit indices are not appropriate for use in an exploratory factor analysis context. A particularly prominent problem in scale evaluation is the ubiquity of correlated residuals and imperfect model specification. Our research focuses on a scale evaluation context and the performance of four standard model fit indices: root mean square error of approximate (RMSEA), standardized root mean square residual (SRMR), comparative fit index (CFI), and Tucker\textendash Lewis index (TLI), and two equivalence test-based model fit indices: RMSEAt and CFIt. We use Monte Carlo simulation to generate and analyze data based on a substantive example using the positive and negative affective schedule (N = 1,000). We systematically vary the number and magnitude of correlated residuals as well as nonspecific misspecification, to evaluate the impact on model fit indices in fitting a two-factor exploratory factor analysis. Our results show that all fit indices, except SRMR, are overly sensitive to correlated residuals and nonspecific error, resulting in solutions that are overfactored. SRMR performed well, consistently selecting the correct number of factors; however, previous research suggests it does not perform well with categorical data. In general, we do not recommend using model fit indices to select number of factors in a scale evaluation framework.},
  langid = {english},
  keywords = {exploratory factor analysis,factor analysis,fit indices,latent variable modeling,model fit,Monte Carlo simulation},
  file = {/home/justin/Zotero/storage/DVJ7A3WS/Montoya_Edwards_2020_The Poor Fit of Model Fit for Selecting Number of Factors in Exploratory Factor.pdf}
}

@book{mulaik2009foundations,
  title = {Foundations of Factor Analysis},
  author = {Mulaik, Stanley A},
  year = {2009},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/b15851},
  date-added = {2020-01-08 11:24:03 -0600},
  date-modified = {2020-01-08 11:24:03 -0600}
}

@article{mundfrom2005,
  title = {Minimum {{Sample Size Recommendations}} for {{Conducting Factor Analyses}}},
  author = {Mundfrom, Daniel J. and Shaw, Dale G. and Ke, Tian Lu},
  year = {2005},
  month = jun,
  journal = {International Journal of Testing},
  volume = {5},
  number = {2},
  pages = {159--168},
  issn = {1530-5058, 1532-7574},
  doi = {10/ccmx3t},
  langid = {english},
  file = {/home/justin/Zotero/storage/VPZBEZAZ/Mundfrom et al. - 2005 - Minimum Sample Size Recommendations for Conducting.pdf}
}

@article{muthen1985,
  title = {A Comparison of Some Methodologies for the Factor Analysis of Nonnormal {{Likert}} Variables},
  author = {{Muth\'en}},
  year = {1985},
  journal = {British Journal of Mathematical \& Statistical Psychology},
  doi = {10/bmjgfb}
}

@article{muthen1987,
  title = {{{LISCOMP}}: {{Analysis}} of Linear Structural Equations with a Comprehensive Measurement Model},
  author = {Muth{\'e}n, Bengt O},
  year = {1987},
  journal = {Mooresville, IN: Scientific Software},
  keywords = {⛔ No DOI found}
}

@article{myers2015a,
  ids = {myers2015},
  title = {Rotation to a Partially Specified Target Matrix in Exploratory Factor Analysis in Practice},
  author = {Myers, Nicholas D. and Jin, Ying and Ahn, Soyeon and Celimli, Seniz and Zopluoglu, Cengiz},
  year = {2015},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {47},
  number = {2},
  pages = {494--505},
  issn = {1554-3528},
  doi = {10/f7jmdz},
  langid = {english},
  file = {/home/justin/Zotero/storage/8WPBD6KI/Myers et al_2015_Rotation to a partially specified target matrix in exploratory factor analysis.pdf}
}

@article{nester1996,
  title = {An Applied Statistician's Creed},
  author = {Nester, Marks R.},
  year = {1996},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {45},
  number = {4},
  pages = {401--410},
  publisher = {{[Wiley, Royal Statistical Society]}},
  issn = {0035-9254},
  doi = {10/bk59hb},
  abstract = {Hypothesis testing, as performed in the applied sciences, is criticized. Then assumptions that the author believes should be axiomatic in all statistical analyses are listed. These assumptions render many hypothesis tests superfluous. The author argues that the image of statisticians will not improve until the nexus between hypothesis testing and statistics is broken.}
}

@article{neuman2000,
  title = {Identifying {{General Factors}} of {{Intelligence}}: {{A Confirmatory Factor Analysis}} of the {{Ball Aptitude Battery}}},
  shorttitle = {Identifying {{General Factors}} of {{Intelligence}}},
  author = {Neuman, George A. and Bolin, Aaron U. and Briggs, Thomas E.},
  year = {2000},
  month = oct,
  journal = {Educational and Psychological Measurement},
  volume = {60},
  number = {5},
  pages = {697--712},
  issn = {0013-1644, 1552-3888},
  doi = {10/dxx8x4},
  abstract = {Research supports a hierarchical factor structure of intelligence that is consistent with the second-order factor model proposed by Gustafsson in which five first-order factors yield a single second-order factor of General Intelligence (g). Gustafsson's model was tested with structural equation modeling via the Ball Aptitude Battery (BAB), a measure of aptitudes and vocational interests. This study focuses on the tests from the BAB that are believed to measure various aspects of intelligence: Numerical Computation, Numerical Reasoning, Inductive Reasoning, Analytical Reasoning, Paper Folding, Idea Fluency, Idea Generation, Vocabulary, Associative Memory, Auditory Memory, Clerical, and Writing Speed. Results indicate that the factor structure of the BAB is consistent with Gustafsson's second-order factor model of intelligence. Implications of this finding are discussed.},
  langid = {english}
}

@article{nydick,
  title = {Multidimensional {{Mastery Testing}} with {{CAT}}},
  author = {Nydick, Steven Warren},
  pages = {267},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/justin/Zotero/storage/EEYASUKG/Nydick - A THESIS SUBMITTED TO THE FACULTY OF THE GRADUATE .pdf}
}

@article{obenchain1975residual,
  title = {Residual Optimality: {{Ordinary}} vs. Weighted vs. Biased Least Squares},
  author = {Obenchain, Robert L},
  year = {1975},
  journal = {Journal of the American Statistical Association},
  volume = {70},
  number = {350},
  pages = {375--379},
  publisher = {{Taylor \& Francis}},
  doi = {10/gjrkcn},
  date-added = {2020-02-19 15:15:56 -0600},
  date-modified = {2020-02-19 15:15:56 -0600}
}

@article{oberski2014,
  title = {Evaluating {{Sensitivity}} of {{Parameters}} of {{Interest}} to {{Measurement Invariance}} in {{Latent Variable Models}}},
  author = {Oberski, Daniel L.},
  year = {2014},
  journal = {Political Analysis},
  volume = {22},
  number = {1},
  pages = {45--60},
  issn = {1047-1987, 1476-4989},
  doi = {10/f5qz8r},
  abstract = {Latent variable models can only be compared across groups when these groups exhibit measurement equivalence or ``invariance,'' since otherwise substantive differences may be confounded with measurement differences. This article suggests examining directly whether measurement differences present could confound substantive analyses, by examining the expected parameter change (EPC)-interest. The EPC-interest approximates the change in parameters of interest that can be expected when freeing cross-group invariance restrictions. Monte Carlo simulations suggest that the EPC-interest approximates these changes well. Three empirical applications show that the EPC-interest can help avoid two undesirable situations: first, it can prevent unnecessarily concluding that groups are incomparable, and second, it alerts the user when comparisons of interest may still be invalidated even when the invariance model appears to fit the data. R code and data for the examples discussed in this article are provided in the electronic appendix (               http://hdl.handle.net/1902.1/21816               ).},
  langid = {english},
  file = {/home/justin/Zotero/storage/MR78I5IK/Oberski_2014_Evaluating Sensitivity of Parameters of Interest to Measurement Invariance in.pdf}
}

@article{oberski2015,
  title = {Evaluating {{Measurement Invariance}} in {{Categorical Data Latent Variable Models}} with the {{EPC-Interest}}},
  author = {Oberski, Daniel L. and Vermunt, Jeroen K. and Moors, Guy B. D.},
  year = {2015},
  journal = {Political Analysis},
  volume = {23},
  number = {4},
  pages = {550--563},
  issn = {1047-1987, 1476-4989},
  doi = {10/f7wc5f},
  abstract = {Many variables crucial to the social sciences are not directly observed but instead are latent and measured indirectly. When an external variable of interest affects this measurement, estimates of its relationship with the latent variable will then be biased. Such violations of ``measurement invariance'' may, for example, confound true differences across countries in postmaterialism with measurement differences. To deal with this problem, researchers commonly aim at ``partial measurement invariance'' that is, to account for those differences that may be present and important. To evaluate this importance directly through sensitivity analysis, the ``EPC-interest'' was recently introduced for continuous data. However, latent variable models in the social sciences often use categorical data. The current paper therefore extends the EPC-interest to latent variable models for categorical data and demonstrates its use in example analyses of U.S. Senate votes as well as respondent rankings of postmaterialism values in the World Values Study.},
  langid = {english},
  file = {/home/justin/Zotero/storage/SIRFL3K3/Oberski et al_2015_Evaluating Measurement Invariance in Categorical Data Latent Variable Models.pdf}
}

@misc{oconnorCautionsRegardingItemLevel,
  title = {Cautions {{Regarding Item-Level Factor Analyses}}},
  author = {O'Connor, Brian}
}

@article{ogasawara2001,
  title = {Approximations to the Distributions of Fit Indexes for Misspecified Structural Equation Models},
  author = {Ogasawara, Haruhiko},
  year = {2001},
  month = oct,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {8},
  number = {4},
  pages = {556--574},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/bv4mwk},
  abstract = {Approximations to the distributions of goodness-of-fit indexes in structural equation modeling are derived with the assumption of multivariate normality and slight misspecification of models. The fit indexes considered in this article are Joreskog and Sorbom's goodness-of-fit index (GFI) and the adjusted GFI, McDonald's absolute GFI, Steiger and Lind's root mean squared error of approximation, Steiger's {$\Gamma$}1 and {$\Gamma$}2, Bentler and Bonett's normed fit index, Bollen's incremental fit index and {$\rho$}1, Tucker and Lewis's index {$\rho$}2, and Bentler's fit index (McDonald and Marsh's relative noncentrality index). An approximation to the asymptotic covariance matrix for the fit indexes is derived by using the delta method. Furthermore, approximations to the densities of the fit indexes are obtained from the transformations of the asymptotically noncentral chi-square distributed variable. A simulation is carried out to confirm the accuracy of the approximations.},
  annotation = {\_eprint: https://doi.org/10.1207/S15328007SEM0804\_03},
  file = {/home/justin/Zotero/storage/NA47C9J2/Ogasawara_2001_Approximations to the Distributions of Fit Indexes for Misspecified Structural.pdf;/home/justin/Zotero/storage/ZC99QWM4/S15328007SEM0804_03.html}
}

@misc{ognyanova2016,
  title = {Basic and Advanced Network Visualization with {{R}}},
  author = {Ognyanova, Katherine},
  year = {2016},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/justin/Zotero/storage/FTRFRM6T/Ognyanova - Basic and advanced network visualization with R.pdf}
}

@book{olsson,
  title = {Do the Test Statistics for {{ML}}, {{NWLS}} and {{GLS}} Follow a Non-Central Chi-Square Distribution under Model Misspecification?},
  author = {Olsson, Ulf Henning and Foss, Tron and Breivik, Einar and Troye, Sigurd V.},
  abstract = {Over the years several discrepancy functions have been introduced both in the literature and in the software of Structural Equation Modeling (SEM). The test statistics for discrepancy functions such as Maximum Likelihood (ML), Generalized Least Squares (GLS) and Normal Theory Weighted Least Squares (NWLS) are all asymptotically equivalent and approximate a central Chi-square statistic when the models are correctly specified and the observed variables are multivariate normally distributed. However, it is known that the distribution of these test statistics will not approximate a central Chisquare distribution for models containing specification error, but is more likely to follow a non-central Chi-square distribution (Browne 1984). This study investigates the empirical distributions of the test statistics for the three discrepancy functions, ML, GLS and NWLS, with regard to their ability to approximate a theoretical non-central Chisquare distribution, when models contain specification error. The study includes different factor models with different types and degrees of specification error. Furthermore, the models also vary with respect to the number of},
  file = {/home/justin/Zotero/storage/K7HU7PPP/Olsson et al_Do the Test Statistics for ML, NWLS and GLS Follow a Non-central Chi-square.pdf;/home/justin/Zotero/storage/F4YMYNNX/summary.html}
}

@article{olsson1979,
  title = {On the Robustness of Factor Analysis against Crude Classification of the Observations},
  author = {Olsson, U},
  year = {1979},
  journal = {Multivariate Behavioral Research},
  doi = {10/fw69pn}
}

@article{olsson1979maximum,
  title = {Maximum Likelihood Estimation of the Polychoric Correlation Coefficient},
  author = {Olsson, Ulf},
  year = {1979},
  journal = {Psychometrika},
  volume = {44},
  number = {4},
  pages = {443--460},
  publisher = {{Springer}},
  doi = {10/ft7tjn},
  date-added = {2019-11-11 14:30:24 -0600},
  date-modified = {2019-11-11 14:30:24 -0600}
}

@article{olsson2004,
  title = {Two Equivalent Discrepancy Functions for Maximum Likelihood Estimation: {{Do}} Their Test Statistics Follow a Non-Central Chi-Square Distribution under Model Misspecification?},
  shorttitle = {Two Equivalent Discrepancy Functions for Maximum Likelihood Estimation},
  author = {Olsson, Ulf Henning and Foss, Tron and Breivik, Einar},
  year = {2004},
  month = may,
  journal = {Sociological Methods \& Research},
  volume = {32},
  number = {4},
  pages = {453--500},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10/ft43gk},
  abstract = {Over the years several discrepancy functions have been introduced both in the literature and in the software of Structural Equation Modeling (SEM). The test statistics for the discrepancy functions associated with Maximum Likelihood (ML), Generalized Least Squares (GLS), and Normal Theory Weighted Least Squares (NWLS) are all asymptotically equivalent. These test statistics are all approximately distributed as central chi-square under correct model specification and if the observed variables are multivariate normally distributed. However, it is known that the distribution of these test statistics will not approximate a central Chi-square distribution for models containing specification error, but is more likely to follow a non-central Chi-square distribution (Browne 1984). This study investigates the empirical distributions of the ML and NWLS discrepancy functions. The study includes 13 different factor models with different types and degrees of specification error. It is found, except for small samples, that the empirical distribution of the ML-test statistic outperforms the empirical distribution of the NWLS-test statistic in terms of approximation to the theoretical non-central Chi-square distribution. Furthermore, in some cases, it turned out that the non-central Chi-square approximation was not appropriate even for models that contained minor and moderate degrees of specification error.},
  langid = {english},
  keywords = {fit assessment,non-central chi-square approximation,specification error,Structural equation modeling},
  file = {/home/justin/Zotero/storage/8CSUR57W/Olsson et al_2004_Two Equivalent Discrepancy Functions for Maximum Likelihood Estimation.pdf}
}

@article{opensciencecollaboration2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716-aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10/68c},
  langid = {english},
  file = {/home/justin/Zotero/storage/QZ7YRXSE/Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf}
}

@book{osterlindDifferentialItemFunctioning2009,
  title = {Differential {{Item Functioning}}},
  author = {Osterlind, Steven and Everson, Howard},
  year = {2009},
  publisher = {{SAGE Publications, Inc.}},
  address = {{2455 Teller Road,~Thousand Oaks~California~91320~United States of America}},
  doi = {10.4135/9781412993913},
  isbn = {978-1-4129-5494-5 978-1-4129-9391-3}
}

@article{paatero1997,
  title = {Least Squares Formulation of Robust Non-Negative Factor Analysis},
  author = {Paatero, Pentti},
  year = {1997},
  month = may,
  journal = {Chemometrics and Intelligent Laboratory Systems},
  volume = {37},
  number = {1},
  pages = {23--35},
  issn = {01697439},
  doi = {10/bbwpf3},
  langid = {english}
}

@article{paek2014,
  title = {A {{Comparison}} of {{Item Parameter Standard Error Estimation Procedures}} for {{Unidimensional}} and {{Multidimensional Item Response Theory Modeling}}},
  author = {Paek, Insu and Cai, Li},
  year = {2014},
  month = feb,
  journal = {Educational and Psychological Measurement},
  volume = {74},
  number = {1},
  pages = {58--76},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10/gf4t8w},
  abstract = {The present study was motivated by the recognition that standard errors (SEs) of item response theory (IRT) model parameters are often of immediate interest to practitioners and that there is currently a lack of comparative research on different SE (or error variance\textendash covariance matrix) estimation procedures. The present study investigated item parameter SEs based on three error variance\textendash covariance matrix estimation procedures for unidimensional and multidimensional IRT models: Fisher information, empirical cross-product, and supplemented expectation maximization. This study centers on the direct comparisons of SEs from different procedures and complements a recent study by Tian, Cai, Thissen, and Xin by providing insights and suggestions on the nature of the differences and similarities as well as on practical matters such as the computational cost. The simulation results show that all three procedures produced similar results with respect to bias in the SE estimates for most conditions. When the number of items is large and sample size is small, empirical cross-product, which was the most computationally efficient procedure, appeared to be affected most, producing slight upward bias.},
  langid = {english},
  file = {/home/justin/Zotero/storage/RFV2GM3T/Paek and Cai - 2014 - A Comparison of Item Parameter Standard Error Esti.pdf}
}

@article{parr2018,
  title = {The {{Matrix Calculus You Need For Deep Learning}}},
  author = {Parr, Terence and Howard, Jeremy},
  year = {2018},
  month = jul,
  journal = {arXiv:1802.01528 [cs, stat]},
  eprint = {1802.01528},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here. See related articles at http://explained.ai},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/justin/Zotero/storage/WWQT4TPB/Parr_Howard_2018_The Matrix Calculus You Need For Deep Learning.pdf;/home/justin/Zotero/storage/PYXXZNK7/1802.html}
}

@article{parry1991,
  title = {An Applied Comparison of Methods for Least- Squares Factor Analysis of Dichotomous Variables},
  author = {Parry, Charles D. H. and McArdle, J.J.},
  year = {1991},
  journal = {Applied Psychological Measurement},
  volume = {15},
  number = {1},
  pages = {35--46},
  publisher = {{SAGE Publications Inc}},
  issn = {0146-6216},
  doi = {10/dc75mq},
  abstract = {A statistical simulation was performed to com pare four least-squares methods of factor analysis on datasets comprising dichotomous variables. In put matrices were: (1) phi correlation coefficients between the observed variables, (2) tetrachoric correlations estimated from bivariate tables of the observed variables, (3) tetrachoric correlations esti mated on the basis of the latent continuous nor mal response variables underlying the observed variables (using LISCOMP with a weighted least- squares factor extraction), or (4) correlations be tween the latent response variables underlying the observed variables based on a variant of latent trait theory (using NOHARM). The simulations were studied under varying sample sizes, threshold values, and population loadings of a factor model. Factor extraction was performed, and a measure of deviation between the population and estimated factor loadings was used as an index of fit. The more sophisticated and less readily available third and fourth methods were not found to be marked ly superior to the first two methods, even for high ly skewed data with small sample sizes. Further simulations were performed to demonstrate the sta bility of the results.},
  langid = {english},
  keywords = {Index terms: binary factor analysis,LISCOMP,NOHARM,simulation.},
  file = {/home/justin/Zotero/storage/M3TBGQ38/Parry_McArdle_1991_An Applied Comparison of Methods for Least- Squares Factor Analysis of.pdf}
}

@article{pavlov2021,
  title = {Using the {{Standardized Root Mean Squared Residual}} ({{SRMR}}) to {{Assess Exact Fit}} in {{Structural Equation Models}}},
  author = {Pavlov, Goran and {Maydeu-Olivares}, Alberto and Shi, Dexin},
  year = {2021},
  month = feb,
  journal = {Educational and Psychological Measurement},
  volume = {81},
  number = {1},
  pages = {110--130},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10/gmchg6},
  abstract = {We examine the accuracy of p values obtained using the asymptotic mean and variance (MV) correction to the distribution of the sample standardized root mean squared residual (SRMR) proposed by Maydeu-Olivares to assess the exact fit of SEM models. In a simulation study, we found that under normality, the MV-corrected SRMR statistic provides reasonably accurate Type I errors even in small samples and for large models, clearly outperforming the current standard, that is, the likelihood ratio (LR) test. When data shows excess kurtosis, MV-corrected SRMR p values are only accurate in small models (p = 10), or in medium-sized models (p = 30) if no skewness is present and sample sizes are at least 500. Overall, when data are not normal, the MV-corrected LR test seems to outperform the MV-corrected SRMR. We elaborate on these findings by showing that the asymptotic approximation to the mean of the SRMR sampling distribution is quite accurate, while the asymptotic approximation to the standard deviation is not.},
  langid = {english},
  keywords = {exact fit,SRMR,structural equation modeling},
  file = {/home/justin/Zotero/storage/NULZNBTT/Pavlov et al_2021_Using the Standardized Root Mean Squared Residual (SRMR) to Assess Exact Fit in.pdf}
}

@article{pearson1900,
  title = {I. {{Mathematical}} Contributions to the Theory of Evolution. \textemdash{{VII}}. {{On}} the Correlation of Characters Not Quantitatively Measurable},
  author = {Pearson, Karl},
  year = {1900},
  journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume = {195},
  number = {262-273},
  pages = {1--47},
  issn = {0264-3952, 2053-9258},
  doi = {10/dmrcvk},
  abstract = {In August, 1899, I presented a memoir to the Royal Society on the inheritance of coat-colour in the horse and of eye-colour in man, which was read November, 1899, and ultimately ordered to be published in the 'Phil. Trans.' Before that memoir was printed, Mr. Yule's valuable memoir on Association was read, and, further, Mr. Leslie Bramley-Moore showed me that the theory of my memoir as given in \textsection{} 6 of the present memoir led to somewhat divergent results according to the methods of proportioning adopted. We therefore undertook a new investigation of the theory of the whole subject, which is embodied in the present memoir. The data involved in the paper on coat-colour in horses and eye-colour in man have all been recalculated, and that paper is nearly ready for presentation. But it seemed best to separate the purely theoretical considerations from their application to special cases of inheritance, and accordingly the old memoir now reappears in two sections. The theory discussed in this paper was, further, the basis of a paper on the Law of Reversion with special reference to the Inheritance of Coat-colour in Basset Hounds recently communicated to the Society, and about to appear in the ` Proceedings. While I am responsible for the general outlines of the present paper, the rough draft of it was taken up and carried on in leisure moments by Mr. Leslie Bramley-Moore, Mr. L. N. G. Filon, M. A., and Miss Alice Lee, D. Sc. Mr. Bramley-Moore discovered the               u               -functions ; Mr. Filon proved most of their general properties and the convergency of the series; I alone am responsible for sections 4, 5, and 6. Mr. Leslie Bramley-Moore sent me, without proof, on the eve of his departure for the Cape, the general expansion for               z               on p. 26. I am responsible for the present proof and its applications. To Dr. Alice Lee we owe most of the illustrations and the table on p. 17. Thus the work is essentially a joint memoir in which we have equal part, and the use of the first personal pronoun is due to the fact that the material had to be put together and thrown into form by one of our number.\textemdash K. P.},
  langid = {english},
  file = {/home/justin/Zotero/storage/48R2EWCI/1901_I.pdf}
}

@article{pearson2010,
  title = {Recommended {{Sample Size}} for {{Conducting Exploratory Factor Analysis}} on {{Dichotomous Data}}},
  author = {Pearson, Robert and Mundform, Daniel},
  year = {2010},
  journal = {Journal of Modern Applied Statistical Methods},
  volume = {9},
  number = {2},
  issn = {1538 - 9472},
  doi = {10/c7dn},
  file = {/home/justin/Zotero/storage/44LYR42P/Pearson_Mundform_2010_Recommended Sample Size for Conducting Exploratory Factor Analysis on.pdf;/home/justin/Zotero/storage/HBZCRK7X/5.html}
}

@phdthesis{pek2012,
  title = {Fungible Parameter Contours and Confidence Regions in Structural Equation Models},
  author = {Pek, Jolynn},
  year = {2012},
  address = {{Chapel Hill, N.C.}},
  doi = {10.17615/S9V9-5M31},
  abstract = {There are at least two kinds of uncertainty associated with parameter estimates when statistical models are fit to sample data. The first kind of uncertainty is typically conveyed by confidence regions which provide a plausible range of values for population parameters of interest. The second kind of uncertainty involves a sensitivity analysis (Cook, 1986) with respect to model fit. Here, contours representing alternative solutions for parameter estimates that are almost as good as the optimal estimates in terms of model fit are obtained. Contours of these slightly suboptimal parameter values have been termed fungible weights or contours (Waller, 2008; Waller \& Jones, 2009; MacCallum, Browne \& Lee, 2009). Although distinct from each other, confidence regions and fungible contours communicate parameter uncertainty and are both computed from the likelihood function. Given these commonalities, we set out to clarify the relationship between confidence regions and fungible parameter contours by accomplishing three objectives. First, we show that confidence regions and fungible parameter contours are analytically related when both types of parameter uncertainty are unified under a general perturbation framework. Second, we carried out a simulation study that confirms the distinction between confidence regions and fungible parameter contours. Although the magnitude of correlations among measured variables have an impact on these two kinds of parameter uncertainty, confidence regions are primarily determined by sample size while fungible parameter contours are determined primarily by model fit and, to a much smaller extent, sampling variability. Third, we implemented a new computational procedure for obtaining confidence regions and fungible parameter contours associated with focal parameters by the profile likelihood method, which takes account of nuisance parameters. We conclude with directions for future research and end with a discussion of what applied researchers may gain from examining these two distinct kinds of parameter uncertainty.},
  copyright = {In Copyright},
  langid = {english},
  school = {University of North Carolina},
  file = {/home/justin/Zotero/storage/VGSMT43B/Jolynn_Fungible Parameter Contours and Confidence Regions in Structural Equation Models.pdf}
}

@article{pendergast2017,
  title = {Measurement Equivalence: {{A}} Non-Technical Primer on Categorical Multi-Group Confirmatory Factor Analysis in School Psychology},
  shorttitle = {Measurement Equivalence},
  author = {Pendergast, Laura L. and {von der Embse}, Nathaniel and Kilgus, Stephen P. and Eklund, Katie R.},
  year = {2017},
  month = feb,
  journal = {Journal of School Psychology},
  volume = {60},
  pages = {65--82},
  issn = {00224405},
  doi = {10/f9t4cv},
  langid = {english}
}

@article{peng2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, Roger D.},
  year = {2011},
  month = dec,
  journal = {Science},
  volume = {334},
  number = {6060},
  pages = {1226--1227},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10/fdv356},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  chapter = {Perspective},
  copyright = {Copyright \textcopyright{} 2011, American Association for the Advancement of Science},
  langid = {english},
  pmid = {22144613},
  file = {/home/justin/Zotero/storage/BAN6G3CP/Peng - 2011 - Reproducible Research in Computational Science.pdf;/home/justin/Zotero/storage/J7GBGNH9/1226.html}
}

@article{peterson2000,
  title = {A {{Meta-Analysis}} of {{Variance Accounted}} for and {{Factor Loadings}} in {{Exploratory Factor Analysis}}},
  author = {Peterson, Robert A.},
  year = {2000},
  month = aug,
  journal = {Marketing Letters},
  volume = {11},
  number = {3},
  pages = {261--275},
  issn = {1573-059X},
  doi = {10/cxkcmp},
  abstract = {A meta-analysis of two factor analysis outcome measures, the percentage of variance accounted for and the average (absolute) factor loading, in 803 substantive factor analyses was undertaken. The average percentage of variance accounted for was 56.6\%, and the average (absolute) factor loading was 0.32. Number of variables factor analyzed, nature of the sample from which data were collected, sample size, number of factors extracted, and (minimal) number of scale categories employed influenced the percentage of variance accounted for in a factor analysis. Number of factors extracted, analytical approach, and number of variables analyzed influenced the average factor loading obtained in a factor analysis. Factor analysis of synthetic (random) data possessing the general structure as the observed data in the meta-analysis accounted for 50.2\% of the variance in the data and produced an average factor loading of 0.21. The latter figures imply that many factor analyses have produced outcome measures of questionable meaningfulness.},
  langid = {english},
  file = {/home/justin/Zotero/storage/R2CB4A6I/Peterson - 2000 - A Meta-Analysis of Variance Accounted for and Fact.pdf}
}

@book{phadkeSummaryMatrixSmoothing2013,
  title = {Summary: {{Matrix Smoothing Methods}}},
  author = {Phadke, Chaitali},
  year = {2013}
}

@inproceedings{piao2019,
  title = {Housing {{Price Prediction Based}} on {{CNN}}},
  booktitle = {2019 9th {{International Conference}} on {{Information Science}} and {{Technology}} ({{ICIST}})},
  author = {Piao, Yong and Chen, Ansheng and Shang, Zhendong},
  year = {2019},
  month = aug,
  pages = {491--495},
  issn = {2573-3311},
  doi = {10/gjrkc9},
  abstract = {Housing price has been one of the most concerned issues to the public all over the world. The excessive growth of housing price will affect not merely the quality of life, but also the business cycle dynamics. However, the factors influencing residential real estate prices are complex and the selection of effective features is vague, which leads to a lower accuracy in many of the traditional housing price prediction approaches. Accordingly, a novel prediction model based on CNN is proposed for prediction of housing price as well as the process of feature selection. Compared with other traditional methods, our work can obtain a better performance through experiments using actual data of property transaction.},
  keywords = {business cycle dynamics,CNN,convolutional neural nets,Convolutional neural network,Data models,economic forecasting,Economic indicators,Feature extraction,feature selection,Feature selection,Housing price prediction,housing price prediction approaches,Investment,Mathematical model,prediction model,Predictive models,pricing,property market,property transaction,residential real estate prices,Training},
  file = {/home/justin/Zotero/storage/SEN9GYBA/8836731.html}
}

@article{pietersz42004,
  title = {Rank Reduction of Correlation Matrices by Majorization},
  author = {Pietersz 4, Raoul and Groenen, Patrick JF},
  year = {2004},
  journal = {Quantitative Finance},
  volume = {4},
  number = {6},
  pages = {649--662},
  doi = {10/b5trxj}
}

@phdthesis{porritt2015,
  title = {Performance of Number of Factors Procedures in Small Sample Sizes},
  author = {Porritt, Marc},
  year = {2015},
  month = sep,
  school = {Loma Linda University},
  keywords = {⛔ No DOI found},
  file = {/home/justin/Zotero/storage/2SHMWR4U/Porritt_2015_Performance of Number of Factors Procedures in Small Sample Sizes.pdf;/home/justin/Zotero/storage/EUSNEQGZ/283.html}
}

@article{preacher2002,
  title = {Exploratory Factor Analysis in Behavior Genetics Research: {{Factor}} Recovery with Small Sample Sizes},
  shorttitle = {Exploratory Factor Analysis in Behavior Genetics Research},
  author = {Preacher, Kristopher J. and MacCallum, Robert C.},
  year = {2002},
  month = mar,
  journal = {Behavior Genetics},
  volume = {32},
  number = {2},
  pages = {153--161},
  issn = {1573-3297},
  doi = {10/cghpnv},
  abstract = {Results of a Monte Carlo study of exploratory factor analysis demonstrate that in studies characterized by low sample sizes the population factor structure can be adequately recovered if communalities are high, model error is low, and few factors are retained. These are conditions likely to be encountered in behavior genetics research involving mean scores obtained from sets of inbred strains. Such studies are often characterized by a large number of measured variables relative to the number of strains used, highly reliable data, and high levels of communality. This combination of characteristics has special consequences for conducting factor analysis and interpreting results. Given that limitations on sample size are often unavoidable, it is recommended that researchers limit the number of expected factors as much as possible.},
  langid = {english},
  file = {/home/justin/Zotero/storage/FMG6YAJV/Preacher_MacCallum_2002_Exploratory Factor Analysis in Behavior Genetics Research.pdf}
}

@phdthesis{preacher2003,
  title = {The Role of Model Complexity in the Evaluation of Structural Equation Models},
  author = {Preacher, Kristopher J.},
  year = {2003},
  address = {{Columbus, OH}},
  langid = {english},
  school = {Ohio State University},
  file = {/home/justin/Zotero/storage/FMQYW6FA/Preacher_2003_The Role of Model Complexity in the Evaluation of Structural Equation Models.pdf;/home/justin/Zotero/storage/ARZ7PQH3/10.html}
}

@article{preacher2003a,
  title = {Repairing {{Tom Swift}}'s {{Electric Factor Analysis Machine}}},
  author = {Preacher, Kristopher J. and MacCallum, Robert C.},
  year = {2003},
  month = feb,
  journal = {Understanding Statistics},
  volume = {2},
  number = {1},
  pages = {13--43},
  issn = {1534-844X, 1532-8031},
  doi = {10/c3d7r4},
  langid = {english},
  file = {/home/justin/Zotero/storage/X4VI7WLK/Preacher and MacCallum - 2003 - Repairing Tom Swift's Electric Factor Analysis Mac.pdf}
}

@misc{preacher20120123,
  title = {The Problem of Model Selection Uncertainty in Structural Equation Modeling.},
  author = {Preacher, Kristopher J.},
  year = {20120123},
  journal = {Psychological Methods},
  volume = {17},
  number = {1},
  pages = {1},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1463},
  doi = {10.1037/a0026804},
  abstract = {APA PsycNet FullTextHTML page},
  langid = {english},
  file = {/home/justin/Zotero/storage/ZLI4CA7K/Preacher_2012_The problem of model selection uncertainty in structural equation modeling.pdf;/home/justin/Zotero/storage/U3NR7Z3F/2012-02072-001.html}
}

@article{preacher2013,
  ids = {preacher2013a,preacherChoosingOptimalNumber2013},
  title = {Choosing the Optimal Number of Factors in Exploratory Factor Analysis: {{A}} Model Selection Perspective},
  shorttitle = {Choosing the Optimal Number of Factors in Exploratory Factor Analysis},
  author = {Preacher, Kristopher J. and Zhang, Guangjian and Kim, Cheongtag and Mels, Gerhard},
  year = {2013},
  month = jan,
  journal = {Multivariate Behavioral Research},
  volume = {48},
  number = {1},
  pages = {28--56},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/gckf6t},
  abstract = {A central problem in the application of exploratory factor analysis is deciding how many factors to retain (m). Although this is inherently a model selection problem, a model selection perspective is rarely adopted for this task. We suggest that Cudeck and Henly's (1991) framework can be applied to guide the selection process. Researchers must first identify the analytic goal: identifying the (approximately) correct m or identifying the most replicable m. Second, researchers must choose fit indices that are most congruent with their goal. Consistent with theory, a simulation study showed that different fit indices are best suited to different goals. Moreover, model selection with one goal in mind (e.g., identifying the approximately correct m) will not necessarily lead to the same number of factors as model selection with the other goal in mind (e.g., identifying the most replicable m). We recommend that researchers more thoroughly consider what they mean by ``the right number of factors'' before they choose fit indices.},
  pmid = {26789208},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2012.710386},
  file = {/home/justin/Zotero/storage/4ASA9ZG7/Preacher et al. - 2013 - Choosing the Optimal Number of Factors in Explorat.pdf;/home/justin/Zotero/storage/L82B7WG6/Preacher et al_2013_Choosing the Optimal Number of Factors in Exploratory Factor Analysis.pdf;/home/justin/Zotero/storage/TELSI6CW/Preacher et al. - 2013 - Choosing the Optimal Number of Factors in Explorat.pdf;/home/justin/Zotero/storage/5HQ3NLJD/00273171.2012.html;/home/justin/Zotero/storage/F6GZ8NET/00273171.2012.html;/home/justin/Zotero/storage/W8LXJMVN/00273171.2012.html}
}

@book{PublicationManualAmerican2010,
  title = {Publication Manual of the {{American Psychological Association}}},
  year = {2010},
  edition = {Sixth},
  publisher = {{American Psychological Association}}
}

@article{putnick2016,
  title = {Measurement Invariance Conventions and Reporting: {{The}} State of the Art and Future Directions for Psychological Research},
  shorttitle = {Measurement Invariance Conventions and Reporting},
  author = {Putnick, Diane L. and Bornstein, Marc H.},
  year = {2016},
  month = sep,
  journal = {Developmental Review},
  volume = {41},
  pages = {71--90},
  issn = {02732297},
  doi = {10/gdqbt6},
  langid = {english},
  file = {/home/justin/Zotero/storage/6RDZFLA6/1-s2.0-S0273229716300351-main.pdf;/home/justin/Zotero/storage/SCPHBSE8/Putnick and Bornstein - 2016 - Measurement invariance conventions and reporting .pdf}
}

@article{qi,
  title = {Computing the {{Nearest Correlation Matrix Problem}} in {{Finance}}},
  author = {Qi, Houduo},
  keywords = {⛔ No DOI found,matrix smooth}
}

@article{qi2006,
  ids = {qiQuadraticallyConvergentNewton2006},
  title = {A Quadratically Convergent {{Newton}} Method for Computing the Nearest Correlation Matrix},
  author = {Qi, Houduo and Sun, Defeng},
  year = {2006},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {28},
  number = {2},
  pages = {360--385},
  publisher = {{SIAM}},
  issn = {0895-4798},
  doi = {10/dxx8g9},
  abstract = {The nearest correlation matrix problem is to find a correlation matrix which is closest to a given symmetric matrix in the Frobenius norm. The well-studied dual approach is to reformulate this problem as an unconstrained continuously differentiable convex optimization problem. Gradient methods and quasi-Newton methods such as BFGS have been used directly to obtain globally convergent methods. Since the objective function in the dual approach is not twice continuously differentiable, these methods converge at best linearly. In this paper, we investigate a Newton-type method for the nearest correlation matrix problem. Based on recent developments on strongly semismooth matrix valued functions, we prove the quadratic convergence of the proposed Newton method. Numerical experiments confirm the fast convergence and the high efficiency of the method.},
  date-added = {2019-12-11 11:58:21 -0600},
  date-modified = {2019-12-11 11:58:21 -0600},
  keywords = {fungibleR,matrix smooth},
  file = {/home/justin/Zotero/storage/77WRE63Z/Qi and Sun - 2006 - A Quadratically Convergent Newton Method for Compu.pdf;/home/justin/Zotero/storage/FG9MWQ7S/050624509.html}
}

@article{qi2013,
  title = {A Semismooth {{Newton}} Method for the Nearest {{Euclidean}} Distance Matrix Problem},
  author = {Qi, Hou-Duo},
  year = {2013},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {34},
  number = {1},
  pages = {67--93},
  doi = {10/f4s57d},
  keywords = {matrix smooth}
}

@manual{R-base,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2021},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}}
}

@book{R-bookdown,
  title = {Bookdown: {{Authoring}} Books and Technical Documents with {{R}} Markdown},
  author = {Xie, Yihui},
  year = {2016},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@book{R-bookdown,
  title = {Bookdown: {{Authoring}} Books and Technical Documents with {{R}} Markdown},
  author = {Xie, Yihui},
  year = {2016},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@manual{R-broom,
  type = {Manual},
  title = {Broom: {{Convert}} Statistical Objects into Tidy Tibbles},
  author = {Robinson, David and Hayes, Alex and Couch, Simon},
  year = {2021}
}

@book{R-car,
  title = {An {{R}} Companion to Applied Regression},
  author = {Fox, John and Weisberg, Sanford},
  year = {2019},
  edition = {Third},
  publisher = {{Sage}},
  address = {{Thousand Oaks CA}}
}

@article{R-colorspace_a,
  title = {{{colorspace}}: {{A}} Toolbox for Manipulating and Assessing Colors and Palettes},
  author = {Zeileis, Achim and Fisher, Jason C. and Hornik, Kurt and Ihaka, Ross and McWhite, Claire D. and Murrell, Paul and Stauffer, Reto and Wilke, Claus O.},
  year = {2020},
  journal = {Journal of Statistical Software},
  volume = {96},
  number = {1},
  pages = {1--49},
  doi = {10.18637/jss.v096.i01}
}

@article{R-colorspace_a,
  title = {{{colorspace}}: {{A}} Toolbox for Manipulating and Assessing Colors and Palettes},
  author = {Zeileis, Achim and Fisher, Jason C. and Hornik, Kurt and Ihaka, Ross and McWhite, Claire D. and Murrell, Paul and Stauffer, Reto and Wilke, Claus O.},
  year = {2020},
  journal = {Journal of Statistical Software},
  volume = {96},
  number = {1},
  pages = {1--49},
  doi = {10.18637/jss.v096.i01}
}

@article{R-colorspace_b,
  title = {Escaping {{RGBland}}: {{Selecting}} Colors for Statistical Graphics},
  author = {Zeileis, Achim and Hornik, Kurt and Murrell, Paul},
  year = {2009},
  journal = {Computational Statistics \& Data Analysis},
  volume = {53},
  number = {9},
  pages = {3259--3270},
  doi = {10.1016/j.csda.2008.11.033}
}

@article{R-colorspace_b,
  title = {Escaping {{RGBland}}: {{Selecting}} Colors for Statistical Graphics},
  author = {Zeileis, Achim and Hornik, Kurt and Murrell, Paul},
  year = {2009},
  journal = {Computational Statistics \& Data Analysis},
  volume = {53},
  number = {9},
  pages = {3259--3270},
  doi = {10.1016/j.csda.2008.11.033}
}

@article{R-colorspace_c,
  title = {Somewhere over the Rainbow: {{How}} to Make Effective Use of Colors in Meteorological Visualizations},
  author = {Stauffer, Reto and Mayr, Georg J. and Dabernig, Markus and Zeileis, Achim},
  year = {2009},
  journal = {Bulletin of the American Meteorological Society},
  volume = {96},
  number = {2},
  pages = {203--216},
  doi = {10.1175/BAMS-D-13-00155.1}
}

@article{R-colorspace_c,
  title = {Somewhere over the Rainbow: {{How}} to Make Effective Use of Colors in Meteorological Visualizations},
  author = {Stauffer, Reto and Mayr, Georg J. and Dabernig, Markus and Zeileis, Achim},
  year = {2009},
  journal = {Bulletin of the American Meteorological Society},
  volume = {96},
  number = {2},
  pages = {203--216},
  doi = {10.1175/BAMS-D-13-00155.1}
}

@manual{R-crayon,
  type = {Manual},
  title = {Crayon: {{Colored}} Terminal Output},
  author = {Cs{\'a}rdi, G{\'a}bor},
  year = {2022}
}

@manual{R-crayon,
  type = {Manual},
  title = {Crayon: {{Colored}} Terminal Output},
  author = {Cs{\'a}rdi, G{\'a}bor},
  year = {2022}
}

@manual{R-devtools,
  type = {Manual},
  title = {Devtools: {{Tools}} to Make Developing r Packages Easier},
  author = {Wickham, Hadley and Hester, Jim and Chang, Winston and Bryan, Jennifer},
  year = {2021}
}

@manual{R-devtools,
  type = {Manual},
  title = {Devtools: {{Tools}} to Make Developing r Packages Easier},
  author = {Wickham, Hadley and Hester, Jim and Chang, Winston and Bryan, Jennifer},
  year = {2021}
}

@manual{R-dplyr,
  type = {Manual},
  title = {Dplyr: {{A}} Grammar of Data Manipulation},
  author = {Wickham, Hadley and Fran{\c c}ois, Romain and Henry, Lionel and M{\"u}ller, Kirill},
  year = {2021}
}

@manual{R-dplyr,
  type = {Manual},
  title = {Dplyr: {{A}} Grammar of Data Manipulation},
  author = {Wickham, Hadley and Fran{\c c}ois, Romain and Henry, Lionel and M{\"u}ller, Kirill},
  year = {2022}
}

@manual{R-dplyr,
  type = {Manual},
  title = {Dplyr: {{A}} Grammar of Data Manipulation},
  author = {Wickham, Hadley and Fran{\c c}ois, Romain and Henry, Lionel and M{\"u}ller, Kirill},
  year = {2022}
}

@manual{R-fungible,
  type = {Manual},
  title = {Fungible: {{Psychometric}} Functions from the {{Waller}} Lab.},
  author = {Waller, Niels G.},
  year = {2020}
}

@manual{R-fungible,
  type = {Manual},
  title = {Fungible: {{Psychometric}} Functions from the Waller Lab.},
  author = {Waller, Niels G.},
  year = {2021}
}

@manual{R-fungible,
  type = {Manual},
  title = {Fungible: {{Psychometric}} Functions from the Waller Lab.},
  author = {Waller, Niels G.},
  year = {2021}
}

@manual{R-gghighlight,
  type = {Manual},
  title = {Gghighlight: {{Highlight Lines}} and {{Points}} in 'Ggplot2'},
  author = {Yutani, Hiroaki},
  year = {2021}
}

@manual{R-gghighlight,
  type = {Manual},
  title = {Gghighlight: {{Highlight Lines}} and {{Points}} in 'Ggplot2'},
  author = {Yutani, Hiroaki},
  year = {2021}
}

@book{R-ggplot2,
  title = {Ggplot2: {{Elegant}} Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  doi = {10.1007/978-3-319-24277-4},
  isbn = {978-3-319-24277-4}
}

@manual{R-gopherdown,
  type = {Manual},
  title = {Gopherdown: {{An}} Updated {{R Markdown}} Thesis Template for the {{University}} of {{Minneosta}} Using the Bookdown Package},
  author = {Zieffler, Andrew and Ismay, Chester},
  year = {2022}
}

@manual{R-gopherdown,
  type = {Manual},
  title = {Gopherdown: {{An}} Updated {{R Markdown}} Thesis Template for the {{University}} of {{Minneosta}} Using the Bookdown Package},
  author = {Zieffler, Andrew and Ismay, Chester},
  year = {2022}
}

@manual{R-here,
  type = {Manual},
  title = {Here: {{A}} Simpler Way to Find Your Files},
  author = {M{\"u}ller, Kirill},
  year = {2020}
}

@manual{R-here,
  type = {Manual},
  title = {Here: {{A}} Simpler Way to Find Your Files},
  author = {M{\"u}ller, Kirill},
  year = {2020}
}

@manual{R-here,
  type = {Manual},
  title = {Here: {{A}} Simpler Way to Find Your Files},
  author = {M{\"u}ller, Kirill},
  year = {2020}
}

@manual{R-kableExtra,
  type = {Manual},
  title = {{{kableExtra}}: {{Construct}} Complex Table with 'kable' and Pipe Syntax},
  author = {Zhu, Hao},
  year = {2020}
}

@manual{R-kableExtra,
  type = {Manual},
  title = {{{kableExtra}}: {{Construct}} Complex Table with 'kable' and Pipe Syntax},
  author = {Zhu, Hao},
  year = {2021}
}

@manual{R-kableExtra,
  type = {Manual},
  title = {{{kableExtra}}: {{Construct}} Complex Table with 'kable' and Pipe Syntax},
  author = {Zhu, Hao},
  year = {2021}
}

@book{R-knitr,
  title = {Dynamic Documents with {{R}} and Knitr},
  author = {Xie, Yihui},
  year = {2015},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}},
  doi = {10.1201/b15166}
}

@book{R-knitr,
  title = {Dynamic Documents with {{R}} and Knitr},
  author = {Xie, Yihui},
  year = {2015},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@book{R-knitr,
  title = {Dynamic Documents with {{R}} and Knitr},
  author = {Xie, Yihui},
  year = {2015},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@manual{R-latex2exp,
  type = {Manual},
  title = {Latex2exp: {{Use LaTeX}} Expressions in Plots},
  author = {Meschiari, Stefano},
  year = {2015}
}

@manual{R-latex2exp,
  type = {Manual},
  title = {Latex2exp: {{Use LaTeX}} Expressions in Plots},
  author = {Meschiari, Stefano},
  year = {2021}
}

@manual{R-latex2exp,
  type = {Manual},
  title = {Latex2exp: {{Use LaTeX}} Expressions in Plots},
  author = {Meschiari, Stefano},
  year = {2021}
}

@book{R-lattice,
  title = {Lattice: {{Multivariate}} Data Visualization with r},
  author = {Sarkar, Deepayan},
  year = {2008},
  publisher = {{Springer}},
  address = {{New York}},
  doi = {10.1007/978-0-387-75969-2}
}

@article{R-lme4,
  title = {Fitting Linear Mixed-Effects Models Using {{lme4}}},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  pages = {1--48},
  doi = {10/gcrnkw},
  bdsk-url-1 = {10.18637/jss.v067.i01},
  doi2 = {10.18637/jss.v067.i01}
}

@book{R-MASS,
  title = {Modern Applied Statistics with s},
  author = {Venables, W. N. and Ripley, B. D.},
  year = {2002},
  edition = {Fourth},
  publisher = {{Springer}},
  address = {{New York}},
  doi = {10.1007/978-0-387-21706-2}
}

@manual{R-Matrix,
  type = {Manual},
  title = {Matrix: {{Sparse}} and Dense Matrix Classes and Methods},
  author = {Bates, Douglas and Maechler, Martin},
  year = {2021}
}

@manual{R-MBESS2,
  type = {Manual},
  title = {{{MBESS}}: {{The MBESS}} r Package},
  author = {Kelley, Ken},
  year = {2020}
}

@manual{R-MBESS2,
  type = {Manual},
  title = {{{MBESS2}}: {{The MBESS R}} Package},
  author = {Kelley, Ken},
  year = {2020}
}

@article{R-mice,
  title = {{{mice}}: {{Multivariate}} Imputation by Chained Equations in r},
  author = {{van Buuren}, Stef and {Groothuis-Oudshoorn}, Karin},
  year = {2011},
  journal = {Journal of Statistical Software},
  volume = {45},
  number = {3},
  pages = {1--67},
  keywords = {⛔ No DOI found}
}

@manual{R-microbenchmark,
  type = {Manual},
  title = {Microbenchmark: {{Accurate}} Timing Functions},
  author = {Mersmann, Olaf},
  year = {2021}
}

@manual{R-microbenchmark,
  type = {Manual},
  title = {Microbenchmark: {{Accurate}} Timing Functions},
  author = {Mersmann, Olaf},
  year = {2021}
}

@manual{R-noisemaker,
  type = {Manual},
  title = {Noisemaker: {{Simulate}} Population Correlation Matrices with Model Error},
  author = {Kracht, Justin}
}

@manual{R-noisemaker,
  type = {Manual},
  title = {Noisemaker: {{Simulate}} Population Correlation Matrices with Model Error},
  author = {Kracht, Justin}
}

@manual{R-papaja,
  type = {Manual},
  title = {{{papaja}}: {{Create APA}} Manuscripts with {{R Markdown}}},
  author = {Aust, Frederik and Barth, Marius},
  year = {2020}
}

@manual{R-papaja,
  type = {Manual},
  title = {{{papaja}}: {{Create APA}} Manuscripts with {{R Markdown}}},
  author = {Aust, Frederik and Barth, Marius},
  year = {2020}
}

@manual{R-papaja,
  type = {Manual},
  title = {{{papaja}}: {{Create APA}} Manuscripts with {{R Markdown}}},
  author = {Aust, Frederik and Barth, Marius},
  year = {2020}
}

@manual{R-parallel,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2022},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}}
}

@manual{R-parallel,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2022},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}}
}

@manual{R-patchwork,
  type = {Manual},
  title = {Patchwork: {{The}} Composer of Plots},
  author = {Pedersen, Thomas Lin},
  year = {2020}
}

@manual{R-patchwork,
  type = {Manual},
  title = {Patchwork: {{The}} Composer of Plots},
  author = {Pedersen, Thomas Lin},
  year = {2020}
}

@manual{R-pbmcapply,
  type = {Manual},
  title = {Pbmcapply: {{Tracking}} the Progress of {{Mc}}*pply with Progress Bar},
  author = {Kuang, Kevin and Kong, Quyu and Napolitano, Francesco},
  year = {2019}
}

@manual{R-pbmcapply,
  type = {Manual},
  title = {Pbmcapply: {{Tracking}} the Progress of {{Mc}}*pply with Progress Bar},
  author = {Kuang, Kevin and Kong, Quyu and Napolitano, Francesco},
  year = {2019}
}

@manual{R-pbmcapply,
  type = {Manual},
  title = {Pbmcapply: {{Tracking}} the Progress of {{Mc}}*pply with Progress Bar},
  author = {Kuang, Kevin and Kong, Quyu and Napolitano, Francesco},
  year = {2019}
}

@manual{R-psych,
  type = {Manual},
  title = {Psych: {{Procedures}} for Psychological, Psychometric, and Personality Research},
  author = {Revelle, William},
  year = {2020},
  address = {{Evanston, Illinois}},
  organization = {{Northwestern University}}
}

@manual{R-purrr,
  type = {Manual},
  title = {Purrr: {{Functional}} Programming Tools},
  author = {Henry, Lionel and Wickham, Hadley},
  year = {2020}
}

@manual{R-purrr,
  type = {Manual},
  title = {Purrr: {{Functional}} Programming Tools},
  author = {Henry, Lionel and Wickham, Hadley},
  year = {2020}
}

@manual{R-purrrgress,
  type = {Manual},
  title = {Purrrgress: {{Progress}} Bars for Purrr},
  author = {Smith, Tyler Grant},
  year = {2021}
}

@manual{R-purrrgress,
  type = {Manual},
  title = {Purrrgress: {{Progress}} Bars for Purrr},
  author = {Smith, Tyler Grant},
  year = {2021}
}

@book{R-rmarkdown_a,
  title = {R Markdown: {{The}} Definitive Guide},
  author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@book{R-rmarkdown_a,
  title = {R Markdown: {{The}} Definitive Guide},
  author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@book{R-rmarkdown_b,
  title = {R Markdown Cookbook},
  author = {Xie, Yihui and Dervieux, Christophe and Riederer, Emily},
  year = {2020},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@book{R-rmarkdown_b,
  title = {R Markdown Cookbook},
  author = {Xie, Yihui and Dervieux, Christophe and Riederer, Emily},
  year = {2020},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@manual{R-scales,
  type = {Manual},
  title = {Scales: {{Scale}} Functions for Visualization},
  author = {Wickham, Hadley and Seidel, Dana},
  year = {2020}
}

@manual{R-scales,
  type = {Manual},
  title = {Scales: {{Scale}} Functions for Visualization},
  author = {Wickham, Hadley and Seidel, Dana},
  year = {2020}
}

@manual{R-sfsmisc,
  type = {Manual},
  title = {Sfsmisc: {{Utilities}} from 'seminar Fuer Statistik' {{ETH}} Zurich},
  author = {Maechler, Martin},
  year = {2021}
}

@manual{R-stringr,
  type = {Manual},
  title = {Stringr: {{Simple}}, Consistent Wrappers for Common String Operations},
  author = {Wickham, Hadley},
  year = {2019}
}

@manual{R-stringr,
  type = {Manual},
  title = {Stringr: {{Simple}}, Consistent Wrappers for Common String Operations},
  author = {Wickham, Hadley},
  year = {2019}
}

@article{R-texreg,
  title = {{{texreg}}: {{Conversion}} of Statistical Model Output in {{R}} to {{LaTeX}} and {{HTML}} Tables},
  author = {Leifeld, Philip},
  year = {2013},
  journal = {Journal of Statistical Software},
  volume = {55},
  number = {8},
  pages = {1--24},
  doi = {10/gg2h7m}
}

@manual{R-thesisdown,
  type = {Manual},
  title = {Thesisdown: {{An}} Updated {{R Markdown}} Thesis Template Using the Bookdown Package},
  author = {Ismay, Chester and Solomon, Nick},
  year = {2022}
}

@manual{R-tidyr,
  type = {Manual},
  title = {Tidyr: {{Tidy}} Messy Data},
  author = {Wickham, Hadley},
  year = {2021}
}

@manual{R-tidyr,
  type = {Manual},
  title = {Tidyr: {{Tidy}} Messy Data},
  author = {Wickham, Hadley and Girlich, Maximilian},
  year = {2022}
}

@manual{R-tidyr,
  type = {Manual},
  title = {Tidyr: {{Tidy}} Messy Data},
  author = {Wickham, Hadley and Girlich, Maximilian},
  year = {2022}
}

@article{R-tidyverse,
  title = {Welcome to the {{tidyverse}}},
  author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and Fran{\c c}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan Milton and M{\"u}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
  year = {2019},
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  doi = {10/ggddkj},
  bdsk-url-1 = {10.21105/joss.01686},
  doi2 = {10.21105/joss.01686}
}

@article{R-tidyverse,
  title = {Welcome to the {{tidyverse}}},
  author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and Fran{\c c}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan Milton and M{\"u}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
  year = {2019},
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  doi = {10.21105/joss.01686}
}

@article{R-tidyverse,
  title = {Welcome to the {{tidyverse}}},
  author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and Fran{\c c}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan Milton and M{\"u}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
  year = {2019},
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  doi = {10.21105/joss.01686}
}

@manual{R-xfun,
  type = {Manual},
  title = {Xfun: {{Supporting}} Functions for Packages Maintained by 'Yihui Xie'},
  author = {Xie, Yihui},
  year = {2022}
}

@manual{R-xfun,
  type = {Manual},
  title = {Xfun: {{Supporting}} Functions for Packages Maintained by 'Yihui Xie'},
  author = {Xie, Yihui},
  year = {2022}
}

@article{ramsay1991,
  title = {Kernel Smoothing Approaches to Nonparametric Item Characteristic Curve Estimation},
  author = {Ramsay, James},
  year = {1991},
  journal = {Psychometrika},
  volume = {56},
  number = {4},
  pages = {611--630},
  keywords = {❓ Multiple DOI,nonparametric IRT}
}

@article{ramsay1995,
  title = {A Similarity-Based Smoothing Approach to Nondimensional Item Analysis},
  author = {Ramsay, JO},
  year = {1995},
  journal = {Psychometrika},
  volume = {60},
  number = {3},
  pages = {323--339},
  doi = {10/b67kv9},
  keywords = {nonparametric IRT}
}

@book{raykov2012,
  title = {A {{First Course}} in {{Structural Equation Modeling}}},
  author = {Raykov, Tenko and Marcoulides, George A.},
  year = {2012},
  month = aug,
  edition = {Second Edition},
  publisher = {{Routledge}},
  doi = {10.4324/9780203930687},
  isbn = {978-0-203-93068-7},
  langid = {english},
  file = {/home/justin/Zotero/storage/UVDJEWK3/Raykov and Marcoulides - 2012 - A First Course in Structural Equation Modeling.pdf}
}

@incollection{reckase2009,
  title = {Multidimensional {{Item Response Theory Models}}},
  booktitle = {Multidimensional {{Item Response Theory}}},
  author = {Reckase, Mark D.},
  editor = {Reckase, M.D.},
  year = {2009},
  series = {Statistics for {{Social}} and {{Behavioral Sciences}}},
  pages = {79--112},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-89976-3_4},
  abstract = {As the previous chapters suggest, it is not difficult to conceive of test items that require more than one hypothetical construct to determine the correct response. However, when describing multidimensional item response theory (MIRT) models, care should be taken to distinguish between dimensions as defined by MIRT models, which represent statistical abstractions of the observed data, and the hypothetical constructs that represent cognitive or affective dimensions of variation in a population of examinees. The earlier chapters present some of those distinctions. This chapter will elaborate on the distinctions between coordinates and constructs and the distinctions will be given additional treatment in Chaps. 6 and 7.},
  isbn = {978-0-387-89976-3},
  langid = {english},
  keywords = {Compensatory Model,Correct Response,Item Parameter,Score Category,Test Item},
  file = {/home/justin/Zotero/storage/WTTCLKNU/Reckase_2009_Multidimensional Item Response Theory Models.pdf}
}

@article{reise2000,
  title = {Factor Analysis and Scale Revision.},
  author = {Reise, Steven P. and Waller, Niels G. and Comrey, Andrew L.},
  year = {2000},
  month = sep,
  journal = {Psychological Assessment},
  volume = {12},
  number = {3},
  pages = {287--297},
  issn = {1939-134X, 1040-3590},
  doi = {10/bdgsn2},
  langid = {english},
  file = {/home/justin/Zotero/storage/PDF9XFEB/Reise et al. - 2000 - Factor analysis and scale revision..pdf}
}

@book{reise2015,
  title = {Handbook of Item Response Theory Modeling: Applications to Typical Performance Assessment},
  shorttitle = {Handbook of Item Response Theory Modeling},
  editor = {Reise, Steven Paul and Revicki, Dennis A.},
  year = {2015},
  series = {Multivariate Applications Series},
  publisher = {{Routledge, Taylor \& Francis Group}},
  address = {{New York}},
  isbn = {978-1-84872-972-8 978-1-138-78785-8},
  lccn = {BF39.2.I84 H36 2015},
  keywords = {Item response theory},
  file = {/home/justin/Zotero/storage/VLHP2UCC/Reise and Revicki - 2015 - Handbook of item response theory modeling applica.pdf}
}

@book{revelle2019,
  title = {Psych: {{Procedures}} for Psychological, Psychometric, and Personality Research},
  author = {Revelle, William},
  year = {2019},
  address = {{Evanston, Illinois}},
  organization = {{Northwestern University}}
}

@article{revuelta2017,
  title = {Bayesian {{Dimensionality Assessment}} for the {{Multidimensional Nominal Response Model}}},
  author = {Revuelta, Javier and Xim{\'e}nez, Carmen},
  year = {2017},
  journal = {Frontiers in Psychology},
  volume = {8},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10/gbj7m4},
  abstract = {This article introduces Bayesian estimation and evaluation procedures for the multidimensional nominal response model. The utility of this model is to perform a nominal factor analysis of items that consist of a finite number of unordered response categories. The key aspect of the model, in comparison with traditional factorial model, is that there is a factor loading for each response category on the latent traits, instead of having one factor loadings associated to the items. The extended parameterization of the multidimensional nominal response model requires large samples for estimation. When sample size is of a moderate or small size, some of these parameters may be weakly empirically identifiable and the estimation algorithm may run into difficulties. We propose a Bayesian MCMC inferential algorithm to estimate the parameters and the number of latent traits underlying the multidimensional nominal response model. Two Bayesian approaches to model evaluation were compared: discrepancy statistics (DIC, WAICC and LOO) that provide an indication of the relative merit of different models, and the standardized generalized discrepancy measure that requires resampling data and is computationally more involved. A simulation study was conducted to compare these two approaches, and the results show that the standardized generalized discrepancy measure can be used to reliably estimate the dimensionality of the model whereas the discrepancy statistics are questionable. The paper also includes an example with real data in the context of learning styles, in which the model is used to conduct an exploratory factor analysis of nominal data.},
  langid = {english},
  keywords = {Bayesian inference,item response theory,multidimensional nominal response model,multivariate analysis,Posterior predictive model evaluation,standardized generalized discrepancy measure},
  file = {/home/justin/Zotero/storage/AE588PC7/Revuelta and Ximénez - 2017 - Bayesian Dimensionality Assessment for the Multidi.pdf}
}

@article{rights2018,
  title = {Addressing Model Uncertainty in Item Response Theory Person Scores through Model Averaging},
  author = {Rights, Jason D. and Sterba, Sonya K. and Cho, Sun-Joo and Preacher, Kristopher J.},
  year = {2018},
  month = oct,
  journal = {Behaviormetrika},
  volume = {45},
  number = {2},
  pages = {495--503},
  issn = {1349-6964},
  doi = {10/ggjrd7},
  abstract = {Item banks are often created in large-scale research and testing settings in the social sciences to predict individuals' latent trait scores. A common procedure is to fit multiple candidate item response theory (IRT) models to a calibration sample and select a single best-fitting IRT model. The parameter estimates from this model are then used to obtain trait scores for subsequent respondents. However, this model selection procedure ignores model uncertainty stemming from the fact that the model ranking in the calibration phase is subject to sampling variability. Consequently, the standard errors of trait scores obtained from subsequent respondents do not reflect such uncertainty. Ignoring such sources of uncertainty contributes to the current replication crisis in the social sciences. In this article, we propose and demonstrate an alternative procedure to account for model uncertainty in this context\textemdash model averaging of IRT trait scores and their standard errors. We outline the general procedure step-by-step and provide software to aid researchers in implementation, both for large-scale research settings with item banks and for smaller research settings involving IRT scoring. We then demonstrate the procedure with a simulated item-banking illustration, comparing model selection and model averaging within sample in terms of predictive coverage. We conclude by discussing ways that model averaging and IRT scoring can be used and investigated in future research.},
  langid = {english},
  file = {/home/justin/Zotero/storage/F5LKBHYY/Rights et al. - 2018 - Addressing model uncertainty in item response theo.pdf}
}

@article{robustlmm,
  title = {{{robustlmm}}: {{An R}} Package for Robust Estimation of Linear Mixed-Effects Models},
  author = {Koller, Manuel},
  year = {2016},
  journal = {Journal of Statistical Software},
  volume = {75},
  number = {6},
  pages = {1--24},
  doi = {10/gjrg3c},
  bdsk-url-1 = {10.18637/jss.v075.i06},
  doi2 = {10.18637/jss.v075.i06}
}

@article{roff1936some,
  title = {Some Properties of the Communality in Multiple Factor Theory},
  author = {Roff, Merrill},
  year = {1936},
  journal = {Psychometrika},
  volume = {1},
  number = {2},
  pages = {1--6},
  publisher = {{Springer}},
  doi = {10/dh4vgw},
  date-added = {2020-01-08 11:29:32 -0600},
  date-modified = {2020-01-08 11:29:32 -0600}
}

@article{rougier2014,
  title = {Ten {{Simple Rules}} for {{Better Figures}}},
  author = {Rougier, Nicolas P. and Droettboom, Michael and Bourne, Philip E.},
  year = {2014},
  month = sep,
  journal = {PLoS Computational Biology},
  volume = {10},
  number = {9},
  pages = {e1003833},
  issn = {1553-7358},
  doi = {10/vm4},
  langid = {english},
  file = {/home/justin/Zotero/storage/3M5U7A67/Rougier et al. - 2014 - Ten Simple Rules for Better Figures.pdf}
}

@article{rousseeuw1993,
  ids = {rousseeuwTransformationNonPositive1993},
  title = {Transformation of Non Positive Semidefinite Correlation Matrices},
  author = {Rousseeuw, Peter J. and Molenberghs, Geert},
  year = {1993},
  month = jan,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {22},
  number = {4},
  pages = {965--984},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0926},
  doi = {10/c5z7kh},
  abstract = {In multivariate statistics, estimation of the covariance or correlation matrix is of crucial importance. Computational and other arguments often lead to the use of coordinate-dependent estimators, yielding matrices that are symmetric but not positive semidefinite. We briefly discuss existing methods, based on shrinking, for transforming such matrices into positive semidefinite matrices, A simple method based on eigenvalues is also considered. Taking into account the geometric structure of correlation matrices, a new method is proposed which uses techniques similar to those of multidimensional scaling.},
  keywords = {correlation,eigenvalue method,fungibleR,matrix smooth,missing data,multidimensional scaling,multivariate probii model,robust correlations,shrinking},
  annotation = {\_eprint: https://doi.org/10.1080/03610928308831068},
  file = {/home/justin/Zotero/storage/B44RXIRA/03610928308831068.html}
}

@article{rousseeuw1994,
  title = {The {{Shape}} of {{Correlation Matrices}}},
  author = {Rousseeuw, Peter J. and Molenberghs, Geert},
  year = {1994},
  month = nov,
  journal = {The American Statistician},
  volume = {48},
  number = {4},
  pages = {276--279},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10/gg5fn4},
  abstract = {A correlation matrix between three variables has to satisfy certain conditions. Such a matrix essentially contains three numbers and thus can be represented by a point in three dimensions. The set of all possible correlation matrices yields a convex solid body with an uncommon shape. All its cross sections perpendicular to the axes are ellipses. At the same time, its surface contains the vertices and edges of a regular tetrahedron. Another unusual shape is obtained for banded correlation matrices between four variables.},
  keywords = {Convexity,Correlation coefficient,Elliptical tetrahedron,Graphical display,Range restrictions},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1994.10476079},
  file = {/home/justin/Zotero/storage/B3VF3TYT/Rousseeuw and Molenberghs - 1994 - The Shape of Correlation Matrices.pdf;/home/justin/Zotero/storage/VLVJL6UA/00031305.1994.html}
}

@book{rubin2004,
  title = {Multiple Imputation for Nonresponse in Surveys},
  author = {Rubin, Donald B},
  year = {2004},
  volume = {81},
  publisher = {{John Wiley \& Sons}}
}

@article{russell2002,
  title = {In {{Search}} of {{Underlying Dimensions}}: {{The Use}} (and {{Abuse}}) of {{Factor Analysis}} in {{Personality}} and {{Social Psychology Bulletin}}},
  shorttitle = {In {{Search}} of {{Underlying Dimensions}}},
  author = {Russell, Daniel W.},
  year = {2002},
  month = dec,
  journal = {Personality and Social Psychology Bulletin},
  volume = {28},
  number = {12},
  pages = {1629--1646},
  issn = {0146-1672, 1552-7433},
  doi = {10/c9fn3m},
  langid = {english},
  file = {/home/justin/Zotero/storage/X6XJPQY2/Russell - 2002 - In Search of Underlying Dimensions The Use (and A.pdf}
}

@article{sackett2001,
  title = {High-Stakes Testing in Employment, Credentialing, and Higher Education: {{Prospects}} in a Post-Affirmative-Action World},
  shorttitle = {High-Stakes Testing in Employment, Credentialing, and Higher Education},
  author = {Sackett, Paul R. and Schmitt, Neal and Ellingson, Jill E. and Kabin, Melissa B.},
  year = {2001},
  journal = {American Psychologist},
  volume = {56},
  number = {4},
  pages = {302--318},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10/ckw2s6},
  abstract = {Cognitively loaded tests of knowledge, skill, and ability often contribute to decisions regarding education, jobs, licensure, or certification. Users of such tests often face difficult choices when trying to optimize both the performance and ethnic diversity of chosen individuals. The authors describe the nature of this quandary, review research on different strategies to address it, and recommend using selection materials that assess the full range of relevant attributes using a format that minimizes verbal content as much as is consistent with the outcome one is trying to achieve. They also recommend the use of test preparation, face-valid assessments, and the consideration of relevant job or life experiences. Regardless of the strategy adopted, it is unreasonable to expect that one can maximize both the performance and ethnic diversity of selected individuals. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Ability Level,Cognitive Assessment,Educational Measurement,Knowledge Level,Measurement,Occupations,Professional Certification,Professional Licensing},
  file = {/home/justin/Zotero/storage/CTDIR2FE/Sackett et al. - 2001 - High-stakes testing in employment, credentialing, .pdf;/home/justin/Zotero/storage/LEF54U5I/2001-00625-002.html}
}

@article{sato1987,
  title = {Pragmatic Treatment of Improper Solutions in Factor Analysis},
  author = {Sato, Manabu},
  year = {1987},
  month = dec,
  journal = {Annals of the Institute of Statistical Mathematics},
  volume = {39},
  number = {2},
  pages = {443--455},
  issn = {0020-3157, 1572-9052},
  doi = {10/ctc4ws},
  langid = {english},
  file = {/home/justin/Zotero/storage/S4AX9W74/Sato - 1987 - Pragmatic treatment of improper solutions in facto.pdf}
}

@article{satorra1985,
  title = {Power of the Likelihood Ratio Test in Covariance Structure Analysis},
  author = {Satorra, Albert and Saris, Willem E.},
  year = {1985},
  month = mar,
  journal = {Psychometrika},
  volume = {50},
  number = {1},
  pages = {83--90},
  issn = {0033-3123, 1860-0980},
  doi = {10/bqsg46},
  abstract = {A procedure for computing the power of the likelihood ratio test used in the context of covariance structure analysis is derived. The procedure uses statistics associated with the standard output of the computer programs commonly used and assumes that a specific alternative value of the parameter vector is specified. Using the noncentral Chi-square distribution, the power of the test is approximated by the asymptotic one for a sequence of local alternatives. The procedure is illustrated by an example. A Monte Carlo experiment also shows how good the approximation is for a specific case.},
  langid = {english},
  file = {/home/justin/Zotero/storage/5VEHNNXL/Satorra and Saris - 1985 - Power of the likelihood ratio test in covariance s.pdf}
}

@article{satorra2015,
  title = {A {{Comment}} on a Paper by {{H}}. {{Wu}} and {{M}}. {{W}}. {{Browne}} (2014)},
  author = {Satorra, Albert},
  year = {2015},
  month = sep,
  journal = {Psychometrika},
  volume = {80},
  number = {3},
  pages = {613--618},
  issn = {0033-3123, 1860-0980},
  doi = {10/gjrkc3},
  langid = {english},
  file = {/home/justin/Zotero/storage/FRGIEYNZ/Satorra_2015_A Comment on a paper by H.pdf}
}

@article{savalei2006,
  title = {Logistic {{Approximation}} to the {{Normal}}: {{The KL Rationale}}},
  shorttitle = {Logistic {{Approximation}} to the {{Normal}}},
  author = {Savalei, Victoria},
  year = {2006},
  month = jul,
  journal = {Psychometrika},
  volume = {71},
  number = {4},
  pages = {763},
  issn = {1860-0980},
  doi = {10/ccxgr7},
  abstract = {A rationale is proposed for approximating the normal distribution with a logistic distribution using a scaling constant based on minimizing the Kullback-Leibler (KL) information, that is, the expected amount of information available in a sample to distinguish between two competing distributions using a likelihood ratio (LR) test, assuming one of them is true. The new constant 1.749, computed assuming the normal distribution is true, yields an approximation that is an improvement in fit of the tails of the distribution as compared to the minimax constant of 1.702, widely used in item response theory (IRT). The minimax constant is by definition marginally better in its overall maximal error. It is argued that the KL constant is more statistically appropriate for use in IRT.},
  langid = {english},
  file = {/home/justin/Zotero/storage/QGZ4LDEG/Savalei - 2006 - Logistic Approximation to the Normal The KL Ratio.pdf}
}

@article{savalei2011,
  title = {What to Do about Zero Frequency Cells When Estimating Polychoric Correlations},
  author = {Savalei, Victoria},
  year = {2011},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {18},
  number = {2},
  pages = {253--273},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/d7vgj7},
  abstract = {Categorical structural equation modeling (SEM) methods that fit the model to estimated polychoric correlations have become popular in the social sciences. When population thresholds are high in absolute value, contingency tables in small samples are likely to contain zero frequency cells. Such cells make the estimation of the polychoric correlations more difficult. Some SEM programs use solutions such as adding .5 to the zero frequency cells. However, the adequacy of this solution has never been systematically explored in the literature. In this article, the frequency with which contingency tables with zero cells occur in small samples is explored. Second, it is explored whether or not adding .5 is in any way preferable to leaving the cell value as zero. Binary and 3-category data and a variety of threshold values and correlation values are considered. The findings suggest that the two methods of dealing with zero frequency cells have quite different properties. With binary data, the method of adding .5 is recommended, unless thresholds are opposite-signed and the underlying correlation is suspected to be large. No adjustment is recommended for greater numbers of categories.},
  keywords = {Categorical Data,categorical SEM,polychoric correlations,Zero-Frequency Cells},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2011.557339},
  file = {/home/justin/Zotero/storage/X298KRSI/Savalei_2011_What to Do About Zero Frequency Cells When Estimating Polychoric Correlations.pdf;/home/justin/Zotero/storage/23STZ95T/10705511.2011.html}
}

@article{savalei2012,
  title = {The Relationship between Root Mean Square Error of Approximation and Model Misspecification in Confirmatory Factor Analysis Models},
  author = {Savalei, Victoria},
  year = {2012},
  month = dec,
  journal = {Educational and Psychological Measurement},
  volume = {72},
  number = {6},
  pages = {910--932},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10/gfpn5w},
  abstract = {The fit index root mean square error of approximation (RMSEA) is extremely popular in structural equation modeling. However, its behavior under different scenarios remains poorly understood. The present study generates continuous curves where possible to capture the full relationship between RMSEA and various ``incidental parameters,'' such as factor loadings and model size, for different types of misspecification. Population RMSEA is studied, removing the influence of sampling fluctuations and making the findings directly applicable to tests of close fit and not-close fit, which require the specification of a population cutoff value. Confirmatory factor analysis models are studied. The results introduce many new findings, including that RMSEA is often insensitive to multiple omitted cross-loadings and to clusters of correlated residuals, that it sometimes behaves counterintuitively as a function of model size, and that it is insensitive to the underlying number of latent factors when a model with one factor is fit.},
  langid = {english},
  keywords = {fit indices,root mean square error of approximation (RMSEA),structural equation modeling (SEM)},
  file = {/home/justin/Zotero/storage/M7R9P3MU/Savalei_2012_The Relationship Between Root Mean Square Error of Approximation and Model.pdf}
}

@book{schaefferModificationNegativeEigenvalues2010,
  title = {Modification of Negative Eigenvalues to Create Positive Definite Matrices and Approximation of Standard Errors of Correlation Estimates},
  author = {Schaeffer, LR},
  year = {2010},
  keywords = {matrix smooth}
}

@article{schafer1999,
  title = {Multiple Imputation: A Primer},
  author = {Schafer, Joseph L},
  year = {1999},
  journal = {Statistical methods in medical research},
  volume = {8},
  number = {1},
  pages = {3--15},
  publisher = {{Sage Publications Sage CA: Thousand Oaks, CA}},
  keywords = {❓ Multiple DOI}
}

@article{schafer2002,
  title = {Missing Data: Our View of the State of the Art.},
  author = {Schafer, Joseph L and Graham, John W},
  year = {2002},
  journal = {Psychological methods},
  volume = {7},
  number = {2},
  pages = {147},
  doi = {10/dsvqnz}
}

@article{schottle2004,
  title = {Improving the Most General Methodology to Create a Valid Correlation Matrix},
  author = {Sch{\"o}ttle, K and Werner, R},
  year = {2004},
  journal = {Risk Analysis},
  volume = {4},
  pages = {701--710},
  keywords = {⛔ No DOI found,correlation,matrix smooth,smooth matrix}
}

@incollection{schweizerComparisonConfirmatoryFactor2015,
  title = {A {{Comparison}} of {{Confirmatory Factor Analysis}} of {{Binary Data}} on the {{Basis}} of {{Tetrachoric Correlations}} and of {{Probability-Based Covariances}}: {{A Simulation Study}}},
  shorttitle = {A {{Comparison}} of {{Confirmatory Factor Analysis}} of {{Binary Data}} on the {{Basis}} of {{Tetrachoric Correlations}} and of {{Probability-Based Covariances}}},
  booktitle = {Quantitative {{Psychology Research}}},
  author = {Schweizer, Karl and Ren, Xuezhu and Wang, Tengfei},
  editor = {Millsap, Roger E. and Bolt, Daniel M. and {van der Ark}, L. Andries and Wang, Wen-Chung},
  year = {2015},
  volume = {89},
  pages = {273--292},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-07503-7_17},
  isbn = {978-3-319-07502-0 978-3-319-07503-7},
  file = {/home/justin/Zotero/storage/THCN35QA/Schweizer et al. - 2015 - A Comparison of Confirmatory Factor Analysis of Bi.pdf}
}

@article{scrucca2013,
  title = {{{GA}}: {{A}} Package for Genetic Algorithms in {{R}}},
  shorttitle = {Ga},
  author = {Scrucca, Luca},
  year = {2013},
  month = apr,
  journal = {Journal of Statistical Software},
  volume = {53},
  number = {1},
  pages = {1--37},
  issn = {1548-7660},
  doi = {10/gft29t},
  copyright = {Copyright (c) 2011 Luca Scrucca},
  langid = {english},
  file = {/home/justin/Zotero/storage/AW57ELT6/Scrucca_2013_GA.pdf;/home/justin/Zotero/storage/LRPXWR8V/v053i04.html}
}

@article{selim2009,
  title = {Determinants of House Prices in {{Turkey}}: {{Hedonic}} Regression versus Artificial Neural Network},
  shorttitle = {Determinants of House Prices in {{Turkey}}},
  author = {Selim, Hasan},
  year = {2009},
  month = mar,
  journal = {Expert Systems with Applications},
  volume = {36},
  number = {2, Part 2},
  pages = {2843--2852},
  issn = {0957-4174},
  doi = {10/b24b4s},
  abstract = {Determinants of house prices in Turkey are examined in this paper using the 2004 Household Budget Survey Data. In property valuation and housing market research, the locational value is usually analyzed by hedonic methods that use multiple regression techniques on large data sets and require a formality based on microeconomic theory in the analyses. Because of potential non-linearity in the hedonic functions, artificial neural network (ANN) is employed in this study as an alternative method. By comparing the prediction performance between the hedonic regression and artificial neural network models, this study demonstrates that ANN can be a better alternative for prediction of the house prices in Turkey.},
  langid = {english},
  keywords = {Artificial neural networks,Hedonic regression,House price,Turkey},
  file = {/home/justin/Zotero/storage/NCA5KHKR/S0957417408000596.html}
}

@book{semnetSEMNETFAQNPD2016,
  title = {{{SEMNET FAQ NPD R}}},
  author = {{SEMNET}},
  year = {2016},
  keywords = {matrix smooth}
}

@inproceedings{shahhosseini2020,
  title = {Optimizing {{Ensemble Weights}} for {{Machine Learning Models}}: {{A Case Study}} for {{Housing Price Prediction}}},
  shorttitle = {Optimizing {{Ensemble Weights}} for {{Machine Learning Models}}},
  booktitle = {Smart {{Service Systems}}, {{Operations Management}}, and {{Analytics}}},
  author = {Shahhosseini, Mohsen and Hu, Guiping and Pham, Hieu},
  editor = {Yang, Hui and Qiu, Robin and Chen, Weiwei},
  year = {2020},
  series = {Springer {{Proceedings}} in {{Business}} and {{Economics}}},
  pages = {87--97},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10/gjrkdf},
  abstract = {Designing ensemble learners has been recognized as one of the significant trends in the field of data knowledge, especially, in data science competitions. Building models that are able to outperform all individual models in terms of bias, which is the error due to the difference in the average model predictions and actual values, and variance, which is the variability of model predictions, has been the main goal of the studies in this area. An optimization model has been proposed in this paper to design ensembles that try to minimize bias and variance of predictions. Focusing on service sciences, two well-known housing datasets have been selected as case studies: Boston housing and Ames housing. The results demonstrate that our designed ensembles can be very competitive in predicting the house prices in both Boston and Ames datasets.},
  isbn = {978-3-030-30967-1},
  langid = {english},
  keywords = {Bias-Variance trade-off,House price prediction,Machine learning,Optimal ensemble},
  file = {/home/justin/Zotero/storage/4MVW7NTF/Shahhosseini et al. - 2020 - Optimizing Ensemble Weights for Machine Learning M.pdf}
}

@article{shapiro1983,
  title = {Asymptotic Distribution Theory in the Analysis of Covariance Structures},
  author = {Shapiro, Alexander},
  year = {1983},
  month = jan,
  journal = {South African Statistical Journal},
  volume = {17},
  number = {1},
  pages = {33--81},
  publisher = {{South African Statistical Association (SASA)}},
  doi = {10.10520/AJA0038271X_800},
  abstract = {In this paper we present a unified approach to the asymptotic distribution theory of covariance structures. Our approach is based on the differential calculus of min-max functions, which is developed in the first sections of the paper. The general theory is demonstrated by means of generalized least squares estimators. New and well-known results are obtained in the general framework. In particular, the asymptotic behavior of the estimators under an alternative hypothesis and the problem of non-identifiability are discussed. Finally, potential possibilities of the theory are illustrated by the example of minimum trace factor analysis.},
  keywords = {⚠️ Invalid DOI,Asymptotic distribution,Covariance structures,distribution,Maximum likelihood},
  file = {/home/justin/Zotero/storage/B2XFJPB9/Shapiro_1983_Asymptotic distribution theory in the analysis of covariance structures.pdf}
}

@article{shapiro1988,
  title = {On the Asymptotic Bias of Estimators under Parameter Drift},
  author = {Shapiro, Alexander and Browne, Michael W.},
  year = {1988},
  month = dec,
  journal = {Statistics \& Probability Letters},
  volume = {7},
  number = {3},
  pages = {221--224},
  publisher = {{North-Holland}},
  issn = {0167-7152},
  doi = {10/ff8k6m},
  abstract = {It is shown that, under a natural assumption, minimum discrepancy estimators in the analysis of moment structures are asymptotically unbiased.},
  langid = {english},
  file = {/home/justin/Zotero/storage/7G5Z6YFW/1988_On the asymptotic bias of estimators under parameter drift.pdf;/home/justin/Zotero/storage/H8NDJJVM/0167715288900545.html}
}

@article{shapiro2002,
  ids = {shapiroStatisticalInferenceMinimum2002},
  title = {Statistical Inference of Minimum Rank Factor Analysis},
  author = {Shapiro, Alexander and {ten Berge}, Jos M. F.},
  year = {2002},
  journal = {Psychometrika},
  volume = {67},
  number = {1},
  pages = {79--94},
  issn = {0033-3123, 1860-0980},
  doi = {10/fbqgvq},
  langid = {english},
  file = {/home/justin/Zotero/storage/NL3YALBW/Shapiro and ten Berge - 2002 - Statistical inference of minimum rank factor analy.pdf}
}

@inbook{shapiro2007,
  title = {Statistical Inference of Moment Structures},
  booktitle = {Handbook of Latent Variable and Related Models},
  author = {Shapiro, Alexander},
  year = {2007},
  edition = {1st ed..},
  pages = {229--260},
  publisher = {{Amsterdam}},
  address = {{Amsterdam}},
  collaborator = {Lee, Sik-Yum},
  isbn = {978-0-444-52044-9},
  keywords = {Latent structure analysis,Latent variables},
  file = {/home/justin/Zotero/storage/ULWQWER9/Shapiro_2007_Statistical Inference of Moment Structures.pdf}
}

@article{shapiro2015,
  title = {Comments on ``{{Quantifying Adventitious Error}} in a {{Covariance Structure}} as a {{Random Effect}}'' by {{Hao Wu}} and {{Michael Browne}}},
  author = {Shapiro, Alexander},
  year = {2015},
  month = sep,
  journal = {Psychometrika},
  volume = {80},
  number = {3},
  pages = {611--612},
  issn = {1860-0980},
  doi = {10/ggfnkg},
  langid = {english},
  file = {/home/justin/Zotero/storage/PDHHZSVE/Shapiro_2015_Comments on “Quantifying Adventitious Error in a Covariance Structure as a.pdf}
}

@article{shewach2017,
  title = {Differential {{Prediction}} in the {{Use}} of the {{SAT}} and {{High School Grades}} in {{Predicting College Performance}}: {{Joint Effects}} of {{Race}} and {{Language}}},
  shorttitle = {Differential {{Prediction}} in the {{Use}} of the {{SAT}} and {{High School Grades}} in {{Predicting College Performance}}},
  author = {Shewach, Oren R. and Shen, Winny and Sackett, Paul R. and Kuncel, Nathan R.},
  year = {2017},
  journal = {Educational Measurement: Issues and Practice},
  volume = {36},
  number = {3},
  pages = {46--57},
  issn = {1745-3992},
  doi = {10/gf38fg},
  abstract = {The literature on differential prediction of college performance of racial/ethnic minority students for standardized tests and high school grades indicates the use of these predictors often results in overprediction of minority student performance. However, these studies typically involve native English-speaking students. In contrast, a smaller literature on language proficiency suggests academic performance of those with more limited English language proficiency may be underpredicted by standardized tests. These two literatures have not been well integrated, despite the fact that a number of racial/ethnic minority groups within the United States contain recent immigrant populations or heritage language speakers. This study investigates the joint role of race/ethnicity and language proficiency in Hispanic, Asian, and White ethnic groups across three educational admissions systems (SAT, HSGPA, and their composite) in predicting freshman grades. Our results indicate that language may differentially affect academic outcomes for different racial/ethnic subgroups. The SAT loses predictive power for Asian and White students who speak another best language, whereas it does not for Hispanic students who speak another best language. The differential prediction of college grades of linguistic minorities within racial/ethnic minority subgroups appears to be driven by the verbally loaded subtests of standardized tests but is largely unrelated to quantitative tests.},
  copyright = {\textcopyright{} 2017 by the National Council on Measurement in Education},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/emip.12150},
  file = {/home/justin/Zotero/storage/UP7DKNXA/Shewach et al. - 2017 - Differential Prediction in the Use of the SAT and .pdf}
}

@book{shottsLinuxCommandLine2016,
  title = {The {{Linux Command Line}}},
  author = {Shotts, William},
  year = {2016},
  publisher = {{No Starch Press}}
}

@book{silge,
  title = {Supervised {{Machine Learning}} for {{Text Analysis}} in {{R}}},
  author = {Silge, Emil Hvitfeldt {and} Julia},
  abstract = {Supervised Machine Learning for Text Analysis in R},
  file = {/home/justin/Zotero/storage/UL4U9G3I/smltar.com.html}
}

@article{simonian2010,
  title = {The Most Simple Methodology to Create a Valid Correlation Matrix for Risk Management and Option Pricing Purposes},
  author = {Simonian, Joseph},
  year = {2010},
  journal = {Applied Economics Letters},
  volume = {17},
  number = {18},
  pages = {1767--1768},
  doi = {10/fh75h4},
  keywords = {correlation,matrix smooth,smooth matrix}
}

@article{smith1995,
  title = {Differentiation of the {{Cholesky}} Algorithm},
  author = {Smith, Stephen P},
  year = {1995},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {4},
  number = {2},
  pages = {134--147},
  keywords = {❓ Multiple DOI,matrix smooth}
}

@book{socanIncrementalValueMinimum2003,
  title = {The Incremental Value of Minimum Rank Factor Analysis},
  author = {So{\v c}an, Gregor},
  year = {2003},
  publisher = {{s.n.] ; University Library Groningen] [Host}},
  address = {{S.l.; Groningen}},
  isbn = {978-90-367-1882-0},
  langid = {english},
  annotation = {OCLC: 66753097},
  file = {/home/justin/Zotero/storage/IKQEWV5C/Sočan - 2003 - The incremental value of minimum rank factor analy.pdf}
}

@techreport{spray1990comparison,
  type = {Research {{Report}}},
  title = {Comparison of Two Logistic Multidimensional Item Response Theory Models},
  author = {Spray, Judith A and Davey, Tim C and Reckase, Mark D and Ackerman, Terry A and Carlson, James E},
  year = {1990},
  number = {ONR90-8},
  address = {{Iowa City, IA}},
  institution = {{ACT, Inc.}},
  file = {/home/justin/Zotero/storage/4SMZGZ9U/Spray et al_1990_Comparison of two logistic multidimensional item response theory models.pdf}
}

@misc{steele2008,
  title = {Models: {{Masterpieces}} and Lame Excuses},
  author = {Steele, J. M.},
  year = {2008},
  howpublished = {http://www-stat.wharton.upenn.edu/\textasciitilde steele/Rants/ModelsMandLE.html},
  file = {/home/justin/Zotero/storage/CTCSU83D/ModelsMandLE.html}
}

@inproceedings{steiger1980,
  title = {Statistically Based Tests for the Number of Common Factors},
  booktitle = {Annual {{Meeting}} of of the {{Psychometric Society}}},
  author = {Steiger, J. H. and Lind, J. C.},
  year = {1980},
  address = {{Iowa City, IA}},
  date-added = {2020-05-05 13:09:45 -0500},
  date-modified = {2020-05-05 13:11:56 -0500},
  howpublished = {Paper presented at the annual meeting of the Psychometric Society},
  keywords = {⛔ No DOI found}
}

@book{steiger1989ezpath,
  title = {{{EzPATH}}: Causal Modeling: A Supplementary Module for {{SYSTAT}} and {{SYGRAPH}}: {{PC-MS-DOS}}, Version 1.0},
  author = {Steiger, J. H.},
  year = {1989},
  publisher = {{Systat}}
}

@article{steiger1990,
  title = {Structural Model Evaluation and Modification: {{An}} Interval Estimation Approach},
  shorttitle = {Structural Model Evaluation and Modification},
  author = {Steiger, J. H.},
  year = {1990},
  month = apr,
  journal = {Multivariate Behavioral Research},
  volume = {25},
  number = {2},
  pages = {173--180},
  issn = {0027-3171, 1532-7906},
  doi = {10/db5},
  langid = {english},
  file = {/home/justin/Zotero/storage/TZ6JZEAB/Steiger_1990_Structural Model Evaluation and Modification.pdf}
}

@article{steiger1998,
  title = {A Note on Multiple Sample Extensions of the {{RMSEA}} Fit Index},
  author = {Steiger, J. H.},
  year = {1998},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {5},
  number = {4},
  pages = {411--419},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/dwjkgf},
  abstract = {Generalization of the Steiger-Lind root mean square error of approximation fit indexes and interval estimation procedure to models based on multiple independent samples is discussed. In this article, we suggest an approach that seems both reasonable and workable, and caution against one that definitely seems inappropriate.},
  annotation = {\_eprint: https://doi.org/10.1080/10705519809540115},
  file = {/home/justin/Zotero/storage/ZE8DS4IP/Steiger - 1998 - A note on multiple sample extensions of the RMSEA .pdf;/home/justin/Zotero/storage/27PEGINF/10705519809540115.html}
}

@article{steiger2007,
  title = {Understanding the Limitations of Global Fit Assessment in Structural Equation Modeling},
  author = {Steiger, J. H.},
  year = {2007},
  month = may,
  journal = {Personality and Individual Differences},
  series = {Special Issue on {{Structural Equation Modeling}}},
  volume = {42},
  number = {5},
  pages = {893--898},
  issn = {0191-8869},
  doi = {10/dsxstk},
  abstract = {Barrett's (2007) article on ``adjudging model fit'' raises some important issues concerning the use of global fit indices to justify weak structural equation models, and recommends prohibition of future use of such indices. In this commentary, I critique Barrett's presentation, and show that his recommendations are (a) unnecessarily regressive, and (b) likely to be ignored. Then I suggest a constructive alternative in line with the spirit of his concerns.},
  langid = {english},
  keywords = {Confidence interval estimation,Fit indices,Hypothesis testing,Structural equation modeling},
  file = {/home/justin/Zotero/storage/BMQ5BLWD/Steiger_2007_Understanding the limitations of global fit assessment in structural equation.pdf;/home/justin/Zotero/storage/SMX76UVG/S0191886906003825.html}
}

@misc{sterba20160125,
  title = {Effects of Parceling on Model Selection: {{Parcel-allocation}} Variability in Model Ranking.},
  shorttitle = {Effects of Parceling on Model Selection},
  author = {Sterba, Sonya K.},
  year = {20160125},
  journal = {Psychological Methods},
  volume = {22},
  number = {1},
  pages = {47},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1463},
  doi = {10.1037/met0000067},
  abstract = {APA PsycNet FullTextHTML page},
  langid = {english},
  file = {/home/justin/Zotero/storage/SDKB42HR/Sterba_2016_Effects of parceling on model selection.pdf;/home/justin/Zotero/storage/YTMR9SNZ/2016-03903-001.html}
}

@article{steyer2015,
  title = {Some Comments on {{Wu}} and {{Browne}}},
  author = {Steyer, Rolf and Sengewald, Erik and Hahn, Sonja},
  year = {2015},
  month = sep,
  journal = {Psychometrika},
  volume = {80},
  number = {3},
  pages = {608--610},
  issn = {1860-0980},
  doi = {10/ggfnkh},
  langid = {english},
  file = {/home/justin/Zotero/storage/JDUJXIDA/Steyer et al. - 2015 - Some Comments on Wu and Browne.pdf}
}

@article{stirling1981,
  title = {Least {{Squares Subject}} to {{Linear Constraints}}},
  author = {Stirling, W Douglas},
  year = {1981},
  journal = {Journal of the Royal Society, Applied Statistics, C},
  volume = {30},
  number = {2},
  pages = {20H-212},
  keywords = {⛔ No DOI found}
}

@book{strang2005,
  title = {Introduction to Linear Algebra},
  author = {Strang, Gilbert},
  year = {2005},
  edition = {3rd ed.},
  publisher = {{Wellesley-Cambridge Pr}},
  address = {{Wellesley, Mass}},
  isbn = {978-0-9614088-9-3 978-0-9614088-4-8},
  langid = {english}
}

@article{stuive2008,
  title = {The Empirical Verification of an Assignment of Items to Subtests: {{The}} Oblique Multiple Group Method versus the Confirmatory Common Factor Method},
  shorttitle = {The Empirical Verification of an Assignment of Items to Subtests},
  author = {Stuive, Ilse and Kiers, Henk A. L. and Timmerman, Marieke E. and {ten Berge}, Jos M. F.},
  year = {2008},
  month = dec,
  journal = {Educational and Psychological Measurement},
  volume = {68},
  number = {6},
  pages = {923--939},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10/fc4gmd},
  abstract = {This study compares two confirmatory factor analysis methods on their ability to verify whether correct assignments of items to subtests are supported by the data. The confirmatory common factor (CCF) method is used most often and defines nonzero loadings so that they correspond to the assignment of items to subtests. Another method is the oblique multiple group (OMG) method, which defines subtests as unweighted sums of the scores on all items assigned to the subtest, and (corrected) correlations are used to verify the assignment. A simulation study compares both methods, accounting for the influence of model error and the amount of unique variance. The CCF and OMG methods show similar behavior with relatively small amounts of unique variance and low interfactor correlations. However, at high amounts of unique variance and high interfactor correlations, the CCF detected correct assignments more often, whereas the OMG was better at detecting incorrect assignments.},
  langid = {english},
  keywords = {comparative study,confirmatory common factor method,confirmatory factor analysis,oblique multiple group method},
  file = {/home/justin/Zotero/storage/I98UZAXR/Stuive et al_2008_The Empirical Verification of an Assignment of Items to Subtests.pdf;/home/justin/Zotero/storage/NX8MSIP8/Stuive et al_2008_The Empirical Verification of an Assignment of Items to Subtests.pdf}
}

@article{suedoenihm1976,
  title = {Polynomial Law of Sensation},
  author = {{Sue Doe Nihm}},
  year = {1976},
  journal = {American Psychologist},
  volume = {31},
  number = {11},
  pages = {808--809},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10/dc5xsp},
  abstract = {A new theory proposes that sensation grows as a polynomial function of physical intensity. The theory reproduces all of the published data perfectly without error. The degree of the polynomial is found to be always independent of all the experimental manipulations affecting the power function exponent except the number of stimuli. The polynomial law always provides a superior fit to the data, and should be used to determine the status of experimental methods and as a validity criterion for testing substantive theories. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Mathematical Modeling,Psychophysical Measurement,Stimulus Intensity},
  file = {/home/justin/Zotero/storage/VW6TL4II/1977-28640-001.html}
}

@article{sueiro2011,
  title = {Assessing {{Goodness}} of {{Fit}} in {{Item Response Theory With Nonparametric Models}}: {{A Comparison}} of {{Posterior Probabilities}} and {{Kernel-Smoothing Approaches}}},
  author = {Sueiro, Manuel J and Abad, Francisco J},
  year = {2011},
  journal = {Educational and Psychological Measurement},
  pages = {0013164410393238},
  doi = {10/d6rkcs},
  keywords = {nonparametric IRT}
}

@article{sun2005,
  title = {Assessing {{Goodness}} of {{Fit}} in {{Confirmatory Factor Analysis}}},
  author = {Sun, Jun},
  year = {2005},
  month = jan,
  journal = {Measurement and Evaluation in Counseling and Development},
  volume = {37},
  number = {4},
  pages = {240--256},
  publisher = {{Routledge}},
  issn = {0748-1756},
  doi = {10/gg4hbk},
  abstract = {The author identifies 3 main purposes of conducting confirmatory factor analysis (CFA). and their different requirements on goodness-of-fit assessment. For a better understanding of fit indices, he proposes a hierarchical classification scheme based on J. S. Tanaka's (1993) multifaceted conceptions and discusses how to assess goodness of fit for different purposes in CFA.},
  annotation = {\_eprint: https://doi.org/10.1080/07481756.2005.11909764},
  file = {/home/justin/Zotero/storage/62LZZSRL/Sun_2005_Assessing Goodness of Fit in Confirmatory Factor Analysis.pdf}
}

@article{sun2005a,
  title = {Assessing {{Goodness}} of {{Fit}} in {{Confirmatory Factor Analysis}}},
  author = {Sun, Jun},
  year = {2005},
  month = jan,
  journal = {Measurement and Evaluation in Counseling and Development},
  volume = {37},
  number = {4},
  pages = {240--256},
  issn = {0748-1756, 1947-6302},
  doi = {10/gg4hbk},
  langid = {english}
}

@phdthesis{sun2015,
  ids = {sun2015a},
  title = {Constructing a Misspecified Item Response Model That Yields a Specified Estimate and a Specified Model Misfit Value},
  author = {Sun, Yinghao},
  year = {2015},
  address = {{Columbus, OH}},
  abstract = {Item response theory (IRT) models are usually built on a set of  statistical assumptions which may not necessarily hold in real data. Understanding the behavior of IRT models in response to deviations from these assumptions can provide valuable information as how to apply IRT models in practice and how to interpret results. One way to study the behavior of IRT models when their assumptions do not hold exactly is through simulations, where data can be generated from a model constructed by deliberately violating some of the IRT model assumptions. This dissertation presents a method to perturb an IRT model so that its particular structure only holds approximately. The departure of the original IRT model from the perturbed model is operationalized by an exact value of model misfit. Meanwhile, maximum likelihood estimates (MLEs) of parameters in the original IRT model given data generated from the perturbed model converge almost surely to specified values. Therefore, starting from an IRT model with a set of specified parameter values, the proposed method allows us to construct a perturbed (or misspecified) IRT model such that MLEs remain unchanged and yet there is a specified degree of model misfit. It is then possible to construct a simulated environment where the original IRT model only holds approximately through generating data from the perturbed model. The proposed perturbation method can be formulated as a constrained optimization problem, which can be solved by several commonly available optimization routines, such as the interior-point method. Illustrated through a few simulation studies using the 1- and 2-parameter logistic model, it is shown that the perturbation method is working as expected, yielding specified estimates and specified model misfit values. Despite its application to IRT models in this dissertation, the perturbation method is generic and can be applied to a wide range of statistical models with different measures of model misfit.},
  school = {Ohio State University},
  file = {/home/justin/Zotero/storage/NSX6XTQ9/Sun_2015_Constructing a Misspecified Item Response Model That Yields a Specified.pdf;/home/justin/Zotero/storage/VPQTTA8F/10.html}
}

@inproceedings{sun2019,
  title = {The {{Impact}} of {{Person-Organization Fit}} on {{Talent Management}}: {{A Structure-Aware Convolutional Neural Network Approach}}},
  shorttitle = {The {{Impact}} of {{Person-Organization Fit}} on {{Talent Management}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Sun, Ying and Zhuang, Fuzhen and Zhu, Hengshu and Song, Xin and He, Qing and Xiong, Hui},
  year = {2019},
  pages = {1625--1633},
  publisher = {{ACM}},
  address = {{Anchorage AK USA}},
  doi = {10/gf7nbq},
  abstract = {Person-Organization fit (P-O fit) refers to the compatibility between employees and their organizations. The study of P-O fit is important for enhancing proactive talent management. While considerable efforts have been made in this direction, it still lacks a quantitative and holistic way for measuring P-O fit and its impact on talent management. To this end, in this paper, we propose a novel datadriven neural network approach for dynamically modeling the compatibility in P-O fit and its meaningful relationships with two critical issues in talent management, namely talent turnover and job performance. Specifically, inspired by the practical management scenarios, we first creatively design an Organizational Structureaware Convolutional Neural Network (OSCN) for hierarchically extracting organization-aware compatibility features for measuring P-O fit. Then, to capture the dynamic nature of P-O fit and its consequent impact, we further exploit an adapted Recurrent Neural Network with attention mechanism to model the temporal information of P-O fit. Finally, we compare our approach with a number of state-of-the-art baseline methods on real-world talent data. Experimental results clearly demonstrate the effectiveness in terms of turnover prediction and job performance prediction. Moreover, we also show some interesting indicators of talent management through the visualization of network layers.},
  isbn = {978-1-4503-6201-6},
  langid = {english},
  file = {/home/justin/Zotero/storage/F59XMJ6N/Sun et al. - 2019 - The Impact of Person-Organization Fit on Talent Ma.pdf}
}

@article{suresh2020,
  title = {A {{Framework}} for {{Understanding Unintended Consequences}} of {{Machine Learning}}},
  author = {Suresh, Harini and Guttag, John V.},
  year = {2020},
  month = feb,
  journal = {arXiv:1901.10002 [cs, stat]},
  eprint = {1901.10002},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of potential sources of unwanted consequences. For instance, downstream harms to particular groups are often blamed on "biased data," but this concept encompass too many issues to be useful in developing solutions. In this paper, we provide a framework that partitions sources of downstream harm in machine learning into six distinct categories spanning the data generation and machine learning pipeline. We describe how these issues arise, how they are relevant to particular applications, and how they motivate different solutions. In doing so, we aim to facilitate the development of solutions that stem from an understanding of application-specific populations and data generation processes, rather than relying on general statements about what may or may not be "fair."},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/justin/Zotero/storage/YGALDKX9/Suresh and Guttag - 2020 - A Framework for Understanding Unintended Consequen.pdf}
}

@incollection{sympson1978,
  title = {A Model for Testing with Multidimensional Items},
  booktitle = {Proceedings of the 1977 {{Computerized Adaptive Testing Conference}}},
  author = {Sympson, JB},
  editor = {Weiss, David J},
  year = {1978},
  address = {{University of Minnesota, Minneapolis}}
}

@article{takane1987,
  title = {On the Relationship between Item Response Theory and Factor Analysis of Discretized Variables},
  author = {Takane, Yoshio and {de Leeuw}, Jan},
  year = {1987},
  month = sep,
  journal = {Psychometrika},
  volume = {52},
  number = {3},
  pages = {393--408},
  issn = {1860-0980},
  doi = {10/cshs9w},
  abstract = {Equivalence of marginal likelihood of the two-parameter normal ogive model in item response theory (IRT) and factor analysis of dichotomized variables (FA) was formally proved. The basic result on the dichotomous variables was extended to multicategory cases, both ordered and unordered categorical data. Pair comparison data arising from multiple-judgment sampling were discussed as a special case of the unordered categorical data. A taxonomy of data for the IRT and FA models was also attempted.},
  langid = {english},
  file = {/home/justin/Zotero/storage/DSXW7NYS/Takane and de Leeuw - 1987 - On the relationship between item response theory a.pdf}
}

@article{tanaka1989,
  title = {A General Coefficient of Determination for Covariance Structure Models under Arbitrary {{GLS}} Estimation},
  author = {Tanaka, J. S. and Huba, G. J.},
  year = {1989},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {42},
  number = {2},
  pages = {233--239},
  issn = {2044-8317},
  doi = {10/brh26z},
  abstract = {Previous results presented in Tanaka \& Huba (1985) introduced a general Fit index for covariance structure models under generalized least squares (GLS) estimation which, in some cases, specialized to the fit indices presented in J\"oreskog \& S\"orbom (1981). Here, it is shown that, for a wide class of models, the general form of this fit index can be expressed as a weighted coefficient of determination. This coefficient is given as the ratio of weighted trace functions of predicted and observed covariance matrix elements.},
  copyright = {1989 The British Psychological Society},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1989.tb00912.x},
  file = {/home/justin/Zotero/storage/LFCBL2IK/Tanaka_Huba_1989_A general coefficient of determination for covariance structure models under.pdf;/home/justin/Zotero/storage/IPUFLDJJ/j.2044-8317.1989.tb00912.html}
}

@article{teng2021,
  title = {Exploiting {{Network Fusion}} for {{Organizational Turnover Prediction}}},
  author = {Teng, Mingfei and Zhu, Hengshu and Liu, Chuanren and Xiong, Hui},
  year = {2021},
  journal = {ACM Transactions on Management Information Systems},
  volume = {12},
  number = {2},
  pages = {1--18},
  issn = {2158-656X, 2158-6578},
  doi = {10/gj7hmh},
  abstract = {As an emerging measure of proactive talent management, talent turnover prediction is critically important for companies to attract, engage, and retain talents in order to prevent the loss of intellectual capital. While tremendous efforts have been made in this direction, it is not clear how to model the influence of employees' turnover within multiple organizational social networks. In this article, we study how to exploit turnover contagion by developing a Turnover Influence-based Neural Network (TINN) for enhancing organizational turnover prediction. Specifically, TINN can construct the turnover similarity network which is then fused with multiple organizational social networks. The fusion is achieved either through learning a hidden turnover influence network or through integrating the turnover influence on multiple networks. Taking advantage of the Graph Convolutional Network and the Long Short-Term Memory network, TINN can dynamically model the impact of social influence on talent turnover. Meanwhile, the utilization of the attention mechanism improves the interpretability, providing insights into the impact of different networks along time on the future turnovers. Finally, we conduct extensive experiments in real-world settings to evaluate TINN. The results validate the effectiveness of our approach to enhancing organizational turnover prediction. Also, our case studies reveal some interpretable findings, such as the importance of each network or hidden state which potentially impacts future organizational turnovers.},
  langid = {english},
  file = {/home/justin/Zotero/storage/TH5WPG7J/Teng et al. - 2021 - Exploiting Network Fusion for Organizational Turno.pdf}
}

@book{thurstone1947,
  ids = {thurstone1947multiple,thurstoneMultiplefactorAnalysisDevelopment1947a},
  title = {Multiple-Factor Analysis; a Development and Expansion of {{The Vectors}} of {{Mind}}},
  author = {Thurstone, L. L.},
  year = {1947},
  series = {Multiple-Factor Analysis; a Development and Expansion of {{The Vectors}} of {{Mind}}},
  pages = {xix, 535},
  publisher = {{University of Chicago Press}},
  address = {{Chicago, IL, US}},
  abstract = {This book presents a review of the work on multiple-factor analysis done in the past decade by Thurstone and his associates, and it presents in coherent fashion the mathematical development of multiple-factor analysis. The present book includes most of the material that was present in "The Vectors of Mind" (see 9: 5998). Much of the book is spent on methods of factorial analysis using examples which tend to point out clearly the problems involved. There is detailed discussion of various controversial areas. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  collection = {Multiple-factor analysis; a development and expansion of The Vectors of Mind},
  keywords = {Factor analysis,Methodology,Psychology},
  file = {/home/justin/Zotero/storage/FI39JAYQ/000580702.html;/home/justin/Zotero/storage/IKPN72FV/1947-02833-000.html}
}

@article{timmerman2011,
  ids = {timmerman2011dimensionality,timmermanDimensionalityAssessmentOrdered2011},
  title = {Dimensionality Assessment of Ordered Polytomous Items with Parallel Analysis},
  author = {Timmerman, Marieke E. and {Lorenzo-Seva}, Urbano},
  year = {2011},
  journal = {Psychological Methods},
  volume = {16},
  number = {2},
  pages = {209--220},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10/ctx6h7},
  abstract = {Parallel analysis (PA) is an often-recommended approach for assessment of the dimensionality of a variable set. PA is known in different variants, which may yield different dimensionality indications. In this article, the authors considered the most appropriate PA procedure to assess the number of common factors underlying ordered polytomously scored variables. They proposed minimum rank factor analysis (MRFA) as an extraction method, rather than the currently applied principal component analysis (PCA) and principal axes factoring. A simulation study, based on data with major and minor factors, showed that all procedures consistently point at the number of major common factors. A polychoric-based PA slightly outperformed a Pearson-based PA, but convergence problems may hamper its empirical application. In empirical practice, PA-MRFA with a 95\% threshold based on polychoric correlations or, in case of nonconvergence, Pearson correlations with mean thresholds appear to be a good choice for identification of the number of common factors. PA-MRFA is a common-factor-based method and performed best in the simulation experiment. PA based on PCA with a 95\% threshold is second best, as this method showed good performances in the empirically relevant conditions of the simulation experiment. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  date-added = {2020-02-07 11:25:03 -0600},
  date-modified = {2020-02-07 11:25:03 -0600},
  keywords = {Common Factors,correlation,Exploratory Factor Analysis,Factor Analysis,number of factors,parallel analysis,Principal Component Analysis,ShapeImproperR,smooth matrix,Statistical Correlation},
  file = {/home/justin/Zotero/storage/EJL5CXKV/2011-07789-001.html}
}

@article{tomarken2003,
  title = {Potential Problems with ``well Fitting'' Models.},
  author = {Tomarken, Andrew J. and Waller, Niels G.},
  year = {2003},
  journal = {Journal of Abnormal Psychology},
  volume = {112},
  number = {4},
  pages = {578},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1846},
  doi = {10/fpmxjq},
  abstract = {APA PsycNet FullTextHTML page},
  langid = {english},
  file = {/home/justin/Zotero/storage/YTV4ES2B/Tomarken_2003_Potential problems with well fitting models.pdf;/home/justin/Zotero/storage/8ENL5IFJ/2003-10098-005.html}
}

@article{touw2013,
  title = {Data Mining in the {{Life Sciences}} with {{Random Forest}}: A Walk in the Park or Lost in the Jungle?},
  shorttitle = {Data Mining in the {{Life Sciences}} with {{Random Forest}}},
  author = {Touw, Wouter G. and Bayjanov, Jumamurat R. and Overmars, Lex and Backus, Lennart and Boekhorst, Jos and Wels, Michiel and {van Hijum}, Sacha A. F. T.},
  year = {2013},
  month = may,
  journal = {Briefings in Bioinformatics},
  volume = {14},
  number = {3},
  pages = {315--326},
  publisher = {{Oxford Academic}},
  issn = {1467-5463},
  doi = {10/h4b},
  abstract = {Abstract.  In the Life Sciences `omics' data is increasingly generated by different high-throughput technologies. Often only the integration of these data allow},
  langid = {english},
  file = {/home/justin/Zotero/storage/YEAFKTD3/Touw et al. - 2013 - Data mining in the Life Sciences with Random Fores.pdf;/home/justin/Zotero/storage/IVQ9VITV/255469.html}
}

@article{tran2009,
  ids = {tranPerformanceParallelAnalysis2009},
  title = {Performance of {{Parallel Analysis}} in {{Retrieving Unidimensionality}} in the {{Presence}} of {{Binary Data}}},
  author = {Tran, Ulrich S. and Formann, Anton K.},
  year = {2009},
  month = feb,
  journal = {Educational and Psychological Measurement},
  volume = {69},
  number = {1},
  pages = {50--61},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10/btzmpg},
  abstract = {Parallel analysis has been shown to be suitable for dimensionality assessment in factor analysis of continuous variables. There have also been attempts to demonstrate that it may be used to uncover the factorial structure of binary variables conforming to the unidimensional normal ogive model. This article provides both theoretical and empirical evidence that this is not appropriate. Results of a simulation study indicate that sample size, item discrimination, and type of correlation coefficient (Pearson vs. tetrachoric correlation) considerably influence the performance of parallel analysis. Reliability of parallel analysis with binary variables is found to be notably poor for Pearson correlations and also limited for tetrachoric correlations.},
  langid = {english},
  keywords = {correlation,ShapeImproperR,smooth matrix},
  file = {/home/justin/Zotero/storage/AJ3NXTPV/Tran and Formann - 2009 - Performance of Parallel Analysis in Retrieving Uni.pdf}
}

@article{tran2010,
  title = {{{IRT Modelling}} of {{Dichotomous Items}} with {{Linear Factor Analysis}}},
  author = {Tran, Ulrich S. and Formann, Anton K.},
  year = {2010},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10/gkq37z},
  abstract = {Bearing on linear factor analytical techniques applied to matrices of tetrachoric and Pearson correlations of dichotomous variables, we describe heuristic methods of parameter estimation in the twoparameter normal ogive model and discuss some indices and methods of factor analysis and structural equation modeling which were never investigated systematically for the assessment of unidimensionality in this case. The results of a simulation study indicate that heuristic parameter estimation is surprisingly accurate when distributional assumptions are met, mostly being only slightly inferior to marginal maximum likelihood estimation. Moreover, recovery of unidimensionality appears feasible with residualbased indices and methods but also depends on type of correlation coefficient used. We discuss our findings with regard to their generalizability, their relevance to applied research, and the refinement of parameter estimation techniques in item response theory.},
  langid = {english},
  file = {/home/justin/Zotero/storage/I43JS3S6/Tran and Formann - 2010 - IRT Modelling of Dichotomous Items with Linear Fac.pdf}
}

@article{trichtinger2020,
  title = {Quantifying Model Error in {{P-technique}} Factor Analysis},
  author = {Trichtinger, Lauren A. and Zhang, Guangjian},
  year = {2020},
  month = jan,
  journal = {Multivariate Behavioral Research},
  volume = {0},
  number = {0},
  pages = {1--16},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/gh64s8},
  abstract = {P-technique factor analysis is an exploratory factor model for multivariate time series data. Assessing model fit of P-technique factor models is non-trivial because time series data are correlated at nearby time points. We present a test statistic that is appropriate for P-technique factor analysis. In addition, the test statistic allows researchers to quantify the amount of model error. We explore the statistical properties of the test statistic with simulated data and we illustrate its use with an empirical study of personality states. Results of the simulation study include (1) the empirical distributions of the test statistic approximately followed their respective theoretical chi-square distributions, (2) the empirical Type I error rates of the test of perfect fit are close to the nominal level and the empirical Type I error rates of the test of close fit are slightly lower than the nominal level, and (3) the empirical power rates of the test of perfect fit are satisfactory but the empirical power rates of the test of close fit are only satisfactory for small models.},
  pmid = {32000534},
  keywords = {factor analysis,intensive longitudinal data,P-technique,time series},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2020.1717414},
  file = {/home/justin/Zotero/storage/IVXLKDD5/Trichtinger_Zhang_2020_Quantifying Model Error in P-technique Factor Analysis.pdf;/home/justin/Zotero/storage/EDF3C65H/00273171.2020.html}
}

@incollection{truran2013,
  title = {Models: {{Useful}} but {{Not True}}},
  shorttitle = {Models},
  booktitle = {Practical {{Applications}} of the {{Philosophy}} of {{Science}}: {{Thinking}} about {{Research}}},
  author = {Truran, Peter},
  editor = {Truran, Peter},
  year = {2013},
  series = {{{SpringerBriefs}} in {{Philosophy}}},
  pages = {61--67},
  publisher = {{Springer International Publishing}},
  address = {{Heidelberg}},
  doi = {10.1007/978-3-319-00452-5_10},
  abstract = {Our theories are, to some extent, models of reality. Like all models, a theory or hypothesis will tell us a truth about an aspect of reality, but it will not be the whole truth. All models are approximations of reality. Understand the limitations and assumptions of any theoretical model that you are using.},
  isbn = {978-3-319-00452-5},
  langid = {english},
  keywords = {Constructive Empiricism,Empirical Adequacy,Global Warming,Mathematical Construct,Scientific Realism},
  file = {/home/justin/Zotero/storage/RILXNWUD/Truran_2013_Models.pdf}
}

@techreport{tucker1967evaluation,
  title = {Evaluation of Factor Analytic Research Procedures by Means of Simulated Correlation Matrices},
  author = {Tucker, L. R. and Koopman, R.F. and Linn, R.L.},
  year = {1967},
  institution = {{Department of Psychology, University of Illinois, Urbana}}
}

@article{tucker1969,
  title = {Evaluation of Factor Analytic Research Procedures by Means of Simulated Correlation Matrices},
  author = {Tucker, L. R. and Koopman, R. F. and Linn, R. L.},
  year = {1969},
  journal = {Psychometrika},
  volume = {34},
  number = {4},
  pages = {421--459},
  publisher = {{Springer}},
  issn = {1860-0980},
  doi = {10/chcxvf},
  abstract = {In order to study the effectiveness of factor analytic methods, a procedure was developed for computing simulated correlation matrices which are more similar to real data correlation matrices than are those matrices computed from the factor analysis structural model. In the present investigation, three methods of factor extraction were studied as applied to 54 simulated correlation matrices which varied in proportion of variance derived from a major factor domain, number of factors in the major domain, and closeness of the simulation procedure to the factor analysis structural model. While the factor extraction methods differed little from one another in quality of results for matrices more dissimilar to the factor analytic model, major differences in quality of results were associated with fewer factors in the major domain, higher proportion of variance from the major domain, and closeness of the simulation procedure to the factor analysis structural model.},
  date-added = {2019-11-20 15:02:05 -0600},
  date-modified = {2019-11-20 15:02:05 -0600},
  langid = {english},
  file = {/home/justin/Zotero/storage/JVP8CPKK/Tucker et al. - 1969 - Evaluation of factor analytic research procedures .pdf}
}

@article{tucker1973,
  title = {A Reliability Coefficient for Maximum Likelihood Factor Analysis},
  author = {Tucker, L. R. and Lewis, Charles},
  year = {1973},
  month = mar,
  journal = {Psychometrika},
  volume = {38},
  number = {1},
  pages = {1--10},
  issn = {1860-0980},
  doi = {10/bcz7k9},
  abstract = {Maximum likelihood factor analysis provides an effective method for estimation of factor matrices and a useful test statistic in the likelihood ratio for rejection of overly simple factor models. A reliability coefficient is proposed to indicate quality of representation of interrelations among attributes in a battery by a maximum likelihood factor analysis. Usually, for a large sample of individuals or objects, the likelihood ratio statistic could indicate that an otherwise acceptable factor model does not exactly represent the interrelations among the attributes for a population. The reliability coefficient could indicate a very close representation in this case and be a better indication as to whether to accept or reject the factor solution.},
  langid = {english},
  file = {/home/justin/Zotero/storage/GCES5TR8/Tucker_Lewis_1973_A reliability coefficient for maximum likelihood factor analysis.pdf}
}

@misc{ulrich2004,
  title = {On the Correlation of a Naturally and an Artificially Dichotomized Variable},
  author = {Ulrich, Rolf and Wirtz, Markus},
  year = {2004},
  month = nov,
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {57},
  number = {2},
  pages = {235--251},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {2044-8317},
  howpublished = {https://onlinelibrary.wiley.com/doi/abs/10.1348/0007110042307203},
  langid = {english},
  file = {/home/justin/Zotero/storage/HVTL4ZQY/showCitFormats.html}
}

@book{UniBiDimensional2011,
  title = {({{Uni-}} and {{Bi-}}) {{Dimensional Scaling}}: {{A Toolbox}} for {{MATLAB}}},
  year = {2011}
}

@book{vaart1998,
  title = {Asymptotic {{Statistics}}},
  author = {van der Vaart, A. W.},
  year = {1998},
  month = oct,
  edition = {First},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9780511802256},
  isbn = {978-0-521-49603-2 978-0-521-78450-4 978-0-511-80225-6},
  file = {/home/justin/Zotero/storage/8FNLYXUV/Vaart - 1998 - Asymptotic Statistics.pdf}
}

@article{vanberkel2019,
  ids = {vanberkelCrowdsourcingPerceptionsFair2019a},
  title = {Crowdsourcing {{Perceptions}} of {{Fair Predictors}} for {{Machine Learning}}: {{A Recidivism Case Study}}},
  shorttitle = {Crowdsourcing {{Perceptions}} of {{Fair Predictors}} for {{Machine Learning}}},
  author = {{van Berkel}, Niels and Goncalves, Jorge and Hettiachchi, Danula and Wijenayake, Senuri and Kelly, Ryan M. and Kostakos, Vassilis},
  year = {2019},
  month = nov,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {3},
  number = {CSCW},
  pages = {28:1--28:21},
  doi = {10/gjk7xg},
  abstract = {The increased reliance on algorithmic decision-making in socially impactful processes has intensified the calls for algorithms that are unbiased and procedurally fair. Identifying fair predictors is an essential step in the construction of equitable algorithms, but the lack of ground-truth in fair predictor selection makes this a challenging task. In our study, we recruit 90 crowdworkers to judge the inclusion of various predictors for recidivism. We divide participants across three conditions with varying group composition. Our results show that participants were able to make informed decisions on predictor selection. We find that agreement with the majority vote is higher when participants are part of a more diverse group. The presented workflow, which provides a scalable and practical approach to reach a diverse audience, allows researchers to capture participants' perceptions of fairness in private while simultaneously allowing for structured participant discussion.},
  keywords = {algorithmic decision making,artificial intelligence,bias,chatbots,crime,crowdsourcing,fairness,intelligible models,modelling bias,perceived fairness},
  file = {/home/justin/Zotero/storage/4PVHYEC3/van Berkel et al. - 2019 - Crowdsourcing Perceptions of Fair Predictors for M.pdf;/home/justin/Zotero/storage/CR2DLXCK/van Berkel et al. - 2019 - Crowdsourcing Perceptions of Fair Predictors for M.pdf}
}

@article{vandriel1978,
  ids = {van1978various},
  title = {On Various Causes of Improper Solutions in Maximum Likelihood Factor Analysis},
  author = {{van Driel}, Otto P.},
  year = {1978},
  journal = {Psychometrika},
  volume = {43},
  number = {2},
  pages = {225--243},
  publisher = {{Springer}},
  issn = {0033-3123, 1860-0980},
  doi = {10/cdh8hk},
  date-added = {2020-02-07 11:19:50 -0600},
  date-modified = {2020-02-07 11:19:50 -0600},
  langid = {english},
  file = {/home/justin/Zotero/storage/D9YE7D9U/van Driel - 1978 - On various causes of improper solutions in maximum.pdf}
}

@article{vardaman2015,
  title = {Translating {{Intentions}} to {{Behavior}}: {{The Interaction}} of {{Network Structure}} and {{Behavioral Intentions}} in {{Understanding Employee Turnover}}},
  shorttitle = {Translating {{Intentions}} to {{Behavior}}},
  author = {Vardaman, James M. and Taylor, Shannon G. and Allen, David G. and Gondo, Maria B. and Amis, John M.},
  year = {2015},
  journal = {Organization Science},
  volume = {26},
  number = {4},
  pages = {1177--1191},
  publisher = {{INFORMS}},
  issn = {1047-7039},
  doi = {10/f6c8},
  abstract = {This paper integrates psychological and sociological perspectives to provide a more complete explanation of the link between intended and actual turnover. Findings from two studies suggest that the translation of intentions to leave one's job into turnover behavior is attenuated by centrality in organizational advice and friendship networks. Our results demonstrate that psychological and network factors jointly impact employee turnover, and distinguish the effects of different types of networks (friendship, advice), ties (in-degree, out-degree), and levels (dyadic, triadic) in the turnover process. We discuss the implications of these findings for research and practice, and propose a two-stage model of turnover grounded in temporal construal theory that describes how psychological and structural factors variously influence the turnover decision process.},
  file = {/home/justin/Zotero/storage/JKYBTQ68/Vardaman et al_2015_Translating Intentions to Behavior.pdf;/home/justin/Zotero/storage/NDQYEKWV/orsc.2015.html}
}

@article{velicer1990,
  title = {Component {{Analysis}} versus {{Common Factor Analysis}}: {{Some}} Issues in {{Selecting}} an {{Appropriate Procedure}}},
  shorttitle = {Component {{Analysis}} versus {{Common Factor Analysis}}},
  author = {Velicer, Wayne F. and Jackson, Douglas N.},
  year = {1990},
  month = jan,
  journal = {Multivariate Behavioral Research},
  volume = {25},
  number = {1},
  pages = {1--28},
  issn = {0027-3171, 1532-7906},
  doi = {10/c2bc34},
  langid = {english},
  file = {/home/justin/Zotero/storage/RANWI3HS/Velicer and Jackson - 1990 - Component Analysis versus Common Factor Analysis .pdf}
}

@article{velicer1998,
  title = {Affects of Variable and Subject Sampling on Factor Pattern Recovery.},
  author = {Velicer, Wayne F. and Fava, Joseph L.},
  year = {1998},
  journal = {Psychological Methods},
  volume = {3},
  number = {2},
  pages = {231--251},
  issn = {1082-989X},
  doi = {10/cdpq54},
  langid = {english},
  file = {/home/justin/Zotero/storage/DPN6GBMD/Velicer and Fava - 1998 - Affects of variable and subject sampling on factor.pdf}
}

@article{verbeke1997,
  title = {The Effect of Misspecifying the Random-Effects Distribution in Linear Mixed Models for Longitudinal Data},
  author = {Verbeke, Geert and Lesaffre, Emmanuel},
  year = {1997},
  journal = {Computational Statistics \& Data Analysis},
  volume = {23},
  number = {4},
  pages = {541--556},
  publisher = {{Elsevier}},
  doi = {10/cr498h}
}

@misc{voulodimosDeepLearningComputer2018,
  type = {Review {{Article}}},
  title = {Deep {{Learning}} for {{Computer Vision}}: {{A Brief Review}}},
  shorttitle = {Deep {{Learning}} for {{Computer Vision}}},
  author = {Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
  year = {2018},
  month = feb,
  journal = {Computational Intelligence and Neuroscience},
  volume = {2018},
  pages = {e7068349},
  publisher = {{Hindawi}},
  issn = {1687-5265},
  doi = {10.1155/2018/7068349},
  abstract = {Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.},
  langid = {english},
  keywords = {computer vision,deep learning},
  file = {/home/justin/Zotero/storage/6EXWSDXW/Voulodimos et al. - 2018 - Deep Learning for Computer Vision A Brief Review.pdf;/home/justin/Zotero/storage/RGYHC3I7/7068349.html}
}

@article{waller1994,
  title = {The Cultural Transmission of Romantic Love Styles: {{A Twin-family}} Study},
  author = {Waller, Niels G.},
  year = {1994},
  journal = {Psychological Science},
  volume = {5},
  pages = {268--274},
  issn = {0956-7976},
  doi = {10/fgmzhc},
  langid = {english}
}

@article{waller1994a,
  title = {Individual Differences in Age Preferences in Mates},
  author = {Waller, Niels G.},
  year = {1994},
  month = sep,
  journal = {Behavioral and Brain Sciences},
  volume = {17},
  number = {3},
  pages = {578--581},
  publisher = {{Cambridge University Press}},
  issn = {1469-1825, 0140-525X},
  doi = {10/gjrkcr},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0140525X00036050/resource/name/firstPage-S0140525X00036050a.jpg},
  langid = {english},
  file = {/home/justin/Zotero/storage/GSKFK84M/D924FD074DBDD5C5D1B800A8EB07A057.html}
}

@incollection{Waller1999Eval,
  title = {Evaluating the {{Structure}} of {{Personality}}},
  booktitle = {Personality and Psychopathology},
  author = {Waller, Niels G},
  year = {1999},
  pages = {155--200},
  publisher = {{American Psychiatric Press}},
  address = {{Washington, DC}},
  isbn = {0-88048-923-5},
  langid = {english},
  keywords = {Mental Disorders – etiology,Mental illness – Etiology,Pathological,Personality,Personality Development,Personality disorders – Complications,Personality Disorders – complications,Psychology}
}

@incollection{waller2013,
  title = {Antecedent {{Probability}} and the {{Efficiency}} of {{Psychometric Signs}}, {{Patterns}}, or {{Cutting Scores}}},
  booktitle = {A {{Paul Meehl Reader}}: {{Essays}} on the {{Practice}} of {{Scientific Psychology}}},
  author = {Waller, Niels G.},
  year = {2013},
  month = sep,
  series = {Multivariate Applications Series},
  edition = {First},
  publisher = {{Routledge}},
  doi = {10.4324/9780203759554},
  isbn = {978-0-203-75955-4},
  langid = {english}
}

@article{waller2016,
  title = {Fungible {{Correlation Matrices}}: {{A Method}} for {{Generating Nonsingular}}, {{Singular}}, and {{Improper Correlation Matrices}} for {{Monte Carlo Research}}},
  author = {Waller, Niels G},
  year = {2016},
  journal = {Multivariate behavioral research},
  volume = {51},
  number = {4},
  pages = {554--568},
  doi = {10/ggd4dw}
}

@article{waller2016a,
  title = {The Recaptured Scale Technique: {{A}} Method for Testing the Structural Robustness of Personality Scales},
  shorttitle = {The Recaptured Scale Technique},
  author = {Waller, Niels G. and DeYoung, Colin G. and Bouchard, Thomas J.},
  year = {2016},
  month = jul,
  journal = {Multivariate Behavioral Research},
  volume = {51},
  number = {4},
  pages = {433--445},
  issn = {0027-3171, 1532-7906},
  doi = {10/gfpf3d},
  langid = {english}
}

@article{waller2017,
  title = {Generating {{Correlation Matrices}} with {{Specified Eigenvalues Using}} the {{Method}} of {{Alternating Projections}}},
  author = {Waller, Niels G.},
  year = {2017},
  journal = {The American Statistician},
  volume = {0},
  number = {ja},
  pages = {0--0},
  doi = {10/gf3wt3}
}

@manual{waller2021,
  type = {Manual},
  title = {Fungible: {{Psychometric}} Functions from the {{Waller}} Lab.},
  author = {Waller, Niels G.},
  year = {2021}
}

@article{warton2008,
  title = {Penalized {{Normal Likelihood}} and {{Ridge Regularization}} of {{Correlation}} and {{Covariance Matrices}}},
  author = {Warton, David I},
  year = {2008},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {481},
  pages = {340--349},
  issn = {0162-1459, 1537-274X},
  doi = {10/dgd8fr},
  langid = {english},
  file = {/home/justin/Zotero/storage/L48JISFU/Warton - 2008 - Penalized Normal Likelihood and Ridge Regularizati.pdf}
}

@article{watkins2018exploratory,
  title = {Exploratory Factor Analysis: {{A}} Guide to Best Practice},
  author = {Watkins, Marley W},
  year = {2018},
  journal = {Journal of Black Psychology},
  volume = {44},
  number = {3},
  pages = {219--246},
  publisher = {{Sage Publications Sage CA: Los Angeles, CA}},
  doi = {10/gdk2zx},
  date-added = {2020-02-07 11:06:42 -0600},
  date-modified = {2020-02-07 11:06:42 -0600}
}

@article{watson1992,
  title = {Algorithms for Minimum Trace Factor Analysis},
  author = {Watson, GA},
  year = {1992},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {13},
  number = {4},
  pages = {1039--1053},
  doi = {10/dnspw9},
  keywords = {factor analysis,matrix smooth}
}

@article{weng2005,
  ids = {wengParallelAnalysisUnidimensional2005},
  title = {Parallel {{Analysis}} with {{Unidimensional Binary Data}}},
  author = {Weng, Li-Jen and Cheng, Chung-Ping},
  year = {2005},
  month = oct,
  journal = {Educational and Psychological Measurement},
  volume = {65},
  number = {5},
  pages = {697--716},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10/bmjfkk},
  abstract = {The present simulation investigated the performance of parallel analysis for unidimensional binary data. Single-factor models with 8 and 20 indicators were examined, and sample size (50, 100, 200, 500, and 1,000), factor loading (.45, .70, and .90), response ratio on two categories (50/50, 60/40, 70/30, 80/20, and 90/10), and types of correlation coefficients (phi and tetrachoric correlations) were manipulated. The results indicated that parallel analysis performed well in identifying the number of factors. The performance improved as factor loading and sample size increased and as the percentages of responses on two categories became close. Using the 95th and 99th percentiles of the random data eigenvalues as the criteria for comparison in parallel analysis yielded higher correct rate than using mean eigenvalues.},
  langid = {english},
  keywords = {correlation,ShapeImproperR,smooth matrix},
  file = {/home/justin/Zotero/storage/5XEQ5NBA/Weng and Cheng - 2005 - Parallel Analysis with Unidimensional Binary Data.pdf}
}

@article{white1982,
  title = {Maximum {{Likelihood Estimation}} of {{Misspecified Models}}},
  author = {White, Halbert},
  year = {1982},
  journal = {Econometrica},
  volume = {50},
  number = {1},
  pages = {1--25},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10/dcgdqk},
  abstract = {This paper examines the consequences and detection of model misspecification when using maximum likelihood techniques for estimation and inference. The quasi-maximum likelihood estimator (OMLE) converges to a well defined limit, and may or may not be consistent for particular parameters of interest. Standard tests (Wald, Lagrange Multiplier, or Likelihood Ratio) are invalid in the presence of misspecification, but more general statistics are given which allow inferences to be drawn robustly. The properties of the QMLE and the information matrix are exploited to yield several useful tests for model misspecification.},
  file = {/home/justin/Zotero/storage/WBDXBXQ3/White - 1982 - Maximum Likelihood Estimation of Misspecified Mode.pdf}
}

@article{wicherts2009,
  title = {The Absence of Underprediction Does Not Imply the Absence of Measurement Bias.},
  author = {Wicherts, Jelte M. and Millsap, Roger E.},
  year = {2009},
  journal = {American Psychologist},
  volume = {64},
  number = {4},
  pages = {281--283},
  issn = {1935-990X, 0003-066X},
  doi = {10/c5zjgf},
  langid = {english},
  file = {/home/justin/Zotero/storage/VRJ77B8I/Wicherts_Millsap_2009_The absence of underprediction does not imply the absence of measurement bias.pdf}
}

@book{wicklinComputingNearestCorrelation2012,
  title = {Computing the Nearest Correlation Matrix: {{SAS FAQ}}},
  author = {Wicklin, R},
  year = {2012},
  keywords = {matrix smooth}
}

@article{widaman1985,
  ids = {widaman1985iterative},
  title = {Iterative Least Squares Estimates of Communality: {{Initial}} Estimate Need Not Affect Stabilized Value},
  shorttitle = {Iterative Least Squares Estimates of Communality},
  author = {Widaman, Keith F. and Herringer, Lawrence G.},
  year = {1985},
  journal = {Psychometrika},
  volume = {50},
  number = {4},
  pages = {469--477},
  publisher = {{Springer}},
  issn = {1860-0980},
  doi = {10/b3b8x9},
  abstract = {A common criticism of iterative least squares estimates of communality is that method of initial estimation may influence stabilized values. As little systematic research on this topic has been performed, the criticism appears to be based on cumulated experience with empirical data sets. In the present paper, two studies are reported in which four types of initial estimate (unities, squared multiple correlations, highestr, and zeroes) and four levels of convergence criterion were employed using four widely available computer packages (BMDP, SAS, SPSS, and SOUPAC). The results suggest that initial estimates have no effect on stabilized communality estimates when a stringent criterion for convergence is used, whereas initial estimates appear to affect stabilized values employing rather gross convergence criteria. There were no differences among the four computer packages for matrices without Heywood cases.},
  date-added = {2020-01-08 11:38:00 -0600},
  date-modified = {2020-01-08 11:38:00 -0600},
  langid = {english},
  file = {/home/justin/Zotero/storage/L99GFTSJ/Widaman_Herringer_1985_Iterative least squares estimates of communality.pdf}
}

@article{widaman1993,
  title = {Common {{Factor Analysis Versus Principal Component Analysis}}: {{Differential Bias}} in {{Representing Model Parameters}}?},
  shorttitle = {Common {{Factor Analysis Versus Principal Component Analysis}}},
  author = {Widaman, Keith F.},
  year = {1993},
  month = jul,
  journal = {Multivariate Behavioral Research},
  volume = {28},
  number = {3},
  pages = {263--311},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/dfs5np},
  abstract = {The aim of the present article was to reconsider several conclusions by Velicer and Jackson (1990a) in their review of issues that arise when comparing common factor analysis and principal component analysis. Specifically, the three conclusions by Velicer and Jackson that are considered in the present article are: (a) that common factor and principal component solutions are similar, (b) that differences between common factor and principal component solutions appear only when too many dimensions are extracted, and (c) that common factor and principal component parameters are equally generalizable. In contrast, Snook and Gorsuch (1989) argued recently that principal component analysis and common factor analysis led to different, dissimilar estimates of pattern loadings, terming the principal component loadings biased and the common factor loadings unbiased. In the present article, after replicating the Snook and Gorsuch results, an extension demonstrated that the difference between common factor and principal component pattern loadings is inversely related to the number of indicators per factor, not to the total number of observed variables in the analysis, countering claims by both Snook and Gorsuch and Velicer and Jackson. Considering the more general case of oblique factors, one concomitant of overrepresentation of pattern loadings is an underrepresentation of intercorrelations among dimensions represented by principal component analysis, whereas comparable values obtained using factor analysis are accurate. Differences in parameters deriving from principal component analysis and common factor analysis were explored in relation to several additional aspects of population data, such as variation in the level of communality of variables on a given factor and the moving of a variable from one battery of measures to another. The results suggest that principal component analysis should not be used if a researcher wishes to obtain parameters reflecting latent constructs or factors.},
  pmid = {26776890},
  annotation = {\_eprint: https://doi.org/10.1207/s15327906mbr2803\_1},
  file = {/home/justin/Zotero/storage/YTETTLAD/Widaman_1993_Common Factor Analysis Versus Principal Component Analysis.pdf;/home/justin/Zotero/storage/LUBQ6AGN/s15327906mbr2803_1.html}
}

@incollection{widaman2012,
  title = {Exploratory Factor Analysis and Confirmatory Factor Analysis},
  booktitle = {{{APA}} Handbook of Research Methods in Psychology, {{Vol}} 3: {{Data}} Analysis and Research Publication},
  author = {Widaman, Keith F.},
  year = {2012},
  series = {{{APA}} Handbooks in Psychology\textregistered},
  pages = {361--389},
  publisher = {{American Psychological Association}},
  address = {{Washington, DC, US}},
  doi = {10.1037/13621-018},
  abstract = {Factor analysis has been of central importance in evaluating empirical evidence, adjudicating conflicting conjectures, and developing usable dimensional taxonomies for mental abilities and personality as well as in many other domains. Thus, factor analysis is of foundational importance to many aspects of the research process. The goal of this chapter is to provide a general introduction to both exploratory factor analysis and confirmatory factor analysis, noting the similarities and differences between the methods. In addition to providing an introduction to the factor analysis model and various options for conducting analyses, several empirical applications will illustrate the use of factor analysis. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  isbn = {978-1-4338-1006-0},
  keywords = {Confirmatory Factor Analysis,Exploratory Factor Analysis,Factor Analysis},
  file = {/home/justin/Zotero/storage/K3A3CB5I/Widaman_2012_Exploratory factor analysis and confirmatory factor analysis.pdf;/home/justin/Zotero/storage/S457TRSA/2011-23865-018.html}
}

@book{wikipediaDykstraProjectionAlgorithm2015,
  title = {Dykstra's Projection Algorithm \textemdash{} {{Wikipedia}}, {{The Free Encyclopedia}}},
  author = {{Wikipedia}},
  year = {2015}
}

@book{wikipediaTikhonovRegularizationWikipedia2017,
  title = {Tikhonov Regularization \textemdash{} {{Wikipedia}}, {{The Free Encyclopedia}}},
  author = {{Wikipedia}},
  year = {2017}
}

@misc{wilson1991,
  title = {{{TESTFACT}}: {{Test}} Scoring, Item Statistics, and Item Factor Analysis},
  author = {Wilson, D. and Wood, R. and Gibbons, R.},
  year = {1991},
  address = {{Mooresville, IN}},
  howpublished = {Scientific Software, Inc}
}

@article{wilson2014,
  title = {Best {{Practices}} for {{Scientific Computing}}},
  author = {Wilson, Greg and Aruliah, D. A. and Brown, C. Titus and Chue Hong, Neil P. and Davis, Matt and Guy, Richard T. and Haddock, Steven H. D. and Huff, Kathryn D. and Mitchell, Ian M. and Plumbley, Mark D. and Waugh, Ben and White, Ethan P. and Wilson, Paul},
  editor = {Eisen, Jonathan A.},
  year = {2014},
  month = jan,
  journal = {PLoS Biology},
  volume = {12},
  number = {1},
  pages = {e1001745},
  issn = {1545-7885},
  doi = {10/qtt},
  langid = {english},
  file = {/home/justin/Zotero/storage/TX53GPR7/Wilson et al. - 2014 - Best Practices for Scientific Computing.pdf}
}

@article{wilson2017,
  title = {Good Enough Practices in Scientific Computing},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  editor = {Ouellette, Francis},
  year = {2017},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {6},
  pages = {e1005510},
  issn = {1553-7358},
  doi = {10/gbkbwp},
  langid = {english},
  file = {/home/justin/Zotero/storage/C2DFJH59/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf}
}

@article{wirth2007,
  ids = {wirth2007item,wirthItemFactorAnalysis2007},
  title = {Item Factor Analysis: {{Current}} Approaches and Future Directions.},
  shorttitle = {Item Factor Analysis},
  author = {Wirth, R. J. and Edwards, Michael C.},
  year = {2007},
  journal = {Psychological Methods},
  volume = {12},
  number = {1},
  pages = {58--79},
  publisher = {{American Psychological Association}},
  issn = {1939-1463, 1082-989X},
  doi = {10/bt2f3g},
  date-added = {2019-11-11 14:35:04 -0600},
  date-modified = {2019-11-11 14:35:04 -0600},
  langid = {english},
  file = {/home/justin/Zotero/storage/2Z844VKN/Wirth and Edwards - 2007 - Item factor analysis Current approaches and futur.pdf;/home/justin/Zotero/storage/VG2NZBEW/Wirth and Edwards - 2007 - Item factor analysis Current approaches and futur.pdf}
}

@article{wise1983,
  title = {Comparisons of Order Analysis and Factor Analysis in Assessing the Dimensionality of Binary Data},
  author = {Wise, Steven L.},
  year = {1983},
  month = jun,
  journal = {Applied Psychological Measurement},
  volume = {7},
  number = {3},
  pages = {311--321},
  publisher = {{SAGE Publications Inc}},
  issn = {0146-6216},
  doi = {10/frh6pw},
  abstract = {Previous research has not shown a clear relationship between order analytic and factor analytic approaches to assessing the dimensionality of binary data. This study compared factor analysis with three order analy sis procedures. Comparisons were based on eight data sets with known dimensionality and two multidimen sional sets of mathematics data. Two of the order analysis procedures fared poorly in reproducing the factor structure of the datasets. The third procedure re produced the factors for datasets with orthogonal fac tors but failed to reproduce the factors for datasets containing oblique factors. Reasons for the differences between these procedures are discussed.},
  langid = {english},
  file = {/home/justin/Zotero/storage/KDJLKIF6/Wise_1983_Comparisons of Order Analysis and Factor Analysis in Assessing the.pdf}
}

@article{wollan1987,
  title = {Algorithm {{AS}} 225: {{Minimizing}} Linear Inequality Constrained {{Mahalanobis}} Distances},
  author = {Wollan, Peter C and Dykstra, Richard L},
  year = {1987},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {36},
  number = {2},
  pages = {234--240},
  keywords = {⛔ No DOI found}
}

@incollection{wothke1993,
  ids = {wothkeTestingStructuralEquation1993},
  title = {Testing Structural Equation Models},
  booktitle = {Testing Structural Equation Models},
  author = {Wothke, W.},
  editor = {Bollen, K.A. and Long, J. Scott},
  year = {1993},
  volume = {154},
  pages = {256--293},
  publisher = {{A Sage Focus Edition}},
  chapter = {Nonpositive definite matrices in structural modeling},
  date-added = {2018-11-27 16:37:46 -0600},
  date-modified = {2018-11-27 16:37:54 -0600},
  isbn = {978-0-8039-4507-4},
  keywords = {matrix book,matrix smooth,tetrachoric}
}

@article{wu2010,
  title = {Sensitivity of Fit Indices to Misspecification in Growth Curve Models},
  author = {Wu, Wei and West, Stephen G.},
  year = {2010},
  month = may,
  journal = {Multivariate Behavioral Research},
  volume = {45},
  number = {3},
  pages = {420--452},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/ddp9hv},
  abstract = {This study investigated the sensitivity of fit indices to model misspecification in within-individual covariance structure, between-individual covariance structure, and marginal mean structure in growth curve models. Five commonly used fit indices were examined, including the likelihood ratio test statistic, root mean square error of approximation, standardized root mean square residual, comparative fit index, and Tucker-Lewis Index. The fit indices were found to have differential sensitivity to different types of misspecification in either the mean or covariance structures with severity of misspecification controlled. No fit index was always more (or less) sensitive to misspecification in the marginal mean structure relative to those in the covariance structure. Specifying the covariance structure to be saturated can substantially improve the sensitivity of fit indices to misspecification in the marginal mean structure; this result might help identify the sources of specification error in a growth curve model. An empirical example of children's growth in math achievement (Wu, West, \& Hughes, 2008) was used to illustrate the results.},
  pmid = {26760488},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2010.483378},
  file = {/home/justin/Zotero/storage/XA6I4L6J/Wu_West_2010_Sensitivity of Fit Indices to Misspecification in Growth Curve Models.pdf;/home/justin/Zotero/storage/U5T56F2W/00273171.2010.html}
}

@phdthesis{wu2010a,
  title = {An Empirical {{Bayesian}} Approach to Misspecified Covariance Structures},
  author = {Wu, Hao},
  year = {2010},
  address = {{Columbus, OH}},
  langid = {english},
  school = {The Ohio State University},
  file = {/home/justin/Zotero/storage/FZANI6JR/Wu_2010_An Empirical Bayesian Approach to Misspecified Covariance Structures.pdf;/home/justin/Zotero/storage/SI787Q45/10.html}
}

@article{wu2015,
  title = {Quantifying Adventitious Error in a Covariance Structure as a Random Effect},
  author = {Wu, Hao and Browne, Michael W.},
  year = {2015},
  month = sep,
  journal = {Psychometrika},
  volume = {80},
  number = {3},
  pages = {571--600},
  issn = {1860-0980},
  doi = {10/gjrkc4},
  abstract = {We present an approach to quantifying errors in covariance structures in which adventitious error, identified as the process underlying the discrepancy between the population and the structured model, is explicitly modeled as a random effect with a distribution, and the dispersion parameter of this distribution to be estimated gives a measure of misspecification. Analytical properties of the resultant procedure are investigated and the measure of misspecification is found to be related to the root mean square error of approximation. An algorithm is developed for numerical implementation of the procedure. The consistency and asymptotic sampling distributions of the estimators are established under a new asymptotic paradigm and an assumption weaker than the standard Pitman drift assumption. Simulations validate the asymptotic sampling distributions and demonstrate the importance of accounting for the variations in the parameter estimates due to adventitious error. Two examples are also given as illustrations.},
  langid = {english},
  file = {/home/justin/Zotero/storage/KIMTM962/Wu_Browne_2015_Quantifying adventitious error in a covariance structure as a random effect.pdf}
}

@article{wu2015a,
  title = {Random Model Discrepancy: {{Interpretations}} and Technicalities (a Rejoinder)},
  shorttitle = {Random Model Discrepancy},
  author = {Wu, Hao and Browne, Michael W.},
  year = {2015},
  month = sep,
  journal = {Psychometrika},
  volume = {80},
  number = {3},
  pages = {619--624},
  issn = {1860-0980},
  doi = {10/ggfngm},
  abstract = {In this rejoinder we discuss the following aspects of our approach to model discrepancy: the interpretations of the two populations and adventitious error, the choice of inverse Wishart distribution, the perceived danger of justifying a model with bad fit, the relationship among our new approach, Chen's (J R Stat Soc Ser B, 41:235\textendash 248, 1979) approach and the existing RMSEA-based approach, and the Pitman drift assumption.},
  langid = {english},
  file = {/home/justin/Zotero/storage/TVVA4WV8/Wu and Browne - 2015 - Random Model Discrepancy Interpretations and Tech.pdf}
}

@article{wyse2016,
  title = {Does {{Maximizing Information}} at the {{Cut Score Always Maximize Classification Accuracy}} and {{Consistency}}?},
  author = {Wyse, Adam E. and Babcock, Ben},
  year = {2016},
  journal = {Journal of Educational Measurement},
  volume = {53},
  number = {1},
  pages = {23--44},
  issn = {1745-3984},
  doi = {10/gcphkd},
  abstract = {A common suggestion made in the psychometric literature for fixed-length classification tests is that one should design tests so that they have maximum information at the cut score. Designing tests in this way is believed to maximize the classification accuracy and consistency of the assessment. This article uses simulated examples to illustrate that one can obtain higher classification accuracy and consistency by designing tests that have maximum test information at locations other than at the cut score. We show that the location where one should maximize the test information is dependent on the length of the test, the mean of the ability distribution in comparison to the cut score, and, to a lesser degree, whether or not one wants to optimize classification accuracy or consistency. Analyses also suggested that the differences in classification performance between designing tests optimally versus maximizing information at the cut score tended to be greatest when tests were short and the mean of ability distribution was further away from the cut score. Larger differences were also found in the simulated examples that used the 3PL model compared to the examples that used the Rasch model.},
  copyright = {Copyright \textcopyright{} 2016 by the National Council on Measurement in Education},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jedm.12099},
  file = {/home/justin/Zotero/storage/9MF5SEPE/Wyse and Babcock - 2016 - Does Maximizing Information at the Cut Score Alway.pdf;/home/justin/Zotero/storage/D459TBGF/jedm.html}
}

@article{wysocki2019,
  title = {On {{Penalty Parameter Selection}} for {{Estimating Network Models}}},
  author = {Wysocki, Anna C. and Rhemtulla, Mijke},
  year = {2019},
  month = nov,
  journal = {Multivariate Behavioral Research},
  volume = {0},
  number = {0},
  pages = {1--15},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10/gjrkdc},
  abstract = {Network models are gaining popularity as a way to estimate direct effects among psychological variables and investigate the structure of constructs. A key feature of network estimation is determining which edges are likely to be non-zero. In psychology, this is commonly achieved through the graphical lasso regularization method that estimates a precision matrix of Gaussian variables using an {$\mathscr{l}$}1-penalty to push small values to zero. A tuning parameter, {$\lambda$}, controls the sparsity of the network. There are many methods to select {$\lambda$}, which can lead to vastly different graphs. The most common approach in psychological network applications is to minimize the extended Bayesian information criterion, but the consistency of this method for model selection has primarily been examined in high dimensional settings (i.e., n {$<$} p) that are uncommon in psychology. Further, there is some evidence that alternative selection methods may have superior performance. Here, using simulation, we compare four different methods for selecting {$\lambda$}, including the stability approach to regularization selection (StARS), K-fold cross-validation, the rotation information criterion (RIC), and the extended Bayesian information criterion (EBIC). Our results demonstrate that penalty parameter selection should be made based on data characteristics and the inferential goal (e.g., to increase sensitivity versus to avoid false positives). We end with recommendations for selecting the penalty parameter when using the graphical lasso.},
  pmid = {31672065},
  keywords = {Network analysis,partial correlation networks,penalty selection,regularization,simulation study},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2019.1672516},
  file = {/home/justin/Zotero/storage/WMAZXQY6/00273171.2019.html}
}

@phdthesis{xia2016,
  title = {Investigating the Chi-Square-Based Model-Fit Indexes for {{WLSMV}} and {{ULSMV}} Estimators},
  author = {Xia, Yan},
  year = {2016},
  langid = {english},
  file = {/home/justin/Zotero/storage/VR4L5HAX/fsu366138.html}
}

@article{xia2016a,
  title = {Evaluating the Selection of Normal-Theory Weight Matrices in the {{Satorra}}\textendash{{Bentler}} Correction of Chi-Square and Standard Errors},
  author = {Xia, Yan and Yung, Yiu-Fai and Zhang, Wei},
  year = {2016},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {23},
  number = {4},
  pages = {585--594},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/gcz6zq},
  abstract = {In the application of the Satorra\textendash Bentler scaling correction, the choices of normal-theory weight matrices (i.e., the model-predicted vs. the sample covariance matrix) in the calculation of the correction remains unclear. Different software programs use different matrices by default. This simulation study investigates the discrepancies due to the weight matrices in the robust chi-square statistics, standard errors, and chi-square-based model fit indexes. This study varies the sample sizes at 100, 200, 500, and 1,000; kurtoses at 0, 7, and 21; and degrees of model misspecification, measured by the population root mean square error of approximation (RMSEA), at 0, .03, .05, .08, .10, and .15. The results favor the use of the model-predicted covariance matrix because it results in less false rejection rates under the correctly specified model, as well as more accurate standard errors across all conditions. For the sample-corrected robust RMSEA, comparative fit index (CFI) and Tucker\textendash Lewis index (TLI), 2 matrices result in negligible differences.},
  keywords = {model fit,nonnormality,Satorra–Bentler robust correction,standard error},
  annotation = {\_eprint: https://doi.org/10.1080/10705511.2016.1141354},
  file = {/home/justin/Zotero/storage/FD4QE7MW/Xia et al_2016_Evaluating the Selection of Normal-Theory Weight Matrices in the.pdf}
}

@article{xia2019,
  title = {{{RMSEA}}, {{CFI}}, and {{TLI}} in Structural Equation Modeling with Ordered Categorical Data: {{The}} Story They Tell Depends on the Estimation Methods},
  shorttitle = {{{RMSEA}}, {{CFI}}, and {{TLI}} in Structural Equation Modeling with Ordered Categorical Data},
  author = {Xia, Yan and Yang, Yanyun},
  year = {2019},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {1},
  pages = {409--428},
  issn = {1554-3528},
  doi = {10/ggssdx},
  abstract = {In structural equation modeling, application of the root mean square error of approximation (RMSEA), comparative fit index (CFI), and Tucker\textendash Lewis index (TLI) highly relies on the conventional cutoff values developed under normal-theory maximum likelihood (ML) with continuous data. For ordered categorical data, unweighted least squares (ULS) and diagonally weighted least squares (DWLS) based on polychoric correlation matrices have been recommended in previous studies. Although no clear suggestions exist regarding the application of these fit indices when analyzing ordered categorical variables, practitioners are still tempted to adopt the conventional cutoff rules. The purpose of our research was to answer the question: Given a population polychoric correlation matrix and a hypothesized model, if ML results in a specific RMSEA value (e.g., .08), what is the RMSEA value when ULS or DWLS is applied? CFI and TLI were investigated in the same fashion. Both simulated and empirical polychoric correlation matrices with various degrees of model misspecification were employed to address the above question. The results showed that DWLS and ULS lead to smaller RMSEA and larger CFI and TLI values than does ML for all manipulated conditions, regardless of whether or not the indices are scaled. Applying the conventional cutoffs to DWLS and ULS, therefore, has a pronounced tendency not to discover model\textendash data misfit. Discussions regarding the use of RMSEA, CFI, and TLI for ordered categorical data are given.},
  langid = {english},
  file = {/home/justin/Zotero/storage/TM38ZVXJ/Xia_Yang_2019_RMSEA, CFI, and TLI in structural equation modeling with ordered categorical.pdf}
}

@article{xia2021,
  title = {Determining the Number of Factors When Population Models Can Be Closely Approximated by Parsimonious Models},
  author = {Xia, Yan},
  year = {2021},
  month = feb,
  journal = {Educational and Psychological Measurement},
  pages = {0013164421992836},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10/gh68s2},
  abstract = {Despite the existence of many methods for determining the number of factors, none outperforms the others under every condition. This study compares traditional parallel analysis (TPA), revised parallel analysis (RPA), Kaiser's rule, minimum average partial, sequential {$\chi$}2, and sequential root mean square error of approximation, comparative fit index, and Tucker\textendash Lewis index under a realistic scenario in behavioral studies, where researchers employ a closing\textendash fitting parsimonious model with K factors to approximate a population model, leading to a trivial model-data misfit. Results show that while traditional and RPA both stand out when zero population-level misfits exist, the accuracy of RPA substantially deteriorates when a K-factor model can closely approximate the population. TPA is the least sensitive to trivial misfits and results in the highest accuracy across most simulation conditions. This study suggests the use of TPA for the investigated models. Results also imply that RPA requires further revision to accommodate a degree of model\textendash data misfit that can be tolerated.},
  langid = {english},
  keywords = {dimensionality assessment,factor analysis,model–data fit,parallel analysis},
  file = {/home/justin/Zotero/storage/K89B2LEL/Xia_2021_Determining the Number of Factors When Population Models Can Be Closely.pdf}
}

@article{xia2021a,
  title = {Determining the {{Number}} of {{Factors When Population Models Can Be Closely Approximated}} by {{Parsimonious Models}} - {{Yan Xia}}, 2021},
  author = {Xia, Yan},
  year = {2021},
  month = feb,
  journal = {Educational and Psychological Measurement},
  abstract = {Despite the existence of many methods for determining the number of factors, none outperforms the others under every condition. This study compares traditional ...},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/justin/Zotero/storage/GPVFWST4/0013164421992836.html}
}

@article{xiang2019,
  title = {On the {{Legal Compatibility}} of {{Fairness Definitions}}},
  author = {Xiang, Alice and Raji, Inioluwa Deborah},
  year = {2019},
  month = nov,
  journal = {arXiv:1912.00761 [cs, stat]},
  eprint = {1912.00761},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Past literature has been effective in demonstrating ideological gaps in machine learning (ML) fairness definitions when considering their use in complex socio-technical systems. However, we go further to demonstrate that these definitions often misunderstand the legal concepts from which they purport to be inspired, and consequently inappropriately co-opt legal language. In this paper, we demonstrate examples of this misalignment and discuss the differences in ML terminology and their legal counterparts, as well as what both the legal and ML fairness communities can learn from these tensions. We focus this paper on U.S. anti-discrimination law since the ML fairness research community regularly references terms from this body of law.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/justin/Zotero/storage/897SYMTW/Xiang and Raji - 2019 - On the Legal Compatibility of Fairness Definitions.pdf;/home/justin/Zotero/storage/G2YUH854/1912.html}
}

@book{xie2016,
  title = {Bookdown: {{Authoring}} Books and Technical Documents with {{R}} Markdown},
  author = {Xie, Yihui},
  year = {2016},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@article{ximenez2006,
  title = {A {{Monte Carlo Study}} of {{Recovery}} of {{Weak Factor Loadings}} in {{Confirmatory Factor Analysis}}},
  author = {Xim{\'e}nez, Carmen},
  year = {2006},
  month = dec,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {13},
  number = {4},
  pages = {587--614},
  issn = {1070-5511, 1532-8007},
  doi = {10/d5mzgn},
  langid = {english},
  file = {/home/justin/Zotero/storage/W3NEJQLY/Ximénez - 2006 - A Monte Carlo Study of Recovery of Weak Factor Loa.pdf}
}

@article{ximenez2009,
  ids = {ximenezRecoveryWeakFactor2009a},
  title = {Recovery of Weak Factor Loadings in Confirmatory Factor Analysis under Conditions of Model Misspecification},
  author = {Xim{\'e}nez, Carmen},
  year = {2009},
  month = nov,
  journal = {Behavior Research Methods},
  volume = {41},
  number = {4},
  pages = {1038--1052},
  issn = {1554-351X, 1554-3528},
  doi = {10/bc3x7x},
  langid = {english},
  file = {/home/justin/Zotero/storage/UYRDVJI3/Ximénez - 2009 - Recovery of weak factor loadings in confirmatory f.pdf}
}

@article{ximenez2016,
  title = {Recovery of {{Weak Factor Loadings When Adding}} the {{Mean Structure}} in {{Confirmatory Factor Analysis}}: {{A Simulation Study}}},
  shorttitle = {Recovery of {{Weak Factor Loadings When Adding}} the {{Mean Structure}} in {{Confirmatory Factor Analysis}}},
  author = {Xim{\'e}nez, Carmen},
  year = {2016},
  month = jan,
  journal = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  doi = {10/gccx5x},
  file = {/home/justin/Zotero/storage/FJYXMVKI/Ximénez - 2016 - Recovery of Weak Factor Loadings When Adding the M.pdf}
}

@article{xu2019,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  year = {2019},
  journal = {arXiv:1810.00826 [cs, stat]},
  eprint = {1810.00826},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the WeisfeilerLehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/justin/Zotero/storage/XK7D6TXM/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf}
}

@article{yang-wallentin2010,
  title = {Confirmatory Factor Analysis of Ordinal Variables with Misspecified Models},
  author = {{Yang-Wallentin}, Fan and J{\"o}reskog, Karl G. and Luo, Hao},
  year = {2010},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {17},
  number = {3},
  pages = {392--423},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10/d6rzz6},
  abstract = {Ordinal variables are common in many empirical investigations in the social and behavioral sciences. Researchers often apply the maximum likelihood method to fit structural equation models to ordinal data. This assumes that the observed measures have normal distributions, which is not the case when the variables are ordinal. A better approach is to use polychoric correlations and fit the models using methods such as unweighted least squares (ULS), maximum likelihood (ML), weighted least squares (WLS), or diagonally weighted least squares (DWLS). In this simulation evaluation we study the behavior of these methods in combination with polychoric correlations when the models are misspecified. We also study the effect of model size and number of categories on the parameter estimates, their standard errors, and the common chi-square measures of fit when the models are both correct and misspecified. When used routinely, these methods give consistent parameter estimates but ULS, ML, and DWLS give incorrect standard errors. Correct standard errors can be obtained for these methods by robustification using an estimate of the asymptotic covariance matrix W of the polychoric correlations. When used in this way the methods are here called RULS, RML, and RDWLS.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/10705511.2010.489003},
  file = {/home/justin/Zotero/storage/I3JID8YQ/Yang-Wallentin et al_2010_Confirmatory Factor Analysis of Ordinal Variables With Misspecified Models.pdf;/home/justin/Zotero/storage/GTA994C8/10705511.2010.html}
}

@inproceedings{ye2019,
  title = {Identifying {{High Potential Talent}}: {{A Neural Network Based Dynamic Social Profiling Approach}}},
  shorttitle = {Identifying {{High Potential Talent}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Ye, Yuyang and Zhu, Hengshu and Xu, Tong and Zhuang, Fuzhen and Yu, Runlong and Xiong, Hui},
  year = {2019},
  pages = {718--727},
  issn = {2374-8486},
  doi = {10/gj733g},
  abstract = {How to identify high-potential talent (HIPO) earlier in their career always has strategic importance for human resource management. While tremendous efforts have been made in this direction, most existing approaches are still based on the subjective selection of human resource experts. This could lead to unintentional bias and inconsistencies. To this end, in this paper, we propose a neural network based dynamic social profiling approach for quantitatively identifying HIPOs from the newly-enrolled employees by modeling the dynamics of their behaviors in organizational social networks. A basic assumption is that HIPOs usually perform more actively and have higher competencies than their peers to accumulate their social capitals during their daily work practice. Along this line, we first propose to model the social profiles of employees with both Graph Convolutional Network (GCN) and social centrality analysis in a comprehensive way. Then, an adaptive Long Short Term Memory (LSTM) network with global attention mechanism is designed to capture the profile dynamics of employees in the organizational social networks during their early career. Finally, extensive experiments on real-world data clearly validate the effectiveness of our approach as well as the interpretability of our results.},
  keywords = {Artificial neural networks,Companies,Engineering profession,Germanium,Human Resource Management; HIPO identification; Social Profiling,Social networking (online)},
  file = {/home/justin/Zotero/storage/3BSBCGEQ/Ye et al_2019_Identifying High Potential Talent.pdf}
}

@inproceedings{ye2019a,
  title = {Identifying {{High Potential Talent}}: {{A Neural Network Based Dynamic Social Profiling Approach}}},
  shorttitle = {Identifying {{High Potential Talent}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Ye, Yuyang and Zhu, Hengshu and Xu, Tong and Zhuang, Fuzhen and Yu, Runlong and Xiong, Hui},
  year = {2019},
  month = nov,
  pages = {718--727},
  issn = {2374-8486},
  doi = {10/gj733g},
  abstract = {How to identify high-potential talent (HIPO) earlier in their career always has strategic importance for human resource management. While tremendous efforts have been made in this direction, most existing approaches are still based on the subjective selection of human resource experts. This could lead to unintentional bias and inconsistencies. To this end, in this paper, we propose a neural network based dynamic social profiling approach for quantitatively identifying HIPOs from the newly-enrolled employees by modeling the dynamics of their behaviors in organizational social networks. A basic assumption is that HIPOs usually perform more actively and have higher competencies than their peers to accumulate their social capitals during their daily work practice. Along this line, we first propose to model the social profiles of employees with both Graph Convolutional Network (GCN) and social centrality analysis in a comprehensive way. Then, an adaptive Long Short Term Memory (LSTM) network with global attention mechanism is designed to capture the profile dynamics of employees in the organizational social networks during their early career. Finally, extensive experiments on real-world data clearly validate the effectiveness of our approach as well as the interpretability of our results.},
  keywords = {Artificial neural networks,Companies,Engineering profession,Germanium,Human Resource Management; HIPO identification; Social Profiling,Social networking (online)},
  file = {/home/justin/Zotero/storage/D8MR8P8L/Ye et al_2019_Identifying High Potential Talent.pdf;/home/justin/Zotero/storage/8BJL2398/8970676.html}
}

@book{young2001,
  title = {Differential Validity, Differential Prediction, and College Admission Testing: {{A}} Comprehensive Review and Analysis. {{Research Report No}}. 2001-6},
  shorttitle = {Differential Validity, Differential Prediction, and College Admission Testing},
  author = {Young, John W.},
  year = {2001},
  journal = {College Entrance Examination Board},
  publisher = {{College Entrance Examination Board}},
  abstract = {This research report is a review and analysis of all of the published studies during the past 25+ years (since 1974) in the area of differential validity/prediction and college admission testing. More specifically, this report includes 49 separate studies of differences in validity and/or prediction for different racial/ethnic groups and/or for men and women. All of the studies that were reviewed originated as journal articles, book chapters, conference papers, or research/technical reports. The breadth of studies range from single-institution studies based on a single cohort of several hundred students to large-scale compilations of results across hundreds of institutions that included several thousand students in all. The typical research design in these studies used first-year grade point average (FGPA) as the criterion and test scores (usually SAT\textregistered{} scores) and high school grades as predictor variables in a multiple regression analysis. Correlation coefficients were also usually reported as evidence of predictive validity. The main contribution of this report is contained in sections 3 and 4 with a focus on racial/ethnic differences and on sex differences, respectively. With regard to racial/ethnic differences, the minority groups that have been studied include Asian Americans, blacks/African Americans, Hispanics, and Native Americans. Some studies used a combined sample of minority students that was usually composed primarily of African American and Hispanic students. Overall, there was no common pattern to the results for validity and prediction for the different minority groups. Correlations between predictors and criterion were different for each minority group with generally lower values (for both blacks/African Americans and Hispanics) or similar values (for Asian Americans) when compared to whites. Too few studies of Native Americans or of combined samples of minority students are available to reliably determine typical validity coefficients for these groups. In terms of grade prediction, the common finding was one of over-prediction of college grades for all of the minority groups (except for Asian Americans), although the magnitude differed for each group. With Asian American students, studies that employed grade adjustment methods found that under-prediction of grades occurred. With respect to sex differences, the correlations between predictors and criterion were generally higher for women than for men. In terms of prediction, the typical finding in these studies was that women's college grades were under-predicted. However, in the most selective universities, the correlations for men and women appear to be equal, while the degree of under-prediction for women's grades appears to be somewhat less than in other institutions. Compared to earlier research on this topic, sex differences in validity and prediction appear to have persisted, although the magnitude of the differences seems to have lessened. The concluding section of the report provides a summary of the results, states several conclusions that can be drawn from the research reviewed, and postulates a number of different avenues for further research on differential validity/prediction that could yield useful additional information on this important and timely topic. Descriptions of Studies Cited in Sections 3 and 4 are appended. [This report was written with the assistance of Jennifer L. Kobrin. A list of Differential Validity/Prediction Studies Cited in Sections 3 and 4 is included.]},
  langid = {english},
  keywords = {African American Students,American Indian Students,Asian American Students,College Entrance Examinations,College Students,Correlation,Educational Research,Evaluation Research,Gender Differences,Grade Point Average,Grades (Scholastic),High School Students,Hispanic American Students,Literature Reviews,Minority Group Students,Multiple Regression Analysis,Prediction,Predictive Validity,Predictor Variables,Racial Differences,Research Reports,Scores,Test Validity},
  file = {/home/justin/Zotero/storage/9DI6B5S9/Young - 2001 - Differential Validity, Differential Prediction, an.pdf;/home/justin/Zotero/storage/DNCZAP2G/eric.ed.gov.html}
}

@article{younis2021,
  title = {Know {{Your Stars Before They Fall Apart}}: {{A Social Network Analysis}} of {{Telecom Industry}} to {{Foster Employee Retention Using Data Mining Technique}}},
  shorttitle = {Know {{Your Stars Before They Fall Apart}}},
  author = {Younis, Sundus and Ahsan, Ali},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {16467--16487},
  issn = {2169-3536},
  doi = {10/gj6knd},
  abstract = {Social network analysis (SNA) has emerged as a significant paradigm for research in data mining community for measuring and analyzing human dynamic network structure. At organizational level, SNA can enhance our understanding of work place social interactions and unveil the hidden stars embedded in informal networks by investigating nodes and edges of complex networks. For this study, we aim to formulate a network centrality based quantitative method to identify the High potential employees (HiPos) and Influencers of telecom sector and explore the relationship between degree centrality of these star employees and their turnover intention by modeling the dynamics of their workplace social ties and predictive data mining technique. We investigated the multiplex work and advice network in two leading telecom operators of Pakistan i.e. Ufone and Zong. For the statistical analysis we conducted a quantitative and visual network analysis in UCINET along with correlation and regression. Our results showed a negative correlation between HiPos out-degree centrality and turnover intention and a positive correlation between influencer's in-degree centrality and turnover. Whereas perceived investment in employee development (PIED) was found to mediate the relationship between in-degree centrality of influencer and turnover intention. The correlation results were then verified in regression model. These findings will guide the telecom operators in designing an optimal structure for business intelligence by providing critical insights of their star employees and help them to investigate the influence of central nodes on dynamical processes of its heterogeneous networks and thus enhance employee retention before a star falls out.},
  keywords = {data mining,Data mining,employee retention,Employment,HiPo,in-degree centrality,Industries,influencer,Multiplexing,Organizations,out-degree centrality,Social network analysis,Social networking (online),Telecommunications},
  file = {/home/justin/Zotero/storage/33V5TERQ/Younis_Ahsan_2021_Know Your Stars Before They Fall Apart.pdf;/home/justin/Zotero/storage/UIT7IBD9/9317820.html}
}

@article{yuan,
  title = {A Class of Population Covariance Matrices for {{Monte Carlo}} Simulation},
  author = {Yuan, Ke-Hai and Hayashi, Kentaro and Yanagihara, Hirokazu},
  pages = {23},
  abstract = {Model evaluation in covariance structure analysis is critical before the results can be trusted. Due to finite sample sizes and unknown distributions of practical data, existing conclusion regarding a particular statistic may not be applicable in practice. The bootstrap procedure automatically takes care of the unknown distribution and, for a given sample size, also provides more accurate results than those based on standard asymptotics. But it needs a matrix to play the role of the population covariance matrix. The closer the matrix is to the true population covariance matrix, the more valid the bootstrap inference is. The current paper proposes a class of covariance matrices by combining theory and data. Thus, a proper matrix from this class is closer to the true population covariance matrix than those constructed by any existing methods. Each of the covariance matrices is easy to generate and also satisfies several desired properties. Examples verify the properties of the matrices and illustrate the details for creating a matrix with a given amount of misspecification.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/justin/Zotero/storage/R3KPZK6K/Yuan et al. - A Class of Population Covariance Matrices for Mont.pdf}
}

@article{yuan2003,
  title = {Bootstrap Approach to Inference and Power Analysis Based on Three Test Statistics for Covariance Structure Models},
  author = {Yuan, Ke-Hai and Hayashi, Kentaro},
  year = {2003},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {56},
  number = {1},
  pages = {93--110},
  issn = {2044-8317},
  doi = {10/cd7dgj},
  abstract = {We study several aspects of bootstrap inference for covariance structure models based on three test statistics, including Type I error, power and sample-size determination. Specifically, we discuss conditions for a test statistic to achieve a more accurate level of Type I error, both in theory and in practice. Details on power analysis and sample-size determination are given. For data sets with heavy tails, we propose applying a bootstrap methodology to a transformed sample by a downweighting procedure. One of the key conditions for safe bootstrap inference is generally satisfied by the transformed sample but may not be satisfied by the original sample with heavy tails. Several data sets illustrate that, by combining downweighting and bootstrapping, a researcher may find a nearly optimal procedure for evaluating various aspects of covariance structure models. A rule for handling non-convergence problems in bootstrap replications is proposed.},
  copyright = {2003 The British Psychological Society},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1348/000711003321645368},
  file = {/home/justin/Zotero/storage/LWB6S74A/Yuan_Hayashi_2003_Bootstrap approach to inference and power analysis based on three test.pdf;/home/justin/Zotero/storage/DMIL3PSN/000711003321645368.html}
}

@article{yuan2004,
  title = {A New Measure of Misfit for Covariance Structure Models},
  author = {Yuan, Ke-Hai and Marshall, Linda L.},
  year = {2004},
  month = jan,
  journal = {Behaviormetrika},
  volume = {31},
  number = {1},
  pages = {67--90},
  issn = {1349-6964},
  doi = {10/cr2mb8},
  abstract = {Various fit indices exist in structural equation models. Most of these indices are related to the noncentrality parameter (NCP) of the chi-square distribution that the involved test statistic is implicitly assumed to follow. Existing literature suggests that few statistics can be well approximated by chi-square distributions. The meaning of the NCP is not clear when the behavior of the statistic cannot be described by a chi-square distribution. In this paper we define a new measure of model misfit (MMM) as the difference between the expected values of a statistic under the alternative and null hypotheses. This definition does not need to assume that the population covariance matrix is in the vicinity of the proposed model, nor does it need for the test statistic to follow any distribution of a known form. The MMM does not necessarily equal the discrepancy between the model and the population covariance matrix as has been assumed in existing literature. Bootstrap approaches to estimating the MMM and a related quantity are developed. An algorithm for obtaining bootstrap confidence intervals of the MMM is constructed. Examples with practical data sets contrast several measures of model misfit. The quantile-quantile plot is used to illustrate the unrealistic nature of chi-square distribution assumptions under either the null or an alternative hypothesis in practice.},
  langid = {english},
  file = {/home/justin/Zotero/storage/3SKI67HQ/Yuan_Marshall_2004_A New Measure of Misfit for Covariance Structure Models.pdf}
}

@article{yuan2006,
  title = {Standard Errors in Covariance Structure Models: {{Asymptotics}} versus Bootstrap},
  shorttitle = {Standard Errors in Covariance Structure Models},
  author = {Yuan, Ke-Hai and Hayashi, Kentaro},
  year = {2006},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {59},
  number = {2},
  pages = {397--417},
  issn = {2044-8317},
  doi = {10/bgmtmb},
  abstract = {Commonly used formulae for standard error (SE) estimates in covariance structure analysis are derived under the assumption of a correctly specified model. In practice, a model is at best only an approximation to the real world. It is important to know whether the estimates of SEs as provided by standard software are consistent when a model is misspecified, and to understand why if not. Bootstrap procedures provide nonparametric estimates of SEs that automatically account for distribution violation. It is also necessary to know whether bootstrap estimates of SEs are consistent. This paper studies the relationship between the bootstrap estimates of SEs and those based on asymptotics. Examples are used to illustrate various versions of asymptotic variance\textendash covariance matrices and their validity. Conditions for the consistency of the bootstrap estimates of SEs are identified and discussed. Numerical examples are provided to illustrate the relationship of different estimates of SEs and covariance matrices.},
  copyright = {2006 The British Psychological Society},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1348/000711005X85896},
  file = {/home/justin/Zotero/storage/R2DE8YMQ/Yuan and Hayashi - 2006 - Standard errors in covariance structure models As.pdf;/home/justin/Zotero/storage/4IYFY8GT/000711005X85896.html}
}

@article{yuan2008,
  title = {Structural Equation Modeling with near Singular Covariance Matrices},
  author = {Yuan, Ke-Hai and Chan, Wai},
  year = {2008},
  journal = {Computational Statistics \& Data Analysis},
  volume = {52},
  number = {10},
  pages = {4842--4858},
  doi = {10/dk4cmq},
  keywords = {matrix smooth}
}

@article{yuan2010,
  title = {Determinants of {{Standard Errors}} of {{MLEs}} in {{Confirmatory Factor Analysis}}},
  author = {Yuan, Ke-Hai and Cheng, Ying and Zhang, Wei},
  year = {2010},
  month = dec,
  journal = {Psychometrika},
  volume = {75},
  number = {4},
  pages = {633--648},
  issn = {0033-3123, 1860-0980},
  doi = {10/b4fqcz},
  langid = {english}
}

@article{yuan2011,
  title = {Ridge Structural Equation Modelling with Correlation Matrices for Ordinal and Continuous Data},
  author = {Yuan, Ke-Hai and Wu, Ruilin and Bentler, Peter M.},
  year = {2011},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {64},
  number = {1},
  pages = {107--133},
  doi = {10/cwd74t},
  keywords = {matrix smooth}
}

@article{yuan2011a,
  title = {Ridge Structural Equation Modelling with Correlation Matrices for Ordinal and Continuous Data: {{Ridge SEM}} with Correlation Matrices},
  shorttitle = {Ridge Structural Equation Modelling with Correlation Matrices for Ordinal and Continuous Data},
  author = {Yuan, Ke-Hai and Wu, Ruilin and Bentler, Peter M.},
  year = {2011},
  month = feb,
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {64},
  number = {1},
  pages = {107--133},
  issn = {00071102},
  doi = {10/cwd74t},
  langid = {english},
  file = {/home/justin/Zotero/storage/NLAV3N2E/ptpmcrender.pdf}
}

@article{yuan2014,
  title = {Information {{Matrices}} and {{Standard Errors}} for {{MLEs}} of {{Item Parameters}} in {{IRT}}},
  author = {Yuan, Ke-Hai and Cheng, Ying and Patton, Jeff},
  year = {2014},
  month = apr,
  journal = {Psychometrika},
  volume = {79},
  number = {2},
  pages = {232--254},
  issn = {1860-0980},
  doi = {10/gjrkcz},
  abstract = {The paper clarifies the relationship among several information matrices for the maximum likelihood estimates (MLEs) of item parameters. It shows that the process of calculating the observed information matrix also generates a related matrix that is the middle piece of a sandwich-type covariance matrix. Monte Carlo results indicate that standard errors (SEs) based on the observed information matrix are robust to many, but not all, conditions of model/distribution misspecifications. SEs based on the sandwich-type covariance matrix perform most consistently across conditions. Results also suggest that SEs based on other matrices are either not consistent or perform not as robust as those based on the sandwich-type covariance matrix or the observed information matrix.},
  langid = {english},
  file = {/home/justin/Zotero/storage/47LBTZB5/Yuan et al. - 2014 - Information Matrices and Standard Errors for MLEs .pdf}
}

@article{yuan2016,
  title = {Measurement Invariance via Multigroup {{SEM}}: {{Issues}} and Solutions with Chi-Square-Difference Tests.},
  shorttitle = {Measurement Invariance via Multigroup {{SEM}}},
  author = {Yuan, Ke-Hai and Chan, Wai},
  year = {2016},
  month = sep,
  journal = {Psychological Methods},
  volume = {21},
  number = {3},
  pages = {405--426},
  issn = {1939-1463, 1082-989X},
  doi = {10/f848vr},
  langid = {english},
  file = {/home/justin/Zotero/storage/SUJJI82B/Yuan_Chan_2016_Measurement invariance via multigroup SEM.pdf}
}

@article{yuan2017,
  title = {More Efficient Parameter Estimates for Factor Analysis of Ordinal Variables by Ridge Generalized Least Squares},
  author = {Yuan, Ke-Hai},
  year = {2017},
  journal = {British Journal of Mathematical and Statistical Psychology},
  doi = {10/gb4dpr}
}

@article{zhang2001,
  title = {Linear Mixed Models with Flexible Distributions of Random Effects for Longitudinal Data},
  author = {Zhang, Daowen and Davidian, Marie},
  year = {2001},
  journal = {Biometrics},
  volume = {57},
  number = {3},
  pages = {795--802},
  publisher = {{Wiley Online Library}},
  doi = {10/d3n22b}
}

@article{zhang2006,
  title = {Bootstrap Fit Testing, Confidence Intervals, and Standard Error Estimation in the Factor Analysis of Polychoric Correlation Matrices},
  author = {Zhang, Guangjian and Browne, Michael W.},
  year = {2006},
  journal = {Behaviormetrika},
  volume = {33},
  number = {1},
  pages = {61--74},
  issn = {1349-6964},
  doi = {10/ch4dxg},
  abstract = {Ordinary least squares estimation is considered for fitting a factor analysis model to polychoric correlation matrices. A parametric bootstrap procedure is proposed for obtaining test statistics, standard error estimates, and confidence intervals associated with the OLS estimates. The adequacy of the proposed procedure is demonstrated using a simulation study.},
  langid = {english},
  file = {/home/justin/Zotero/storage/B325SXQJ/Zhang_Browne_2006_Bootstrap Fit Testing, Confidence Intervals, and Standard Error Estimation in.pdf}
}

@article{zhang2014estimating,
  title = {Estimating Standard Errors in Exploratory Factor Analysis},
  author = {Zhang, Guangjian},
  year = {2014},
  journal = {Multivariate behavioral research},
  volume = {49},
  number = {4},
  pages = {339--353},
  publisher = {{Taylor \& Francis}},
  doi = {10/gckfq2},
  date-added = {2020-02-07 10:53:57 -0600},
  date-modified = {2020-02-07 10:53:57 -0600}
}

@article{zhang2019,
  title = {A Sandwich Standard Error Estimator for Exploratory Factor Analysis with Nonnormal Data and Imperfect Models},
  author = {Zhang, Guangjian and Preacher, Kristopher J. and Hattori, Minami and Jiang, Ge and Trichtinger, Lauren A.},
  year = {2019},
  month = jul,
  journal = {Applied Psychological Measurement},
  volume = {43},
  number = {5},
  pages = {360--373},
  publisher = {{Sage Publications Inc}},
  address = {{Thousand Oaks}},
  issn = {0146-6216},
  doi = {10/ggjq7m},
  abstract = {This article is concerned with standard errors (SEs) and confidence intervals (CIs) for exploratory factor analysis (EFA) in different situations. The authors adapt a sandwich SE estimator for EFA parameters to accommodate nonnormal data and imperfect models, factor extraction with maximum likelihood and ordinary least squares, and factor rotation with CF-varimax, CF-quartimax, geomin, or target rotation. They illustrate the sandwich SEs and CIs using nonnormal continuous data and ordinal data. They also compare SE estimates and CIs of the conventional information method, the sandwich method, and the bootstrap method using simulated data. The sandwich method and the bootstrap method are more satisfactory than the information method for EFA with nonnormal data and model approximation error.},
  langid = {english},
  keywords = {factor analysis,factor rotation,latent variable models,standard   errors},
  annotation = {WOS:000471767900002},
  file = {/home/justin/Zotero/storage/6X6KGICF/Zhang et al_2019_A Sandwich Standard Error Estimator for Exploratory Factor Analysis With.pdf}
}

@article{zhao2020,
  title = {T-{{GCN}}: {{A Temporal Graph Convolutional Network}} for {{Traffic Prediction}}},
  shorttitle = {T-{{GCN}}},
  author = {Zhao, Ling and Song, Yujiao and Zhang, Chao and Liu, Yu and Wang, Pu and Lin, Tao and Deng, Min and Li, Haifeng},
  year = {2020},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {21},
  number = {9},
  pages = {3848--3858},
  issn = {1558-0016},
  doi = {10/ggkmgz},
  abstract = {Accurate and real-time traffic forecasting plays an important role in the intelligent traffic system and is of great significance for urban traffic planning, traffic management, and traffic control. However, traffic forecasting has always been considered an ``open'' scientific issue, owing to the constraints of urban road network topological structure and the law of dynamic change with time. To capture the spatial and temporal dependences simultaneously, we propose a novel neural network-based traffic forecasting method, the temporal graph convolutional network (T-GCN) model, which is combined with the graph convolutional network (GCN) and the gated recurrent unit (GRU). Specifically, the GCN is used to learn complex topological structures for capturing spatial dependence and the gated recurrent unit is used to learn dynamic changes of traffic data for capturing temporal dependence. Then, the T-GCN model is employed to traffic forecasting based on the urban road network. Experiments demonstrate that our T-GCN model can obtain the spatio-temporal correlation from traffic data and the predictions outperform state-of-art baselines on real-world traffic datasets. Our tensorflow implementation of the T-GCN is available at https://www.github.com/lehaifeng/T-GCN.},
  keywords = {Data models,Forecasting,Kalman filters,Logic gates,Predictive models,Roads,spatial dependence,Task analysis,temporal dependence,temporal graph convolutional network (T-GCN),Traffic forecasting},
  file = {/home/justin/Zotero/storage/FYBCC6ZB/Zhao et al_2020_T-GCN.pdf}
}

@article{zheng2010,
  title = {Using {{Cochran}}'s {{Z Statistic}} to {{Test}} the {{Kernel-Smoothed Item Response Function Differences Between Focal}} and {{Reference Groups}}},
  author = {Zheng, Yinggan and Gierl, Mark J and Cui, Ying},
  year = {2010},
  journal = {Educational and Psychological Measurement},
  keywords = {⛔ No DOI found,nonparametric IRT}
}

@article{zhu1997,
  title = {Algorithm 778: {{L-BFGS-B}}: {{Fortran}} Subroutines for Large-Scale Bound-Constrained Optimization},
  shorttitle = {Algorithm 778},
  author = {Zhu, Ciyou and Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge},
  year = {1997},
  month = dec,
  journal = {ACM Transactions on Mathematical Software},
  volume = {23},
  number = {4},
  pages = {550--560},
  issn = {0098-3500},
  doi = {10/bv55xf},
  abstract = {L-BFGS-B is a limited-memory algorithm for solving large nonlinear optimization problems subject to simple bounds on the variables. It is intended for problems in which information on the Hessian matrix is difficult to obtain, or for large dense problems. L-BFGS-B can also be used for unconstrained problems and in this case performs similarly to its predessor, algorithm L-BFGS (Harwell routine VA15). The algorithm is implemented in Fortran 77.},
  keywords = {large-scale optimization,limited-memory method,nonlinear optimization,variable metric method},
  file = {/home/justin/Zotero/storage/K8U3RE2M/Zhu et al_1997_Algorithm 778.pdf}
}

@phdthesis{zopluoglu2013,
  title = {Assessing Dimensionality of Latent Structures Underlying Dichotomous Item Response Data with Imperfect Models},
  author = {Zopluoglu, Cengiz},
  year = {2013},
  month = jul,
  address = {{Minneapolis, MN}},
  abstract = {The purpose of this study was to investigate the effect of model misspecification due to minor latent factors on a variety of dimensionality assessment methods proposed in the literature by using both real and simulated data. Several dimensionality assessment procedures based on eigenvalue examination (i.e., parallel analysis), conditional covariances (i.e., DETECT), and model selection approach (e.g., NOHARM and Mplus based chi-square statistics, RMSEA, GFI, AIC) were considered in the study. Two studies were conducted. In Study 1, the average, standard deviation, and range of the number of dimensions suggested by different approaches were investigated using sample datasets drawn from a very large real item response dataset treated as the population. In Study 2, a comprehensive simulation study was run, and the performances of the analytical methods were evaluated using the number of major dimensions in the true generating model as a reference.  The current study provides some interesting and provoking results regarding the performances of some well-known and most commonly used practices under certain conditions. The results of the current study suggest that most of the methods proposed in the literature and available for practitioners are not necessarily useful tools in dimensionality assessment, particularly if the goal of dimensionality assessment is to identify the latent traits with major influences, when the underlying factor structure is complex and minor factors are present. The current study provides some insight for the performance of different dimensionality assessment approaches with misspecified models when the underlying latent structure was factorially complex.},
  langid = {english},
  school = {University of Minnesota},
  annotation = {Accepted: 2015-10-13T18:54:08Z},
  file = {/home/justin/Zotero/storage/JZTNC94T/Zopluoglu_2013_Assessing Dimensionality of Latent Structures Underlying Dichotomous Item.pdf;/home/justin/Zotero/storage/ZQWP8MQB/174913.html}
}

@misc{zotero-2244,
  title = {Identifying {{General Factors}} of {{Intelligence}}: {{A Confirmatory Factor Analysis}} of the {{Ball Aptitude Battery}} - {{George A}}. {{Neuman}}, {{Aaron U}}. {{Bolin}}, {{Thomas E}}. {{Briggs}}, 2000},
  howpublished = {https://journals.sagepub.com/doi/abs/10.1177/00131640021970853?casa\_token=7e1T4Nb1aCgAAAAA\%3A2DU\_wrTSrcYSi-L4HIf1twYL9LTkNhHCyVmdFHkIT2Vf9hqZsEOOPOBm90Hlo0ioaUqIcjf3yS0K\&},
  file = {/home/justin/Zotero/storage/9UE5IMML/00131640021970853.html}
}

@article{zumbo2007,
  title = {Three Generations of {{DIF}} Analyses: {{Considering}} Where It Has Been, Where It Is Now, and Where It Is Going},
  shorttitle = {Three Generations of Dif Analyses},
  author = {Zumbo, Bruno D.},
  year = {2007},
  month = jul,
  journal = {Language Assessment Quarterly},
  volume = {4},
  number = {2},
  pages = {223--233},
  publisher = {{Routledge}},
  issn = {1543-4303},
  doi = {10/gjrkct},
  abstract = {The purpose of this article is to reflect on the state of the theorizing and praxis of DIF in general: where it has been; where it is now; and where I think it is, and should, be going. Along the way the major trends in the differential item functioning (DIF) literature are summarized and integrated providing some organizing principles that allow one to catalog and then contrast the various DIF detection methods and to shine a light on the future of DIF analyses. The three generations of DIF are introduced and described with an eye toward issues on the horizon for DIF.},
  annotation = {\_eprint: https://doi.org/10.1080/15434300701375832},
  file = {/home/justin/Zotero/storage/WWHLVKMI/Zumbo_2007_Three Generations of DIF Analyses.pdf;/home/justin/Zotero/storage/H2VYLZRC/15434300701375832.html}
}

@article{zusmanovich2013,
  title = {On near (Est) Correlation Matrix},
  author = {Zusmanovich, Pasha},
  year = {2013},
  journal = {arXiv preprint arXiv:1303.3226},
  eprint = {1303.3226},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,matrix smooth}
}

@article{zwick1986,
  title = {Comparison of Five Rules for Determining the Number of Components to Retain.},
  author = {Zwick, William R. and Velicer, Wayne F.},
  year = {1986},
  journal = {Psychological Bulletin},
  volume = {99},
  number = {3},
  pages = {432--442},
  issn = {1939-1455, 0033-2909},
  doi = {10/b2fsgs},
  langid = {english}
}


