
# Results {#results}

```{r read-results}
results_matrix <- readRDS(here("data/results_matrix_with_conditions.RDS"))
```

In the previous section, I described the simulation study I conducted to learn more about the behavior of different methods for generating error-perturbed population covariance (correlation) matrices. The simulation study included three model-error methods---the (single- and multiple-target) TKL method, the CB method, and the WB method---and was designed to answer two primary questions. 

First, I wanted to know whether different model-error methods led to different values of the CFI, TLI, and CRMR fit indices when used with the same error-free models and target RMSEA values. If the model-error methods led to systematically different values on the alternative fit indices when matched on RMSEA and all other characteristics, it would suggest that they are not exchangeable. In that case, researchers conducting simulation studies would have to consider which of the model-error methods most closely approximates the model error process they are trying to simulate. Moreover, such results would highlight the importance of reporting fit indices other than RMSEA when simulating imperfect models. If there were no meaningful differences among the methods, it would indicate that the choice of one particular model-error method over another (among the methods considered here) is not an important variable in the design of simulation studies.

A second purpose of the study was to evaluate the effectiveness of the proposed multiple-target method for generating correlation matrices with model error that had RMSEA and CFI values that were close to the specified target values. It was not expected that the algorithm would be able to produce correlation matrices with RMSEA and CFI values that were very close to the target values for all of the major-factor population models because of the relationship between RMSEA, CFI, and population model characteristics [@lai2016]. Therefore, I used the absolute deviation between the observed and target RMSEA and CFI values to compare the results from the multiple-target TKL method to the results from the CB and WB methods used in Study 1.

The remainder of the section is structured as follows. First, I report how many of the simulated matrices had properties that would make them unsuitable for use in a simulation study. For instance, the CB method sometimes produced $\bSigma$ matrices that were indefinite (i.e. having one or more negative eigenvalues). Additionally, the TKL-based model-error methods sometimes failed to converge using the L-BFGS-B method or led to solutions with nominally minor factors that would more rightly be considered major factors due to the number of salient factor loadings. Second, I report the distributions of the five fit indices investigated in this study (RMSEA, CFI, TLI, and CRMR) for each of the five model-error methods, conditioned on the other variables in the study design. Third, I report the extent to which the RMSEA and CFI values for the $\bSigma$ matrices produced by each of the model-error methods indicated similar levels of model fit. Fourth, I compared the RMSEA and CFI values corresponding to discrepancy between $\bSigma$ and $\bOmega$ to the RMSEA and CFI values corresponding to the discrepancy between $\bSigma$ and $\bOmegaHat$. Finally, I report results showing that the TKL-based optimization method was able to generate solutions with RMSEA and CFI values that were very close to the target values when the target value combinations were known to be possible.

## Indefinite Matrices (CB) {#indefinite-matrices}

```{r add-is-indefinite-variable}
results_matrix <- mutate(
  results_matrix,
  error = case_when(is.na(error) ~ " ",
                    TRUE ~ error)  
) %>% mutate(
  is_indefinite = str_detect(error, "indefinite")
)

num_cb <- filter(results_matrix, error_method == "CB") %>% nrow()
num_indefinite <- filter(results_matrix,
                         error_method == "CB",
                         is_indefinite == TRUE) %>% nrow()
percent_indefinite <- round((num_indefinite / num_cb) * 100, 1)
```

One drawback of the CB model-error method is that the resulting correlation matrix with model error can be indefinite when the specified target RMSEA value is large [@cudeck1992]. These matrices are undesirable because correlation and covariance matrices are, by definition, at least positive semi definite (i.e., having strictly non-negative eigenvalues). Of the `r comma(num_cb)` correlation matrices with model error that were generated using the CB method, `r comma(num_indefinite)` (`r comma(percent_indefinite)`%) were indefinite. However, indefinite solutions were much more common for some conditions of the simulation design than other. \@ref(fig:fig-percent-indefinite-matrices) shows the percent of indefinite CB solutions for each level of model fit, number of items per factor, number of factors, and factor loading strength. (Exact percentages are reported in \@ref(tab:tab-percent-indefinite-matrices)). The figure shows at least three notable trends. First, the percent of indefinite solutions increased as model fit degraded. Second, the percent of indefinite solutions increased as the total number of items increased (i.e., as the number of factors and items per factor increased). Finally, \@ref(fig:fig-percent-indefinite-matrices) shows that the percent of indefinite solutions increased as factor loadings increased. 

In the best-case scenarios, conditions corresponding to models with 25 items or fewer led to indefinite solutions very infrequently (in less than 1% of cases). On the other hand, conditions with Poor model fit and 45 items or more led to indefinite correlation matrices in more than 90% of cases. These results show that the CB method would be an inefficient way to simulate positive semi definite population correlation or covariance matrices with model error when input matrices are large and the target RMSEA value is relatively large. 

Although inefficient, a potential strategy for dealing with indefinite solutions when using the CB method is to simply generate solutions using the CB method until a sufficient number of positive semi definite solutions have been obtained. However, the amount of time taken by the CB method increases quickly as the number of items increase, making the oversampling strategy impractical for problems with many items. In fact, using the CB method to generate even a small number of solution matrices becomes impractical for large input matrices. This is shown in \@ref(fig:cb-completion-time), which is a plot of the completion time for the CB method when applied to a one-factor model with salient loadings fixed at .6 and the number of items varying between 5 and 120. Using a computer with an Intel Core i5-4570 3.20GHz CPU and 16GB of RAM, the CB method took just over 30 seconds to complete (on average) for an input correlation matrix with 65 items. For an input matrix with 115 items, the CB method took approximately four and a half minutes to complete. Such long completion times are often impractical for large simulation studies, particularly if indefinite solutions are discarded. In fact, I had to skip using the CB method in simulation conditions with 10 factors and 15 items per factor (150 items) because those conditions would have taken an impractical amount of time to complete. Timing a single example, the CB method took 15 minutes and 48 seconds to complete with a 150-item input correlation matrix. At that rate, it would have taken almost 11 days to complete one (out of 36) conditions of the simulation design with 150 items. 

(ref:cb-indefinite-table-caption) The percent of Cudeck and Browne (CB) model-error method solutions that were indefinite.

```{r tab-percent-indefinite-matrices}
cb_indefinite_table <- results_matrix %>%
  filter(error_method == "CB") %>%
  group_by(factors, items_per_factor, 
           loading_numeric, model_fit) %>%
  summarise(mean_indefinite = mean(is_indefinite)) %>%
  ungroup() %>%
  mutate(mean_indefinite = case_when(factors == 10 & items_per_factor == 15 ~ NA_real_,
                                     TRUE ~ mean_indefinite)) %>%
  pivot_wider(values_from = mean_indefinite,
              names_from = factors)

cb_indefinite_table %>%
  mutate(across(.cols = c(`1`:`5`), ~ . * 100)) %>%
  apa_table(col.names = c("Items/Factor", "Loading", "Model Fit", 
                          "1", "3", "5", "10"),
            digits = c(0, 1, 0, 1, 1, 1, 1),
            align = "rrlrrrr",
            col_spanners = list("Factors" = c(4, 7)),
            format.args = list("na_string" = "---"),
            caption = "(ref:cb-indefinite-table-caption)",
            label = "cb-indefinite",
            note = "The Cudeck-Browne method was not used for conditions with 10 major factors and 15 items per factor because it was prohibitively slow for those conditions.")
```

```{r fig-percent-indefinite-matrices, fig.cap = "The percent of Cudeck-Browne (CB) method solutions that were indefinite, conditioned on number of factors, factor loading, number of items per factor, and model fit."}
if (make_plots) {
  cb_percent_indefinite <- results_matrix %>%
    filter(error_method == "CB") %>%
    mutate(error= case_when(is.na(error) ~ " ",
                            TRUE ~ error),
           items_per_factor = as.factor(items_per_factor),
           factors = as.factor(factors),
           model_fit = factor(model_fit,
                              levels = c("Very Good", "Fair", "Poor"),
                              labels = c("Fit: Very Good", 
                                         "Fit: Fair", 
                                         "Fit: Poor"))) %>%
    group_by(factors, items_per_factor_rec, 
             loading_numeric, model_fit) %>%
    summarise(mean_indefinite = mean(
      str_detect(error, "indefinite")
    )) %>%
    ungroup() %>%
    ggplot(aes(y = mean_indefinite, x = loading_numeric,
               color = factors,
               shape = factors, 
               linetype = factors,
               group = factors)) +
    geom_line() + 
    geom_point() +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
    scale_x_continuous(breaks = c(.4, .6, .8)) +
    facet_grid(items_per_factor_rec ~ model_fit) +
    theme_bw() +
    labs(color = "Factors",
         fill = "Factors",
         linetype = "Factors",
         shape = "Factors",
         y = "Indefinite Solutions",
         x = "Factor Loading") +
    theme(legend.position = "bottom")
  
  ggsave(filename = here("img/cb_percent_indefinite.png"),
         plot = cb_percent_indefinite,
         dpi = "retina",
         width = 6,
         height = 5)
}

knitr::include_graphics(here("img/cb_percent_indefinite.png"))
```

```{r cb-completion-time, fig.cap = 'The amount of time (in minutes) taken to generate a single correlation matrix with model error using the CB method. Completion times were recorded 10 times for single-factor models with salient factor loadings fixed at .6 and number of items (\\emph{p}) varying between 5 and 115. The dashed black line is the loess regression line.', align = "center"}
if (make_plots) {
  set.seed(123)
  max_p <- 115
  cb_times <- data.frame(p = seq(5, max_p, by = 10), t = NA)
  safe_cb <- possibly(cb, otherwise = NA)
  
  t <- pblapply(
    X = 1:nrow(cb_times),
    FUN = function(i) {
      mod <- simFA(Model = list(NFac = 1, NItemPerFac = cb_times$p[i]),
                   Loadings = list(FacLoadRange = .6,
                                   FacLoadDist = "fixed"))
      t <- microbenchmark(
        safe_cb(mod, target_rmsea = 0.05),
        times = 10,
        unit = "milliseconds"
      )
      
      t
    }
  )
  
  names(t) <- cb_times$p
  times <- map_dfr(t, ~ .x$time)
  times <- times %>%
    as_tibble() %>%
    pivot_longer(cols = everything(),
                 names_to = "p",
                 values_to = "ns") %>%
    mutate(p = as.numeric(p)) %>%
    mutate(minutes = (ns * 1e-8) / 60) %>%
    select(-ns)
  
  times <- times %>% add_case(p = 150, minutes = 15.8)
  
  cb_time_plot <- ggplot(times, aes(x = p, y = minutes)) +
    geom_point(alpha = 0.3) +
    geom_smooth(size = .4, color =" black", linetype = "dashed", 
                method = "loess", level = 0, span = 0.35) +
    scale_x_continuous(breaks = seq(5, 150, by = 10)) +
    labs(x = "p",
         y = "Completion Time (minutes)",
         title = "CB method completion time",
         subtitle = "One-factor models with salient loadings of .6 and RMSEA = 0.05") +
    theme_bw()
  
  saveRDS(times, here("misc/cb_completion_times.RDS"))
  
  ggsave(filename = here("img", "cb_time_plot.png"),
         plot = cb_time_plot,
         dpi = 320,
         height = 5,
         width = 6)
}

knitr::include_graphics(path = here("img", "cb_time_plot.png"))
```

## L-BFGS-B Non-convergence (Genetic Algorithm)

The default optimization method for the multiple-target TKL method was L-BFGS-B. In most cases, this method worked well and converged relatively quickly to a solution. However, there were a small number of cases where the L-BGFS-B method failed to converge. Specifically, the L-BFGS-B method failed to converge 14 times ($<1$%) when the target CFI and RMSEA values were both used and when model fit was Poor. The L-BFGS-B non-convergence rates for the $\TKLrmseacfi$ method by number of factors, number of items per factor, and model fit are shown in \@ref(tab:tab-l-bfgs-b-convergence). Non-convergence was also somewhat more likely for conditions with few factors compared to conditions with many factors.

Although non-convergence of the L-BFGS-B algorithm was rare, a question that arose was whether the genetic algorithm that was used as a fallback option led to similar results compared to cases where the L-BFGS-B algorithm converged. This question was difficult to answer statistically because non-convergence occurred so infrequently. To get a general sense of whether results for cases where the L-BFGS-B algorithm converged or did not converge were similar, I plotted the CFI and RMSEA values for all converged and non-converged cases in conditions where the multiple-target TKL method was used with target CFI and RMSEA values and model fit was Poor. \@ref(fig:comparison-of-converged-vs-non-converged) shows that cases where the L-BFGS-B algorithm did not converge (shown in black) led to similar RMSEA and CFI values compared to cases where the algorithm converged (shown in gray). Thus, using a genetic algorithm seemed to provide reasonable results in the few cases where the L-BFGS-B algorithm failed to converge (albeit much more slowly than the L-BFGS-B algorithm).

```{r comparison-of-converged-vs-non-converged, fig.cap = "RMSEA and CFI values for cases where the model-error method was $\\textrm{TKL}_{\\textrm{RMSEA/CFI}}$ and model fit was Poor. Grey dots indicate cases where the L-BFGS-G algorithm converged; black dots indicate cases that did not converge and where a genetic algorithm was used instead."}
if (make_plots) {
  nonconverged_data <- results_matrix %>%
    filter(error_method == "TKL (RMSEA/CFI)") %>%
    mutate(converged = 
             warning != "simpleWarning: `optim()` failed to converge, using `ga()` instead.\n") %>%
    mutate(converged = case_when(is.na(converged) ~ TRUE,
                                 TRUE ~ converged)) %>%
    mutate(converged = factor(converged, 
                              levels = c(TRUE, FALSE), 
                              labels = c("Yes", "No")))
  
  nonconverged_rmsea_cfi <- nonconverged_data %>%
  filter(model_fit == "Poor") %>%
  ggplot(aes(y = cfi, x = rmsea)) +
  geom_point(size = 1, alpha = 0.8) +
  gghighlight(converged == "No", use_direct_label = FALSE,
              unhighlighted_params = list(alpha = 0.1)) +
  geom_vline(xintercept = 0.09, size = .25, color = "grey10", linetype = "dashed") +
  geom_hline(yintercept = 0.90, size = .25, color = "grey10", linetype = "dashed") +
  theme_bw() +
  labs(x = "RMSEA", y = "CFI", color = "L-BFGS-B Convergence")

  ggsave(filename = here("img/nonconverged_rmsea_cfi.png"),
         dpi = 320,
         width = 4.25, 
         height = 4)
}

knitr::include_graphics(here("img/nonconverged_rmsea_cfi.png"))
```

(ref:bfgs-convergence-table-caption) The percent of cases in each combination of conditions where the L-BFGS-B algorithm did not converge after 100 random starts and genetic optimization was used instead.

```{r tab-l-bfgs-b-convergence, results='asis'}
bfgs_nonconvergence_table <- results_matrix %>%
  filter(error_method == "TKL (RMSEA/CFI)") %>%
  mutate(warning = case_when(is.na(warning) ~ " ",
                             TRUE ~ warning)) %>%
  group_by(factors, model_fit, items_per_factor) %>%
  summarise(mean_nonconvergence = mean(
    warning == "simpleWarning: `optim()` failed to converge, using `ga()` instead.\n"
  ) * 100) %>%
  ungroup() %>%
  pivot_wider(values_from = mean_nonconvergence,
              names_from = model_fit)

bfgs_nonconvergence_table %>%
  papaja::apa_table(col.names = c("Factors", "Items/Factor",
                                  "Very Good",
                                  "Fair",
                                  "Poor"),
                    digits = c(0, 0, 1, 1, 1),
                    align = "rrrrr",
                    font_size = "small",
                    col_spanners = list("Model Fit" = c(3, 5)),
                    label = "bfgs-convergence-table",
                    caption = "(ref:bfgs-convergence-table-caption)")
```

## Major Minor Factors (W Matrices)

Recall that the multiple-objective TKL method included an optional penalty that penalized cases that had strong minor factors. Specifically, the penalty was applied if any minor factor had two or more absolute factor loadings greater than or equal to a specified value. The purpose of the penalty was to avoid introducing minor factors that might be more accurately characterized as major factors. To determine whether the penalty was effective at helping avoid major minor factors, I checked each of the minor factor loading ($\mathbf{W}$) matrices to determine whether any minor factor had two or more loadings greater than .3 in absolute value. 

The percent of cases where the constraints on $\mathbf{W}$ were violated for each level of number of factors, number of items per factor, factor loading strength, and model fit are shown in \@ref(fig:w-major-factors-plot) and reported in \@ref(tab:tab-major-minor-factors). Only results for the $\TKLrmsea$ were included because the $\TKLrmseacfi$ and $\TKLcfi$ error methods seldom led to solutions that violated the constraints on $\mathbf{W}$. In fact, only 24 out of 180,000 cases had violated $\mathbf{W}$ constraints for the $\TKLrmseacfi$ and $\TKLcfi$ methods combined. \@ref(fig:w-major-factors-plot) shows that the $\mathbf{W}$ constraints were violated most often when model fit was Fair or Poor, factor loadings were relatively low, and there were many total items (i.e., many factors and items per factor).

(ref:major-minor-factors) The percent of cases that violated the minor common factor loading constraints for each level of number of factors, items per factor, factor loading, and model fit when the $\TKLrmsea$ method was used.

```{r tab-major-minor-factors, results='asis'}
percent_w_major_factors_table <- results_matrix %>%
  filter(error_method == "TKL (RMSEA)") %>%
  group_by(factors, items_per_factor, loading_numeric, model_fit) %>%
  mutate(w_has_major_factors = fn_value >= 1e06) %>%
  summarise(mean_w_major_factors =
              mean(w_has_major_factors, na.rm = TRUE) * 100) %>%
  ungroup() %>%
  pivot_wider(values_from = mean_w_major_factors,
              names_from = "model_fit")

percent_w_major_factors_table %>%
  apa_table(col.names = c("Factors", "Items per Factor", "Loading",
                          "Very Good", 
                          "Fair", 
                          "Poor"),
            col_spanner = list("Model Fit" = c(4, 6)),
            digits = c(0, 0, 1, 1, 1, 1),
            align = "rrlrrr",
            caption = "(ref:major-minor-factors)",
            label = "major-minor-factors",
            font_size = "small")
```

```{r w-major-factors-plot}
#| fig.cap = "The percent of cases where the constraints on $\\mathbf{W}$ were violated when the $\\textrm{TKL}_{\\textrm{RMSEA}}$ model-error method was used, conditioned on number of factors, number of items per factor, factor loading, and model fit."

if (make_plots) {
  percent_w_major_factors_table <- results_matrix %>%
    filter(!(error_method %in% c("CB", "WB"))) %>%
    group_by(factors, items_per_factor_rec, loading_numeric, 
             model_fit_rec, error_method_rec, error_method) %>%
    mutate(w_has_major_factors = fn_value >= 1e06) %>%
    summarise(mean_w_major_factors =
                mean(w_has_major_factors, na.rm = TRUE))
  
  w_major_factors_plot <- percent_w_major_factors_table %>%
    filter(error_method == "TKL (RMSEA)") %>%
    mutate(factors = as.factor(factors)) %>%
    ggplot(aes(y = mean_w_major_factors, 
               x = loading_numeric, 
               color = factors, 
               linetype = factors, 
               shape = factors, 
               group = factors)) +
    geom_line() +
    geom_point() +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    scale_y_continuous(label = scales::percent_format(accuracy = 1)) +
    scale_x_continuous(breaks = c(.4, .6, .8)) +
    facet_grid(items_per_factor_rec ~ model_fit_rec) +
    theme_bw() +
    labs(x = "Loadings", y = TeX("Cases with Violtated $W$ Constraints"), 
         color = "Factors", linetype = "Factors", shape = "Factors") +
    theme(legend.position = "bottom")
  
  ggsave(filename = here("img/major_minor_factors.png"),
         w_major_factors_plot,
         dpi = 320,
         width = 6, 
         height = 5)
}

knitr::include_graphics(here("img/major_minor_factors.png"))
```

To understand why certain conditions led to more $\mathbf{W}$ constraint violations than others, it is helpful to understand the relationship between those conditions and the TKL parameters ($\epsilon$ and $\nu_{\textrm{e}}$). \@ref(fig:eps-and-v-values-by-W-violations) shows the $\epsilon$ and $\nu_{\textrm{e}}$ values for each of the TKL model-error methods, conditioned on factor loading strength, number of factors, and number of items per factor. To conserve space, only conditions with Poor model fit and 1, 5, or 10 major factors were included in the figure. Overall, the figure shows that there was a trade-off between $\epsilon$ and $\nu_{\textrm{e}}$ such that higher values of $\nu_{\textrm{e}}$ were related to lower values of $\epsilon$ (and *vice versa*). Moreover, \@ref(fig:eps-and-v-values-by-W-violations) shows that the distributions of $\epsilon$ and $\nu_{\textrm{e}}$ differed depending on which TKL variant was used. The differences between the error-method variants were largest when the there were many items (i.e., many items and many items per factor) and when factor loadings were relatively weak. Under those circumstances, the $\TKLrmsea$ method led to higher values of $\nu_{\textrm{e}}$ than the $\TKLrmseacfi$ or $\textrm{TKL}_\textrm{CFI}$ methods. This effect suggested that higher values of $\nu_{\textrm{e}}$ were required to produce solutions with RMSEA values close to .09 when there are many items. On the other hand, the results for the $\TKLrmseacfi$ and $\TKLcfi$ methods indicated that much lower $\nu_{\textrm{e}}$ values were required to obtain CFI values close to .90 when there were many items.

```{r eps-and-v-values-by-W-violations}
#| fig.cap = "Values of the TKL parameters ($\\epsilon$ and $\\nu_{\\textrm{e}}$) by model-error method, number of factors, number of items per factor, and factor loading strength when model fit was Poor. Results for conditions with three or five major factors were omitted to conserve space. TKL = Tucker, Koopman, and Linn."

if (make_plots) {
  eps_and_nu_plot <- results_matrix %>%
    mutate(w_has_major_factors = factor(w_has_major_factors, 
                                        levels = c(FALSE, TRUE),
                                        labels = c("No", "Yes"))) %>%
    filter(model_fit == "Poor",
           factors %in% c(1,10),
           !(error_method %in% c("CB", "WB"))) %>%
    ggplot(aes(y = v, x = eps, color = error_method)) +
    geom_point(alpha = .1, size = .75) +
    scale_color_brewer(
      palette = "Dark2", type = "qual",
      labels = list("TKL (RMSEA)" = TeX("$TKL_{RMSEA}$"),
                    "TKL (RMSEA/CFI)" = TeX("$TKL_{RMSEA/CFI}$"),
                    "TKL (CFI)" = TeX("$TKL_{CFI}"))) +
    scale_x_continuous(breaks = c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)) +
    facet_grid(factors_rec * items_per_factor_rec ~ loading_rec) +
    labs(y = TeX("$\\nu_e$"), x = TeX("$\\epsilon$"), color = "Error Method") +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +
    theme_bw() +
    theme(legend.position = "bottom")

  ggsave(filename = here("img/eps_and_nu_w_violations.png"),
         eps_and_nu_plot,
         dpi = 320,
         height = 7, width = 6)
}

knitr::include_graphics(here("img/eps_and_nu_w_violations.png"))
```

The apparent trade-off between $\epsilon$ and $\nu_{\textrm{e}}$ values made sense considering their roles in the TKL method. A high $\nu_{\textrm{e}}$ value indicated that much of the unique variance would be assigned to the minor common factors. If the value of $\epsilon$ was also high, it indicated that the first few minor factors would account for most of the minor factor variance. Therefore, the $\mathbf{W}$ constraints were more likely to be violated when both $\epsilon$ and $\nu_{\textrm{e}}$ were high and less likely to be violated if either parameter was low. The link between $\epsilon$ and $\nu_{\textrm{e}}$ and constraint violations is shown in \@ref(fig:w-violation-example), which shows the values of $\epsilon$ and $\nu_{\textrm{e}}$ produced by the three TKL variants for conditions with Poor model fit, 10 factors, 15 items per factor, and factor loadings of 0.8. Each point (corresponding to a single case) was colored according to whether or not the $\mathbf{W}$ constraints were violated. The figure shows that the $\mathbf{W}$ constraints were violated when the values of $\epsilon$ and $\nu_{\textrm{e}}$ were both higher than some threshold values, which only happened when the $\textrm{TKL}_\textrm{RMSEA}$ was used.

```{r w-violation-example}
#| fig.cap = "The distribution of $\\epsilon$ and $\\nu_{\\textrm{e}}$ (and whether or not $\\mathbf{W}$ constraints were violated) for conditions with Poor model fit, 10 factors, 15 items per factor, and factor loadings of 0.8. TKL = Tucker, Koopman, and Linn."

if (make_plots) {
  eps_and_nu_violation_plot <- results_matrix %>%
    mutate(w_has_major_factors = factor(w_has_major_factors, 
                                        levels = c(FALSE, TRUE),
                                        labels = c("No", "Yes"))) %>%
    filter(model_fit == "Poor",
           factors == 10,
           items_per_factor == 15,
           loading_numeric == 0.8,
           !(error_method %in% c("CB", "WB"))) %>%
    ggplot(aes(y = v, x = eps, color = w_has_major_factors)) +
    geom_point(alpha = .3) +
    scale_color_manual(values = c("#CCCCCC", "#440154FF")) +
    facet_grid(~ error_method_rec, labeller = label_parsed) +
    labs(y = TeX("$\\nu_{e}$"), x = TeX("$\\epsilon$"), 
         color = "W Constraints Violated") +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5), 
                                reverse = TRUE)) +
    theme_bw() +
    theme(legend.position = "bottom", panel.spacing.x = unit(14, units = "points"))

  ggsave(filename = here("img/eps_and_nu_violation_example.png"),
         eps_and_nu_violation_plot,
         dpi = 320,
         height = 3, width = 6)
}

knitr::include_graphics(here("img/eps_and_nu_violation_example.png"))
```

### Choice of Penalty Value

The penalty value in \autoref{eq:rmsea-cfi-obj-function}, $\lambda$, can be set at a higher or lower value to change the penalty that is incurred when a potential solution has a $\mathbf{W}$ matrix that violates the user-specified constraints. In the present study, the penalty was set to be $\lambda = 1,000,000$ to ensure that the penalty was sufficiently large. However, it is possible that smaller values of $\lambda$ would have also been as effective or more effective at leading to solutions that satisfied the constraints on $\mathbf{W}$. To determine which values of $\lambda$ worked best, I simulated 200 correlation matrices with model error using the $\TKLrmsea$ method for each of 36 conditions. These conditions were formed by crossing the number of major common factors (1, 3, 5, or 10), and seven values of $\lambda$ ($\lambda \in [0, \:0.1, \:1, \:10, \:100, \:1,000, \:10,000, \:100,000, \:1,000,000]$). All factors were orthogonal, with factor loadings fixed at 0.4, 15 items per factor, and a target RMSEA value of 0.09. These sets of values were chosen because they often resulted in solutions with violated $W$ constraints in the main simulation study.[^effect-of-lambda-code]

[^effect-of-lambda-code]: Code for this simulation study is provided in \@ref(effect-of-lambda).

The proportions of solutions with violated $\mathbf{W}$ constraints for each combination of number of factors and $\lambda$ are shown in \@ref(fig:penalty-values). The figure shows that for conditions with one, three, or five factors, the proportion of solutions with violations of the $\mathbf{W}$ constraints decreased as $\lambda$ increased from zero to one and then leveled off for $\lambda$ values greater than one. For conditions with ten factors, all solutions had violated $\mathbf{W}$ constraints, regardless of the $\lambda$ value. Although only a subset of the conditions from the full study were included in this smaller simulation, the results suggested that using a smaller $\lambda$ value is unlikely to have resulted in a substantial decrease in the number of solutions with violated $\mathbf{W}$ constraints. On the other hand, the results also suggested that $\lambda$ values as small as one are likely just as effective as much larger values at preventing solutions with violated $\mathbf{W}$ constraints. 

```{r penalty-values, out.width='70%'}
#| fig.cap = "The percent of cases with violated $\\mathbf{W}$ constraints conditioned on the constraint penalty value ($\\lambda$) and the number of major common factors when the $\\textrm{TKL}_{\\textrm{RMSEA}}$ method was used. All conditions had 15 items per factor, major common factor loadings fixed at 0.4, and a target RMSEA value of 0.09. One hundred solution matrices were generated for each condition."
if (make_plots) {
  # Set the number of reps
  reps <- 200
  
  # Create a matrix of fully-crossed conditions
  factors <- unique(results_matrix$factors)
  items_per_factor <- 15
  loading <- c(.4)
  target_rmsea <- c(0.090)
  penalty <- c(0, .1, 1, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6)
  
  conditions_matrix <- expand.grid(
    factors = factors,
    items_per_factor = items_per_factor,
    loading = loading,
    target_rmsea = target_rmsea
  )
  
  
  # Function to check whether a W matrix has more than two factor loadings greater
  # than 0.3 for any minor factor.
  check_constraints <- function(W) {
    any(max(apply(abs(W) >= .3, 2, sum)) > 2)
  }
  
  # For each condition, generate a population correlation matrix without model
  # error and then generate 200 population correlations with model error using
  # the TKL (RMSEA) method
  set.seed(666)
  constraint_violations <- map_dfr(
    .x = 1:nrow(conditions_matrix), 
    .f = function(condition) {
      
      cat("\nWorking on condition", condition, "of", nrow(conditions_matrix))
      
      # Generate population correlation matrix without model error
      mod <- simFA(
        Model = list(NFac = conditions_matrix$factors[condition],
                     NItemPerFac = conditions_matrix$items_per_factor[condition],
                     Model = "orthogonal"),
        Loadings = list(FacLoadDist = "fixed",
                        FacLoadRange = conditions_matrix$loading[condition])
      )
      
      # Generate 200 population correlation matrices with model error
      pro_map_dfr(.x = 1:reps,
                  .f = function(x, target_rmsea) {
                    map_dfr(.x = penalty, 
                            .f = function(penalty, mod, target_rmsea, seed) {
                              set.seed(seed)
                              sol <- noisemaker(mod = mod, 
                                                method = "TKL", 
                                                target_rmsea = target_rmsea,
                                                target_cfi = NULL,
                                                tkl_ctrl = list(penalty = penalty,
                                                                NWmaxLoading = 2,
                                                                WmaxLoading = .3))
                              w_constraints_violated <- check_constraints(sol$W)
                              
                              c(penalty = penalty, 
                                constraints_violated = w_constraints_violated)
                            },
                            mod = mod,
                            target_rmsea = target_rmsea,
                            seed = sample(1e6, 1))
                  }, target_rmsea = conditions_matrix$target_rmsea[condition])
    }
  )
  
  # Bind the conditions matrix to the results to indicate which result belongs to
  # which condition
  constraint_violations <- bind_cols(
    conditions_matrix[rep(1:nrow(conditions_matrix), 
                          each = length(penalty) * reps),],
    constraint_violations
  )
  
  # Plot the results
  constraint_violations %>%
    mutate(factors = as.factor(factors),
           penalty = factor(penalty, 
                            labels = c("0", "0.1", "1", "10", 
                                       "100", "1,000", "10,000",
                                       "100,000", "1,000,000"))) %>%
    group_by(factors, penalty) %>%
    summarise(percent = mean(constraints_violated, na.rm = TRUE)) %>%
    ggplot(aes(x = penalty, y = percent, color = factors, shape = factors, 
               linetype = factors, group = factors)) +
    geom_point() +
    geom_line() +
    scale_y_continuous(labels = scales::percent) +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    labs(y = "Cases with Violtated W Constraints",
         x = latex2exp::TeX("$\\lambda$"),
         color = "Factors", shape = "Factors", linetype = "Factors") +
    theme_bw() +
    theme(legend.position = "bottom")
  
  # Save the plot
  ggsave(filename = here("img/penalty_values.png"),
         dpi = 320,
         height = 4,
         width = 6)
}

knitr::include_graphics(here("img/penalty_values.png"))
```

### TKL Model-Error Method Objective Function Surfaces

An unfortunate conclusion from both the main simulation study and the smaller study of $\lambda$ values was that enforcing constraints on $\mathbf{W}$ using the penalty in \autoref{eq:rmsea-cfi-obj-function} was only somewhat effective when the $\TKLrmsea$ method was used. Regardless of the $\lambda$ value used, $\TKLrmsea$ often led to solutions with violated $\mathbf{W}$ constraints in conditions with Poor model fit, weak factor loadings, and many items or factors. In contrast, the $\TKLcfi$ and $\TKLrmseacfi$ methods rarely produced solutions that violated the constraints on $\mathbf{W}$. However, it was unclear whether the the penalty worked better for the $\TKLcfi$ and $\TKLrmseacfi$ methods for some reason, or whether the inclusion of a target CFI value simply made it less likely for the $\mathbf{W}$ constraints to be violated regardless of whether the penalty was included. 

To test whether the inclusion of the target CFI value was sufficient to avoid violated $\mathbf{W}$ constraints when no penalty was applied, I conducted a small-scale simulation study. Specifically, I generated 200 $\bSigma$ matrices using a population correlation matrix $\bOmega$ with ten (orthogonal) major common factors, 15 items per factor, and salient factor loadings fixed at 0.4. Each of the $\bSigma$ matrices were generated using the $\TKLrmseacfi$ method with target RMSEA and CFI values of 0.09 and 0.90, respectively, and $\lambda = 0$ so that no penalty was applied. I chose this condition because nearly 100% of the solutions generated by the $\TKLrmsea$ for this condition in the main simulation study had violated $\mathbf{W}$ constraints, whereas the $\TKLcfi$ and $\TKLrmseacfi$ methods did produce any solutions with violated $\mathbf{W}$. Thus, generating $\bSigma$ matrices for this condition using the $\TKLrmseacfi$ with $\lambda = 0$ was a reasonable test of whether using a penalty to enforce the $\mathbf{W}$ constraints was was necessary when the $\TKLrmseacfi$ was used.[^tkl-cfi-penalty-code] Somewhat surprisingly, I found that none of the 200 solutions that were generated violated the $\mathbf{W}$ constraints, even when $\lambda$ was set to zero. This result suggested that the addition of a target CFI value in and of itself was useful for producing solutions without unacceptably strong minor factors.

[^tkl-cfi-penalty-code]: Code for this simulation study is provided in \@ref(tkl-cfi-penalty).

```{r is-a-penalty-useful-with-cfi-target}
if (make_plots) {
  
  # Check whether the TKL (CFI) method leads to solutions that violate the W 
  # constraints when no penalty is applied
  set.seed(42)
  
  # Create a population correlation matrix without model error (Omega)
  mod <- simFA(Model = list(NFac = 10,
                            NItemPerFac = 15,
                            Model = "orthogonal"),
               Loadings = list(FacLoadDist = "fixed",
                               FacLoadRange = .4))
  
  # Function to check whether a W matrix has more than two factor loadings greater
  # than 0.3 for any minor factor.
  check_constraints <- function(W) {
    any(max(apply(abs(W) >= .3, 2, sum)) > 2)
  }
  
  # Generate 200 population correlation matrices with model error (Sigma) using 
  # the TKL (CFI) method without a penalty, then check how often the constraints
  # on W were violated
  constraints_violated_vec <- pbmclapply(
    X = 1:200,
    FUN = function(x) {
      sol <- noisemaker(mod = mod, 
                        method = "TKL", 
                        target_rmsea = 0.09,
                        target_cfi = 0.9,
                        tkl_ctrl = list(penalty = 0))
      check_constraints(sol$W)
    },
    mc.cores = 4
  )
  
  # What percent of cases had violated W constraints?
  mean(constraints_violated_vec, na.rm = TRUE)
}
```

To better understand why the $\TKLrmsea$ method so often led to solutions that violated the constraints on $\mathbf{W}$ compared to the $\TKLcfi$ and $\TKLrmseacfi$ methods, I evaluated the objective function over a grid of $62,500$ $\epsilon$ and $\nu_{\textrm{e}}$ values for a single pair of $\bOmega$ and $\mathbf{W}^*$ matrices and each of the three TKL model-error methods. The $\bOmega$ population correlation matrix without model error corresponded to a model with 10 orthogonal major factors and 15 items per factor, with salient factor loadings fixed at 0.4. The penalty value, $\lambda$, was fixed at 1,000 to penalize solutions with minor factors that had more than two factor loadings greater or equal to 0.3 in absolute value. The objective function surfaces for each of the three TKL model-error methods are shown in \@ref(fig:obj-function-landscape). To make it easier to compare the objective function surfaces, I made separate plots for cases where the $\mathbf{W}$ constraints were and were not violated. Moreover, I scaled the objective function values ($G_{\textrm{obj}}$) in each panel of \@ref(fig:obj-function-landscape) to fall between zero and one so that the surfaces could be easily compared. The figure showed clear differences between the objective function surfaces for the $\TKLrmsea$ method compared to the $\TKLcfi$ and $\TKLrmseacfi$ methods. In particular, the figure showed that the values of $F_{\textrm{obj}}$ decreased as $\epsilon$ and $\nu_{\textrm{e}}$ increased. In contrast, the values of $F_{\textrm{obj}}$ increased as $\epsilon$ and $\nu_{\textrm{e}}$ increased for the $\TKLcfi$ and $\TKLrmseacfi$ methods. 

The results shown in \@ref(fig:obj-function-landscape) helped explain why the $\TKLrmsea$ method often led to solutions with violated $\mathbf{W}$ constraints even when such solutions were heavily penalized. The figure shows that for $\epsilon$ and $\nu_{\textrm{e}}$ needed to be relatively large to produce solutions with RMSEA values that were close to the target RMSEA value of 0.09. In fact, the lowest $F_{\textrm{obj}}$ values were produced when $\epsilon$ and $\nu_{\textrm{e}}$ were as large as possible without violating the $\mathbf{W}$ constraints. The objective function value increased sharply after $\epsilon$ and $\nu_{\textrm{e}}$ became large enough to violate the $\mathbf{W}$ constraints, but then decreased again as $\epsilon$ and $\nu_{\textrm{e}}$ increased. Therefore, the optimization procedure was likely to move in the direction of steepest descent toward larger values of $\epsilon$ and $\nu_{\textrm{e}}$ corresponding to local minima, regardless of whether the constraints on $\mathbf{W}$ were violated.

```{r obj-function-landscape, out.width = '100%'}
#| fig.cap = "Heatmaps of the scaled objective function surface for combinations of $\\epsilon$ and $\\nu_{\\textrm{e}}$ values and each of the Tucker, Koopman, and Linn (TKL; 1969) model-error methods. Objective function values ($G_{\textrm{obj}}$) were computed using a $\\bm{\\Omega}$ matrix with ten orthogonal major factors, 15 items per factor, and salient factor loadings fixed at 0.4. The penalty value, $\\lambda$, was fixed at 1,000 to penalize solutions with minor factors that had more than two factor loadings greater or equal to 0.3 in absolute value. To aid visualization, the the objective function values for combinations of $\\epsilon$ and $\\nu_{\\textrm{e}}$ where the constraints on $\\mathbf{W}$ were violated were scaled and plotted separately from combinations that did not lead to constraint violations."

if (make_plots) {
  par_grid <- expand.grid(
    eps = seq(0, 1, length.out = 50),
    nu = seq(0, 1, length.out = 50)
  )
  
  simFA_mod <- simFA(Model = list(NFac = 10, 
                                  NItemPerFac = 15, 
                                  Model = "orthogonal"),
                     Loadings = list(FacLoadRange = .4,
                                     FacLoadDist = "fixed"),
                     ModelError = list(ModelError = TRUE),
                     Seed = 123)
  
  Rpop <- simFA_mod$Rpop
  W <- simFA_mod$W
  
  p <- nrow(Rpop)
  k <- ncol(simFA_mod$loadings)
  df <- (p * (p - 1) / 2) - (p * k) + (k * (k - 1) / 2) # model df
  target_rmsea <- 0.09
  target_cfi <- 0.9
  weights <- list("TKL_RMSEA" = c(1, 0), 
                  "TKL_CFI" = c(0, 1), 
                  "TKL_RMSEA_CFI" = c(1, 1))
  WmaxLoading <- .3
  NWmaxLoading <- 2
  return_values <- FALSE
  penalty <- 1000
  
  CovMajor <- simFA_mod$loadings %*% simFA_mod$Phi %*% t(simFA_mod$loadings)
  u <- 1 - diag(CovMajor)
  
  obj_val_list <- vector(mode = "list", length = 3L)
  i <- 1
  for (error_method_weights in weights) {
    val <- map2_dbl(
      par_grid$nu,
      par_grid$eps,
      ~ noisemaker::obj_func(par = c(.x, .y),
                             Rpop = Rpop,
                             W = W,
                             p = p,
                             u = u,
                             df = df,
                             target_rmsea = target_rmsea,
                             target_cfi = target_cfi,
                             weights = weights[[i]],
                             WmaxLoading = WmaxLoading,
                             NWmaxLoading = NWmaxLoading,
                             return_values = return_values,
                             penalty = penalty)
    )
    obj_val_list[[i]] <- val
    i <- i + 1
  }
  
  par_grid <- data.frame(
    eps = rep(par_grid$eps, 3),
    nu  = rep(par_grid$nu, 3),
    error_method = rep(c("TKL[RMSEA]", "TKL[CFI]", "TKL[RMSEA/CFI]"), 
                       each = nrow(par_grid)),
    value = c(obj_val_list[[1]], obj_val_list[[2]], obj_val_list[[3]])
  )
  
  par_grid <- par_grid %>%
    mutate(error_method = factor(error_method,
                                 levels = c("TKL[RMSEA]", 
                                            "TKL[CFI]", 
                                            "TKL[RMSEA/CFI]")))
  
  without_W_penalty <- par_grid %>%
    filter(value < 1000) %>%
    group_by(error_method) %>%
    mutate(value = (value - min(value, na.rm = TRUE)) / 
             (max(value, na.rm = TRUE) - min(value, na.rm = TRUE))) %>%
    ggplot(aes(x = eps, y = nu, fill = value, z = value)) +
    geom_raster() +
    geom_contour(color = "lightgrey") +
    scale_fill_viridis_c() +
    scale_x_continuous(expand = c(0,0)) +
    scale_y_continuous(expand = c(0,0), limits = c(0, .2)) +
    facet_grid( ~ error_method, labeller = label_parsed) +
    theme_bw() +
    labs(x = "",
         y = latex2exp::TeX("$\\nu_e$"),
         fill = latex2exp::TeX("$\\textit{G}_{obj}$"),
         title = "Constraints on W Not Violated") +
    theme(panel.spacing = unit(20, "points"))
  
  with_W_penalty <- par_grid %>%
    filter(value >= 1000) %>%
    group_by(error_method) %>%
    mutate(value = (value - min(value, na.rm = TRUE)) / 
             (max(value, na.rm = TRUE) - min(value, na.rm = TRUE))) %>%
    ggplot(aes(x = eps, y = nu, fill = value, z = value)) +
    geom_raster() +
    geom_contour(color = "grey") +
    scale_fill_viridis_c() +
    scale_x_continuous(expand = c(0,0)) +
    scale_y_continuous(expand = c(0,0), limits = c(0, .2)) +
    facet_grid( ~ error_method, labeller = label_parsed) +
    theme_bw() +
    labs(x = "",
         y = latex2exp::TeX("$\\nu_e$"),
         fill = latex2exp::TeX("$\\textit{G}_{obj}$"),
         title = "Constraints on W Violated") +
    theme(panel.spacing = unit(20, "points"))
  
  obj_func_plot <- without_W_penalty / with_W_penalty + 
    plot_layout(guides = "collect")
  
  # Save as a pdf --- need to insert epsilon axis labels manually because
  # R only has varepsilon.
  ggsave(filename = here("img/obj_func_landscape_ten_factor.pdf"),
         obj_func_plot,
         dpi = 320,
         height = 5.75,
         width = 8)
  
  saveRDS(par_grid, here("data/obj_function_values_ten_factor.RDS")) 
}

knitr::include_graphics(here("img/obj_func_landscape_ten_factor.png"))
```

In contrast to the $\TKLrmsea$ method, the $\TKLcfi$ and $\TKLrmseacfi$ methods rarely produced solutions that violated the constraints on $\mathbf{W}$. This result can be at least partially explained by comparing the objective function surfaces shown in \@ref(fig:obj-function-landscape). Unlike the $\TKLrmsea$ method, the objective function surfaces for the $\TKLcfi$ and $\TKLrmseacfi$ methods were lowest when both $\epsilon$ and $\nu_{\textrm{e}}$ were small. The optimization procedure was therefore unlikely to move toward values of $\epsilon$ and $\nu_{\textrm{e}}$ that produced solutions with violated $\mathbf{W}$ constraints. Moreover, even if the optimization procedure was initialized at large values of of $\epsilon$ and $\nu_{\textrm{e}}$ corresponding to violated $\mathbf{W}$ constraints, the gradient of the objective-function surface would have directed the optimization procedure back toward smaller $\epsilon$ and $\nu_{\textrm{e}}$ values that did not violate the $\mathbf{W}$ constraints. Therefore, adding a target CFI value in addition to (or instead of) a target RMSEA value ended up being far more effective than using an explicit penalty in \autoref{eq:rmsea-cfi-obj-function} for ensuring a meaningful distinction between major and minor common factors.

## Distributions of Fit Statistics

One of the primary questions the simulation study was intended to answer was whether the five model-error methods produced solutions with different fit index values when used with the same error-free models and target RMSEA and CFI values. In this section, I report the distributions of the RMSEA, CFI, TLI and CRMR model-fit indices for solutions produced by each of the five model-error methods ($\TKLrmsea$, $\TKLcfi$, $\TKLrmseacfi$, CB, and WB).

### RMSEA

Of the fit indices investigated in this study, the RMSEA value has been most often used as a measure of model fit when generating covariance or correlation matrices with model error. \@ref(fig:rmsea-distributions) shows box-plots summarizing the distributions of RMSEA values for each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. Notice that the $\TKLrmsea$ and CB methods almost always produced solutions with RMSEA values that were very close to the target RMSEA values. This result makes sense because both methods use optimization to produce solutions with RMSEA values close to a specified target. However, a somewhat unexpected result was that the CB method occasionally produced solutions with RMSEA values that were much higher or lower than the target value, particularly in conditions with ten major factors.

After the $\TKLrmsea$ and CB methods, \@ref(fig:rmsea-distributions) shows that the WB method was the next best model-error method in terms of producing solutions with RMSEA values close to the target values. In fact, the WB method produced solutions with median RMSEA values that were as close to the target values as those from $\TKLrmsea$ and CB solutions. However, the WB method also led to more variable RMSEA values, particularly in conditions with Poor model fit and many factors.

The two methods that performed worst in terms of producing solutions with RMSEA values close to the targets were the $\TKLrmseacfi$ and $\TKLcfi$ methods. \@ref(fig:rmsea-distributions) shows that these methods often led to RMSEA values that were lower than the target values, except when there were relatively few factors and strong factor loadings. The largest differences between the observed and target RMSEA values for the $\TKLrmseacfi$ and $\TKLcfi$ methods occurred in conditions with Poor model fit and relatively weak factor loadings (.3). In those conditions, both methods led to RMSEA values that were considerably lower than the target values.

```{r rmsea-distributions, fig.cap = "Distributions of the RMSEA values for solutions produced by each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. The dashed lines indicate the target RMSEA value for each condition. Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.", fig.align='left', out.width=425}
if (make_plots) {
  rmsea_distributions <- 
    results_matrix %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = rmsea)) +
    geom_hline(aes(yintercept = target_rmsea), 
               color = "black", linetype = "dashed", size = .25) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = "RMSEA") +
    theme_bw() +
    theme(plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/rmsea_distributions.png"), 
    plot = rmsea_distributions,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/rmsea_distributions.png"))
```

### CFI

The distributions of CFI values for the solutions produced using the five model-error methods are shown in \@ref(fig:cfi-distributions), conditioned on number of factors, model fit, and factor loading strength. As with \@ref(fig:rmsea-distributions), the middle levels of model fit and factor loading strength were omitted to converse space. \@ref(fig:cfi-distributions) shows that the results for CFI were nearly opposite to the results for RMSEA. Specifically, whereas the $\TKLrmsea$, CB, and WB methods produced solutions with RMSEA values much closer to the target values than those produced by the $\TKLrmseacfi$ or $\TKLcfi$ methods, the $\TKLrmseacfi$ and $\TKLcfi$ produced solutions with CFI values that were closer to the target CFI values compared to the other model-error methods in most conditions. The differences between the methods that optimized for CFI ($\TKLrmseacfi$ and $\TKLcfi$) and the other model-error methods were largest for conditions with Poor model fit, low factor loadings, and many factors (see the third row of \@ref(fig:cfi-distributions)). On the other hand, \@ref(fig:cfi-distributions) shows that all of the model-error methods produced similar CFI values (that were close to the target CFI value) for conditions with very good model fit and strong factor loadings.

A result in \@ref(fig:cfi-distributions) that is worth highlighting is that the $\TKLrmseacfi$ and $\TKLcfi$ produced solutions with very similar CFI values in most conditions. These two methods also produced solutions with very similar RMSEA values in many conditions, as shown in \@ref(fig:rmsea-distributions). Indeed, the $\TKLrmseacfi$ and $\TKLcfi$ methods led to much more similar results in terms of both RMSEA and CFI than the $\TKLrmseacfi$ and $\TKLrmsea$ methods. Put another way, optimizing for CFI alone generally led to similar results compared to optimizing for both CFI and RMSEA, whereas optimizing for RMSEA alone generally led to solutions with much different RMSEA and CFI values compared to optimizing for both CFI and RMSEA. This suggests that CFI was more sensitive to small changes in parameter values than RMSEA.

The distributions of CFI values shown in \@ref(fig:cfi-distributions) also emphasized the importance of reporting multiple fit indices when simulating correlation or covariance matrices with model error. For instance, the $\TKLrmsea$ and CB methods produced solutions with RMSEA values close to the target RMSEA value of .09 in conditions with Poor model fit and weak factor loadings. However, \@ref(fig:cfi-distributions) shows that those two model-error methods led to solutions with unacceptably low CFI values in the same conditions.

```{r cfi-distributions, fig.cap = "Distributions of the CFI values for solutions produced by each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. The dashed lines indicate the target CFI value for each condition. Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.", out.width=425}
if (make_plots) {
  cfi_distributions <- 
    results_matrix %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = cfi)) +
    geom_hline(aes(yintercept = target_cfi), 
               color = "black", linetype = "dashed", size = .25) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = "CFI") +
    theme_bw() +
    theme(plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/cfi_distributions.png"), 
    plot = cfi_distributions,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/cfi_distributions.png"))
```

### TLI

The distributions of TLI values for the solutions produced using the five model-error methods are shown in \@ref(fig:tli-distributions), conditioned on number of factors, model fit, and factor loading strength. Overall, the distributions of TLI values were quite similar to the distributions of CFI values shown in \@ref(fig:cfi-distributions). In particular, the $\TKLcfi$ and $\TKLrmseacfi$ methods tended to produce solutions with higher TLI values than the other model-error methods, except for conditions with a single factor, Poor model fit, and strong factor loadings. Similar to CFI, the differences in TLI values resulting from the $\TKLcfi$ and $\TKLrmseacfi$ methods compared to the other model-error methods were most pronounced for conditions with many factors, weak factor loadings, and Poor model fit.

```{r tli-distributions, out.width="100%"}
#| fig.cap="Distributions of the TLI values for solutions produced by each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. The dashed lines indicate the threshold values of TLI that correspond to the targeted levels of model fit, according to Hu and Bentler (1999). Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  tli_distributions <- 
    results_matrix %>%
    mutate(tli = case_when(tli < 0 ~ 0, TRUE ~ tli)) %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = tli)) +
    geom_hline(aes(yintercept = target_cfi), 
               color = "black", linetype = "dashed", size = .25) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = "TLI") +
    theme_bw() +
    theme(panel.spacing.x = unit(10, units = "pt"),
          plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/tli_distributions.png"), 
    plot = tli_distributions,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/tli_distributions.png"))
```

### CRMR

The distributions of the CRMR model fit index by level of model fit, factor loading, number of major common factors, and model-error method are represented as boxplots in \@ref(fig:crmr-distributions). The figure shows that in conditions with Very Good model fit, all of the model-error methods led to CRMR values that were generally at or below 0.05, the rule-of-thumb CRMR threshold for good model fit given by Hu and Bentler [-@hu1999]. In general, all of the model fit methods led to similar CRMR distributions when model fit was Very Good. The largest differences in CRMR values between model-error methods when model fit was Very Good occured when there were many factors and low factor loadings. In those conditions, the $\TKLrmseacfi$ and $\TKLcfi$ model-error methods led to lower CRMR values compared to the other methods.

Larger differences between the model-error methods emerged when model fit was Poor. In those conditions, only the $\TKLrmsea$ and WB methods consistently led to CRMR values that were near or above the threshold CRMR value for acceptable model fit given by Hu and Bentler [-@hu1999]. In fact, the $\TKLcfi$ and $\TKLrmseacfi$ methods often led to CRMR values that were less than the Hu and Bentler [-@hu1999] threshold for good model fit and were frequently the methods the led to the lowest CRMR values. The only conditions where the $\TKLcfi$ and $\TKLrmseacfi$ methods led to higher median CRMR values were those with one-factor models, salient factor loadings of 0.8, and Poor (target) model fit.

<!-- [TK: Need to discuss the fit indices in terms of theta hat values as well. Where best to put that material?] -->

```{r crmr-distributions, fig.cap="Distributions of the CRMR values for solutions produced by each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. Recommendations for threshold values of SRMR/CRMR are not as fine-grained as for some other fit indices, but threshold values of 0.05 and 0.08 were proposed by Hu and Bentler (1999) as more and less conservative upper-bounds for acceptable SRMR/CRMR values. The dashed lines indicate these two threshold values. Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.", out.width="100%"}
if (make_plots) {
  crmr_distributions <- 
    results_matrix %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    mutate(target_crmr = case_when(model_fit == "Poor" ~ 0.08,
                                   model_fit == "Very Good" ~ 0.05)) %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = crmr)) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    geom_hline(yintercept = 0.05, 
               color = "black", linetype = "dashed", size = .25) +
    geom_hline(yintercept = 0.08, 
               color = "black", linetype = "dashed", size = .25) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = "CRMR") +
    theme_bw() +
    theme(plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/crmr_distributions.png"), 
    plot = crmr_distributions,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/crmr_distributions.png"))
```

## Fit Index Agreement

In the previous section, I reported results in terms of the RMSEA, CFI, TLI, and CRMR values individually. However, it is often recommended to evaluate model fit using more than one fit index [@hu1999; @raykov2012]. Therefore, I was interested to what extent the model-error methods included in this simulation study led to solutions with fit indices indicating similar levels of model fit. In this section, I report the results in terms of fit index agreement. Specifically, I address two main questions. First, how well did model-error methods do at producing solutions with RMSEA and CFI values that were close to the target values corresponding to a particular level of model fit? Second, to what extent did the model-error methods produce solutions with fit index values that corresponded to the same qualitative interpretation of model fit? For both questions, I focused on the agreement between RMSEA and CFI, both for the sake of simplicity and because RMSEA and CFI were the fit indices that I used as target values for the model-error methods.

To evaluate how well each of the model error-methods did at producing solutions that had both RMSEA and CFI values that were close to the target values, I used the metric

\begin{equation}
D = |\textrm{RMSEA}_{\textrm{obs}} - \textrm{RMSEA}_{\textrm{T}}| + |\textrm{CFI}_{\textrm{obs}} - \textrm{CFI}_{\textrm{T}}|,
(\#eq:distance-between-observed-and-target)
\end{equation}

\noindent where $\textrm{RMSEA}_{\textrm{obs}}$ and $\textrm{CFI}_{\textrm{obs}}$ denote the observed RMSEA and CFI values for each solution. Small $D$ values indicated that the solution was good in the sense that the observed RMSEA and CFI values were close to the target RMSEA and CFI values. On the other hand, large $D$ values indicated a poor solution with either one or both of the observed RMSEA and CFI values far from the corresponding target values.

Box-plots showing the distribution of $D$ values for each of the model-error methods conditioned on the number of major common factors, model fit, and factor loading strength are shown in \@ref(fig:d3-plot). The figure shows that all of the model-error method led to good results (i.e., small $D$ values) in conditions with Very Good model fit and strong factor loadings. Similarly, all of the model-error methods led to reasonably good results in conditions with one or three major factors, Poor model fit, and strong factor loadings. In the remaining conditions where the model-error methods did not all lead to small $D$ values, the $\TKLcfi$ and $\TKLrmseacfi$ methods led to smaller $D$ values than the other model-error methods. In particular, the $\TKLcfi$ and $\TKLrmseacfi$ methods led to much smaller $D$ values than the alternative model-error methods in conditions with many major common factors and weak factor loadings, especially when model fit was Poor. The CB method typically led to the next-smallest $D$ values in these conditions, followed by the $\TKLrmsea$ and WB methods.

```{r d3-plot, out.width="100%"}
#| fig.cap = "The sum of the absolute differences between the observed and target RMSEA and CFI values ($D$), conditioned on number of factors, model fit, and factor loading strength. Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."
if (make_plots) {
  d_plot <- 
    results_matrix %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = d3)) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = latex2exp::TeX("\\textit{D}")) +
    theme_bw() +
    theme(plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/d_plot.png"), 
    plot = d_plot,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/d_plot.png"))
```

Looking at how far the observed RMSEA and CFI values differed from the target RMSEA and CFI values (in terms of $D$) gave one perspective on model fit agreement. I obtained a second perspective on model fit agreement by determining how often fit indices led to the same qualitative assessment of model fit using rule-of-thumb threshold values of RMSEA and CFI such as those reported by Hu and Bentler [-@hu1999]. To calculate the rates of qualitative model fit agreement, I first assigned each simulated correlation matrix into one of three qualitative model fit categories based on its observed RMSEA and CFI values. For RMSEA, a simulated correlation matrix was considered to have good model fit if the observed RMSEA value was less than or equal to .05. If the RMSEA was greater than 0.05 but less than .10, it was considered to have acceptable model fit. RMSEA values greater than .10 were considered to represent unacceptable model fit. Similarly, observed CFI values greater than .95 were considered to represent good model fit, CFI values between .95 and .90 were considered to represent acceptable model fit, and CFI values below .90 were considered to represent unacceptable model fit.

```{r calculate-agreement-rate, echo = FALSE, include = FALSE}
agreement_rates <- results_matrix %>% mutate(
  rmsea_subj_fit = case_when(rmsea <= 0.05 ~ "Good",
                             rmsea > 0.05 & rmsea <= 0.1 ~ "Acceptable",
                             rmsea > 0.1 ~ "Unacceptable"),
  cfi_subj_fit = case_when(cfi >= .95 ~ "Good",
                           cfi < .95 & cfi >= .9 ~ "Acceptable",
                           cfi < .9 ~ "Unacceptable")
) %>% mutate(
  fit_agreement = (rmsea_subj_fit == cfi_subj_fit)
) %>% group_by(error_method) %>% 
  summarise(mean_fit_agreement = mean(fit_agreement, na.rm = TRUE)) %>%
  pivot_wider(names_from = error_method, values_from = mean_fit_agreement)

colnames(agreement_rates) <-
  c("tkl_rmsea", "tkl_cfi", "tkl_rmsea_cfi", "cb", "wb")
```

The percent of simulated correlation matrices with model error that led to fit indices indicating qualitative agreement on model fit are shown in \@ref(fig:fit-agreement-fig), conditioned on number of factors, factor loading strength, target model fit, and model-error method. The figure shows that no single model-error method always led to the highest rates of qualitative fit agreement. When averaged over all conditions, the $\TKLcfi$ model-error method led to the highest rate of qualitative fit index agreement (`r scales::percent(agreement_rates$tkl_cfi, accuracy = .1)`), followed by the CB (`r scales::percent(agreement_rates$cb, accuracy = .1)`), $\TKLrmseacfi$ (`r scales::percent(agreement_rates$tkl_rmsea_cfi, accuracy = .1)`), $\TKLrmsea$ (`r scales::percent(agreement_rates$tkl_rmsea, accuracy = .1)`), and WB (`r scales::percent(agreement_rates$wb, accuracy = .1)`) methods. 

Breaking these results out further by level of target model fit, the $\TKLcfi$ and $\TKLrmseacfi$ methods almost always led to qualitative model fit agreement rates of 100% in conditions with Very Good model fit. The other model-error methods led to similar rates of qualitative model fit agreement in conditions with Very Good target model fit and strong factor loadings (i.e., Loading = 0.8), but led to much lower rates of agreement in conditions with Very Good model fit and weaker factor loadings. Results for conditions with Fair or Poor target model fit were less straightforward. When target model fit was Fair, the $\TKLcfi$ method often led to the highest rates of qualitative fit agreement of the model-error methods, particularly when factor loadings were weak and when there were many major common factors. When target model fit was Poor, all of the model-error methods had low qualitative fit agreement rates except when factor loadings were 0.8.

```{r fit-agreement-fig, out.width='100%'}
#| fig.cap = "The percent of cases where the observed RMSEA and CFI values led to the same qualitative evaluation of model fit based on the threshold values suggested by Hu and Bentler (1999). TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  
  labels <- list("TKL (RMSEA)" = TeX("$TKL_{RMSEA}$"),
                 "TKL (RMSEA/CFI)" = TeX("$TKL_{RMSEA/CFI}$"),
                 "TKL (CFI)" = TeX("$TKL_{CFI}"))
  
  results_matrix %>%
    mutate(
      rmsea_subj_fit = case_when(rmsea <= 0.05 ~ "Good",
                                 rmsea > 0.05 & rmsea <= 0.1 ~ "Acceptable",
                                 rmsea > 0.1 ~ "Unacceptable"),
      cfi_subj_fit = case_when(cfi >= .95 ~ "Good",
                               cfi < .95 & cfi >= .9 ~ "Acceptable",
                               cfi < .9 ~ "Unacceptable")
    ) %>% mutate(
      fit_agreement = (rmsea_subj_fit == cfi_subj_fit)
    ) %>% group_by(factors, loading_rec, model_fit_rec, error_method) %>%
    summarize(agreement_rate = mean(fit_agreement, na.rm = TRUE)) %>%
    ggplot(aes(y = agreement_rate, x = factors, color = error_method, 
               shape = error_method, linetype = error_method, 
               group = error_method)) +
    scale_y_continuous(label = scales::percent) +
    scale_x_continuous(breaks = 1:10) +
    geom_point() +
    geom_line() +
    scale_color_brewer(palette = "Dark2", type = "qual", labels = labels) +
    scale_shape_discrete(labels = labels) +
    scale_linetype_discrete(labels = labels) +
    guides(color = guide_legend(title = "Error Method"),
           shape = guide_legend(title = "Error Method"),
           linetype = guide_legend(title = "Error Method")) + 
    labs(y = "Fit Index Agreement", x = "Factors",
         shape = "Error Method", color = "Error Method", linetype = "Error Method") +
    facet_grid(model_fit_rec ~ loading_rec) +
    theme_bw() +
    theme(legend.position = "bottom")
  
  ggsave(filename = here("img/fit_index_agreement.png"),
         height = 6,
         width = 7,
         dpi = 320)
}

knitr::include_graphics(here("img/fit_index_agreement.png"))
```

(ref:fit-agreement-table-caption) The percent of cases where the observed RMSEA and CFI values led to the same qualitative evaluation of model fit based on the threshold values suggested by Hu and Bentler [-@hu1999].

```{r fit-agreement-table}
fit_agreement_table <- results_matrix %>%
  mutate(
    rmsea_subj_fit = case_when(rmsea <= 0.05 ~ "Good",
                               rmsea > 0.05 & rmsea <= 0.1 ~ "Acceptable",
                               rmsea > 0.1 ~ "Unacceptable"),
    cfi_subj_fit = case_when(cfi >= .95 ~ "Good",
                             cfi < .95 & cfi >= .9 ~ "Acceptable",
                             cfi < .9 ~ "Unacceptable")
  ) %>% mutate(
    fit_agreement = (rmsea_subj_fit == cfi_subj_fit)
  ) %>% group_by(factors, loading_numeric, model_fit, error_method) %>%
  summarize(agreement_rate = mean(fit_agreement, na.rm = TRUE)) %>%
  pivot_wider(names_from = error_method, 
              values_from = agreement_rate) %>%
  ungroup()

fit_agreement_table %>%
    mutate(across(.cols = c(`TKL (RMSEA)`:WB), ~ . * 100)) %>%
  apa_table(col.names = c("Factors", "Loading", "Model Fit", 
                          "$\\textrm{TKL}_{\\textrm{RMSEA}}$", 
                          "$\\textrm{TKL}_{\\textrm{CFI}}$", 
                          "$\\textrm{TKL}_{\\textrm{RMSEA/CFI}}$", 
                          "CB", "WB"),
            digits = c(0, 1, 0, 1, 1, 1, 1, 1),
            align = "rrlrrrrr",
            col_spanners = list("Qualitative Fit Agreement (\\%)" = c(4, 8)),
            format.args = list("na_string" = "---"),
            caption = "(ref:fit-agreement-table-caption)",
            label = "fit-agreement-table",
            font_size = "small",
            note = "TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.")
```

It was both interesting and unexpected that the $\TKLcfi$ and $\TKLrmseacfi$ methods led to similar results, both in terms of $D$ and in terms of observed RMSEA and CFI values, whereas the $\TKLrmsea$ method often led to results that were more similar to those of the CB and WB methods. The similarity between the $\TKLcfi$ and $\TKLrmseacfi$ results suggested that the CFI had more influence in \autoref{eq:rmsea-cfi-obj-function} than RMSEA, even when the fit indices were equally weighted. A possible explanation is provided in \@ref(fig:rmsea-cfi-distributions), which shows observed RMSEA and CFI values for solutions from the $\TKLrmsea$, $\TKLcfi$, and $\TKLrmseacfi$ methods, conditioned on number of factors and model fit. The figure shows that solutions generated using the $\TKLrmsea$ method had little variability in terms of RMSEA values. On the other hand, the CFI values for those solutions often had much more variability, particularly in conditions with weak factor loadings. When the $\TKLcfi$ method was used, solutions had RMSEA and CFI values that were generally constrained to a small range within each condition. These results indicated that the range of possible CFI values was much larger for solutions with a fixed RMSEA value than the range of RMSEA values for solutions with a fixed CFI value in many conditions. Therefore, the $\TKLcfi$ and $\TKLrmseacfi$ might have led to such similar results because including a CFI target constrained the range of RMSEA values. Put another way, incorporating a target RMSEA value in addition to a target CFI value might have had little effect compared to using only a target CFI value because making small changes to CFI often led to large changes in RMSEA.

```{r rmsea-cfi-distributions, out.width='100%'}
#| fig.cap = "RMSEA and CFI values for the TKL-based model-error methods, conditioned on number of factors and model fit. The dashed vertical and horizontal lines indicate the target RMSEA and CFI values, respectively. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."
if (make_plots) {
    labels <- list("TKL (RMSEA)" = TeX("$TKL_{RMSEA}$"),
                 "TKL (RMSEA/CFI)" = TeX("$TKL_{RMSEA/CFI}$"),
                 "TKL (CFI)" = TeX("$TKL_{CFI}"))
  
  results_matrix %>%
    filter(rmsea < 3, str_detect(error_method, "TKL"),
           factors != 3, loading_numeric != .6) %>%
    ggplot(aes(x = rmsea, y = cfi)) +
    geom_vline(aes(xintercept = target_rmsea), size = .3, linetype = "dashed") +
    geom_hline(aes(yintercept = target_cfi), size = .3, linetype = "dashed") +
    geom_jitter(alpha = .1, size = 1.25, 
                aes(color = error_method, shape = error_method)) +
    scale_color_brewer(palette = "Dark2", type = "qual", labels = labels) +
    scale_shape_discrete(labels = labels) +
    facet_grid(model_fit_rec~ factors_rec * loading_rec, scales = "fixed") +
    labs(x = "RMSEA", y = "CFI", color = "Error Method", shape = "Error Method") +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +
    theme_bw() +
    theme(legend.position = "bottom",
          panel.spacing.x = unit(8, units = "points"))
  
  ggsave(filename = here("img/rmsea-cfi-distributions.png"),
         dpi = 320,
         scale = 1.1,
         height = 5, width = 7)
}

knitr::include_graphics(path = here("img/rmsea-cfi-distributions.png"))
```

To better understand the conditional distribution of CFI values for solutions with fixed RMSEA values, I used the $\TKLrmsea$ method to generate solutions with a range of target RMSEA values for each of a subset of conditions from the main simulation study. Specifically, I generated 100 solutions for each of 16 target RMSEA values equally-spaced between .025 and .100 for each condition of the main simulation design with uncorrelated major common factors. (Conditions with correlated major common factors were omitted to make the number of conditions manageable.) The results can be seen in Figure \@ref(fig:rmsea-conditional-cfi), which shows the CFI and RMSEA values for each condition. The solid black lines in the figure indicate where RMSEA is equal to $1 - \textrm{CFI}$ and can be used to easily determine which fit index changed most quickly as a function of the other fit index. RMSEA and $1 - \textrm{CFI}$ are comparable in this context because they indicate roughly the same level of qualitative model fit over the range of target RMSEA values. For example, both $\textrm{RMSEA} = 0.05$ and $\textrm{CFI} = .95$ have been used as threshold values for good model fit [@hu1999]. Figure \@ref(fig:rmsea-conditional-cfi) shows that CFI values decreased faster than RMSEA values increased for most of the included conditions, with the exception of conditions with relatively few major factors and strong factor loadings.

```{r rmsea-conditional-cfi, out.width='100%'}
#| fig.cap = "Observed CFI and RMSEA values for solutions generated using the $\\textrm{TKL}_{\\textrm{RMSEA}}$ method. For each combination of number of factors, number of items per factor, and factor loading strength, 100 solutions were generated for each of 16 target RMSEA values equally-spaced between 0.025 and 0.1. The solid black line indicates where RMSEA and $1 - \\textrm{CFI}$ were equal."

if (make_plots) {
  # Show possible CFI values for a range of RMSEA values
  reps <- 100
  
  factors <- unique(results_matrix$factors)
  items_per_factor <- unique(results_matrix$items_per_factor)
  loading <- unique(results_matrix$loading_numeric)
  target_rmsea <- seq(0.025, 0.1, by = 0.005)
  
  conditions_matrix <- expand.grid(
    factors = factors,
    items_per_factor = items_per_factor,
    loading = loading,
    target_rmsea = target_rmsea
  )
  
  set.seed(666)
  rmsea_cfi_vals <- pro_map_dfr(
    .x = 1:nrow(conditions_matrix), 
    .f = function(condition) {
      mod <- simFA(
        Model = list(NFac = conditions_matrix$factors[condition],
                     NItemPerFac = conditions_matrix$items_per_factor[condition],
                     Model = "orthogonal"),
        Loadings = list(FacLoadDist = "fixed",
                        FacLoadRange = conditions_matrix$loading[condition])
      )
      
      map_dfr(.x = 1:reps,
              .f = function(x, target_rmsea) {
                sol <- noisemaker(mod = mod, 
                                  method = "TKL", 
                                  target_rmsea = target_rmsea,
                                  target_cfi = NULL,
                                  tkl_ctrl = list(penalty = 1e6,
                                                  NWmaxLoading = 2,
                                                  WmaxLoading = .3))
                w_constraints_violated <- sum(sol$W[,1] >= .3) > 2
                c(rmsea = sol$rmsea, cfi = sol$cfi, 
                  w_constraints_violated = w_constraints_violated)
              }, target_rmsea = conditions_matrix$target_rmsea[condition])
    }
  )
  
  rmsea_cfi_vals <- bind_cols(
    rmsea_cfi_vals,
    conditions_matrix[rep(1:nrow(conditions_matrix), each = reps),]
  )
  
  rmsea_cfi_vals <- rmsea_cfi_vals %>%
    mutate(
      factors_rec = factor(factors, 
                           labels = levels(results_matrix$factors_rec)),
      loading_rec = factor(loading, 
                           labels = levels(results_matrix$loading_rec)),
      items_per_factor_rec = factor(items_per_factor, 
                                    labels = levels(results_matrix$items_per_factor_rec))
    )
  
  saveRDS(rmsea_cfi_vals, file = here("data/rmsea_conditional_cfi.RDS"))
  # rmsea_cfi_vals <- readRDS(here("data/rmsea_conditional_cfi.RDS"))
  
  rmsea_cfi_vals %>%
    ggplot(aes(x = rmsea, y = cfi)) +
    geom_point(size = 1, alpha = .1) +
    geom_abline(aes(slope = -1, intercept = 1), size = .5, alpha = .5) +
    facet_grid(loading_rec * items_per_factor_rec ~ factors_rec) +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +
    labs(x = "RMSEA", y = "CFI") +
    theme_bw() +
    theme(legend.position = "bottom",
          panel.spacing.x = unit(14, units = "points"))
  
  ggsave(filename = here("img/rmsea_conditional_cfi.png"),
         height = 7,
         width = 7,
         scale = 1.15,
         dpi = 320)
}

knitr::include_graphics(
  path = here("img/rmsea_conditional_cfi.png")
)
```

In addition to looking at the conditional distribution of CFI values at fixed RMSEA values, I also looked at the conditional distribution of RMSEA values at fixed CFI values. Similar to the previous procedure for RMSEA, I used the $\TKLcfi$ method to generate 100 solutions with 19 target CFI values equally-spaced between 0.90 and 0.99 for each condition of the main simulation design with uncorrelated major common factors. The results are shown in \@ref(fig:cfi-conditional-rmsea). As in the previous figure, the points in \@ref(fig:cfi-conditional-rmsea) were often below the solid black line indicating (roughly) equivalent qualitative model fit. Points falling below the line denoted cases with RMSEA values that indicated better model fit than their corresponding CFI values. The figure also shows that CFI changed more quickly as a function of RMSEA than RMSEA changed as a function of CFI. Finally, the figure shows that there was little variability in the conditional RMSEA values for a particular CFI value in all of the conditions except conditions with one factor and five items per factor.

```{r cfi-conditional-rmsea, out.width = '100%'}
#| fig.cap = "Observed RMSEA and CFI values for solutions generated using the $\\textrm{TKL}_{\\textrm{CFI}}$ method. For each combination of number of factors, number of items per factor, and factor loading strength, 100 solutions were generated for each of 19 target CFI values equally-spaced between 0.90 and 0.99. The solid black line indicates where RMSEA and $1 - \\textrm{CFI}$ were equal."

if (make_plots) {
  reps <- 100
  
  factors <- unique(results_matrix$factors)
  items_per_factor <- unique(results_matrix$items_per_factor)
  loading <- unique(results_matrix$loading_numeric)
  target_cfi <- seq(0.90, 0.99, by = 0.005)
  
  conditions_matrix <- expand.grid(
    factors = factors,
    items_per_factor = items_per_factor,
    loading = loading,
    target_cfi = target_cfi
  )
  
  set.seed(666)
  cfi_rmsea_vals <- pblapply(
    X = 1:nrow(conditions_matrix), 
    FUN = function(condition) {
      mod <- simFA(
        Model = list(NFac = conditions_matrix$factors[condition],
                     NItemPerFac = conditions_matrix$items_per_factor[condition],
                     Model = "orthogonal"),
        Loadings = list(FacLoadDist = "fixed",
                        FacLoadRange = conditions_matrix$loading[condition])
      )
      
      map_dfr(.x = 1:reps,
              .f = function(x, target_cfi) {
                sol <- noisemaker(mod = mod, 
                                  method = "TKL", 
                                  target_rmsea = NULL,
                                  target_cfi = target_cfi,
                                  tkl_ctrl = list(penalty = 1e6,
                                                  NWmaxLoading = 2,
                                                  WmaxLoading = .3))
                w_constraints_violated <- sum(sol$W[,1] >= .3) > 2
                c(rmsea = sol$rmsea, cfi = sol$cfi, 
                  w_constraints_violated = w_constraints_violated)
              }, target_cfi = conditions_matrix$target_cfi[condition])
    }
  )
  
  cfi_rmsea_vals <- bind_rows(cfi_rmsea_vals)
  cfi_rmsea_vals <- bind_cols(
    cfi_rmsea_vals,
    conditions_matrix[rep(1:nrow(conditions_matrix), each = reps),]
  )
  
  cfi_rmsea_vals <- cfi_rmsea_vals %>%
    mutate(
      factors_rec = factor(factors, 
                           labels = levels(results_matrix$factors_rec)),
      loading_rec = factor(loading, 
                           labels = levels(results_matrix$loading_rec)),
      items_per_factor_rec = factor(items_per_factor, 
                                    labels = levels(results_matrix$items_per_factor_rec))
    )
  
  saveRDS(cfi_rmsea_vals, file = here("data/cfi_conditional_rmsea.RDS"))
  
  cfi_rmsea_vals %>%
    ggplot(aes(y = rmsea, x = cfi)) +
    geom_point(size = 1, alpha = .1) +
    geom_abline(aes(slope = -1, intercept = 1), size = .5, alpha = .5) +
    facet_grid(loading_rec * items_per_factor_rec ~ factors_rec) +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +
    labs(y = "RMSEA", x = "CFI") +
    theme_bw() +
    theme(legend.position = "bottom",
          panel.spacing.x = unit(14, units = "points"))
  
  ggsave(filename = here("img/cfi_conditional_rmsea.png"),
         height = 7,
         width = 7,
         scale = 1.15,
         dpi = 320)
}

knitr::include_graphics(here("img/cfi_conditional_rmsea.png"))
```

Taken together, the results in \@ref(fig:rmsea-cfi-distributions), \@ref(fig:rmsea-conditional-cfi), and \@ref(fig:cfi-conditional-rmsea) can help explain why the $\TKLcfi$ and $\TKLrmseacfi$ model-error methods often led to similar results. Because CFI values decreased more quickly than RMSEA values increased in most conditions, changes to parameter values that led to an RMSEA value that was closer to the target RMSEA value but moved the CFI value further from the target CFI value were more costly than changes that prioritized the target CFI value. Moreover, \@ref(fig:rmsea-conditional-cfi) and \@ref(fig:cfi-conditional-rmsea) also provided indications of which conditions were most likely to lead to conflicting qualitative interpretations of RMSEA and CFI. Points far from the line indicated RMSEA and CFI pairs that led to conflicting qualitative interpretations of model fit because RMSEA and $1 - \textrm{CFI}$ indicated approximately the same qualitative model fit over the range of target RMSEA values. Both figures show that conditions with weak factor loadings, many factors and items, and high RMSEA values (or low CFI values) were most likely to result in conflicting fit index interpretations.

The frequent disagreement between RMSEA and CFI can be also be understood by considering how these fit indices differ in how they describe model fit. Recall from \ref{population-model-fit} that in the population context, absolute fit indices (e.g., RMSEA) indicate how different the $\bSigma$ and $\bOmega$ matrices are from one another. On the other hand, relative model fit indices (e.g., CFI) indicate how much better the population model fits $\bSigma$ compared to the independence model. Generating a $\bSigma$ matrix corresponding to a particular RMSEA value is therefore a relatively straightforward matter of perturbing $\bOmega$ until the discrepancy between $\bSigma$ and $\bOmega$ results in the target RMSEA value.[^rmsea-complication] On the other hand, generating a $\bSigma$ matrix corresponding to a particular CFI value is more complex because CFI takes into account both the fit of the hypothesized model indicated by $\bOmega$ and the fit of the baseline (independence) model. Thus, perturbing the elements of $\bOmega$ to create $\bSigma$ negatively affects the fit of the baseline model in addition to the fit of the hypothesized model because the independence model implies that $\bSigma$ should be an identity matrix. However, unless the hypothesized model leads to a much worse fit (i.e., a much smaller minimized discrepancy function value) than the baseline model, the resulting CFI value will be relatively small.

This is demonstrated in \@ref(fig:cfi-as-factors-increase), which shows the minimized discrepancy function values for the hypothesized and baseline models as the number of major factors increased from one to ten, along with the corresponding CFI and RMSEA values. The hypothesized models (i.e., the population models without model error) were all orthogonal models with salient major factor loadings of 0.4, 15 items per factor, and between 1 and 10 common factors. The $\bSigma$ matrices were generated using the $\TKLrmsea$ method with a target RMSEA value of 0.09. To interpret Panel A of the figure, recall that the $\textrm{CFI} = 1 - F_t / F_b$ as defined in \autoref{eq:cfi}, where $F_t / F_b$ is the ratio of the minimized discrepancy function values for the hypothesized and baseline models. Panel A of \@ref(fig:cfi-as-factors-increase) shows that both $F_t$ and $F_b$ increased as the number of factors increased, holding everything else constant. However, $F_t$ grew fast enough that $F_t / F_b$ decreased as the number of factors increased, resulting in lower CFI values as RMSEA remained fixed (as shown in Panels B and C). To keep CFI constant as the number of factors increased, $F_t$ would have needed to increase at the same rate as $F_b$. For instance, Panel A includes a line indicating the values of $F_t$ required to produce a CFI value of 0.90 for each number of factors. Although the figure only shows results for a specific set of conditions, it demonstrates the tension between RMSEA and CFI that occurs as the order of $\bOmega$ increases. Specifically, it shows that fixing RMSEA at a value indicating Fair or Poor model fit when there are many items or factors requires an $F_m$ value that is too large (relative to $F_b$) to produce an acceptable CFI value.

```{r cfi-as-factors-increase, out.width = "100%"}
#| fig.cap = "Panel A: Minimized discrepancy function values, CFI, and RMSEA values for $\\Sigma$ matrices generated from orthogonal models with salient major factor loadings of 0.4, 15 items per factor, and between 1 and 10 factors. The $\\Sigma$ matrices were generated using the $\\textrm{TKL}_{\\textrm{RMSEA}}$ model-error method with a target RMSEA value of 0.09. The line in the left-most panel labeled ``Target for CFI = 0.90'' indicates the value of $F_t$ that would be needed to obtain a CFI value of 0.90, given the value of $F_b$. Panels B and C: Observed CFI and RMSEA values for each simulated $\\Sigma$ matrix in Panel A."

if (make_plots) {
  # Show why CFI degrades as the dimensions of Sigma increase
  factors <- 1:10
  out <- cbind("factors" = factors, "Ft" = NA, "Fb" = NA, "cfi" = NA, "rmsea" = NA)
  
  set.seed(42)
  out <- lapply(
    X = factors, 
    FUN = function(factor_num) {
      m1 <- simFA(Model = list(NFac = factor_num, NItemPerFac = 15),
                  Loadings = list(FacLoadRange = .4,
                                  FacLoadDist = "fixed"))
      
      error_mod <- noisemaker(mod = m1, method = "TKL", target_rmsea = 0.09)
      
      Omega <- m1$Rpop
      Sigma <- error_mod$Sigma
      p <- nrow(Sigma)
      Ft <- log(det(Omega)) - log(det(Sigma)) + sum(diag(Sigma %*% solve(Omega))) - p
      Fb <- -log(det(Sigma))
      
      c(factors = factor_num, 
        Ft = Ft, 
        Fb = Fb, 
        cfi = cfi(Sigma, Omega), 
        rmsea = rmsea(Sigma, Omega, k = factor_num))
    }
  )
  
  out <- out %>% 
    bind_rows() %>% 
    mutate(target = .1 * Fb)
  
  out <- out %>% 
    pivot_longer(-factors, names_to = "func_type")
  
  labels <- list("Ft" = TeX("$\\mathit{F}_t$"),
                 "Fb" = TeX("$\\mathit{F}_b$"),
                 "target" = TeX("Target $\\mathit{F}_t$ for CFI=0.90"))
  
  p1 <- out %>% 
    filter(func_type %in% c("Fb", "Ft", "target")) %>% 
    ggplot(aes(x = factors, y = value, color = func_type, linetype = func_type)) + 
    geom_point(size = 1) +
    geom_line() +
    scale_x_continuous(breaks = c(2,4,6,8,10)) +
    scale_color_brewer(palette = "Dark2", type = "qual", labels = labels) +
    scale_linetype_discrete(labels = labels) +
    guides(color = guide_legend(title = ""), 
           linetype = guide_legend(title = "")) +
    labs(x = "Factors", y = "Minimized Discrepancy Function Value", 
         color = "Model", linetype = "Model") +
    theme_bw() +
    theme(legend.position = "bottom")
  
  p2 <- out %>% 
    filter(func_type == "cfi") %>% 
    ggplot(aes(x = factors, y = value)) + 
    geom_point(size = 1) +
    geom_line() +
    scale_x_continuous(breaks = c(2,4,6,8,10)) +
    labs(y = "CFI", x = "Factors") +
    theme_bw()
  
  p3 <- out %>% 
    filter(func_type == "rmsea") %>% 
    ggplot(aes(x = factors, y = value)) + 
    geom_point(size = 1) +
    geom_line() +
    scale_x_continuous(breaks = c(2,4,6,8,10)) +
    scale_y_continuous(limits = c(0, 0.1)) +
    labs(y = "RMSEA", x = "Factors") +
    theme_bw()
  
  design = "12\n13\n44"
  cfi_factors_plot <- p1 + p2 + p3 + 
    guide_area() + 
    plot_layout(design = design, guides = "collect", heights = c(4,4,1)) +
    plot_annotation(tag_levels = "A") &
    theme(plot.tag = element_text(size = 9))
  
  ggsave(filename = here("img/cfi_factors_plot.png"),
         plot = cfi_factors_plot,
         dpi = 320,
         height = 4.5,
         width = 7)
}

knitr::include_graphics(here("img/cfi_factors_plot.png"))
```

The trade-off between CFI and RMSEA can also be seen by examining the differences between the $\mathbf{\Sigma}$ matrices produced by each of the model-error methods for a condition that often led to conflicting RMSEA and CFI values. \@ref(fig:c129-heat-maps) shows heat-maps of the $\mathbf{\Sigma}$ matrices for each of the five model-error methods. The $\bSigma$ matrices corresponded to the condition with ten orthogonal major common factors, five items per factor, weak factor loadings of 0.3, and Poor model fit.[^more-sigma-heat-maps] The figure shows clear differences between the model-error methods that incorporated a target CFI value ($\TKLcfi$ and $\TKLrmseacfi$) and the remaining model-error methods that only incorporated a target RMSEA value. The $\bSigma$ matrices produced by the $\TKLcfi$ and $\TKLrmseacfi$ methods included relatively small amounts of model error (in terms of the differences between the elements of $\bSigma$ and $\bOmega$), as can be seen by looking at their heat-map representations in \@ref(fig:c129-heat-maps). The block-diagonal structure of $\bOmega$ was well-preserved in both of the $\bSigma$ matrices, with no large off-block-diagonal correlations. On the other hand, the $\bSigma$ matrices produced by the other model-error methods had many off-block-diagonal correlations that were relatively large, many as large (or larger) than the block-diagonal correlations. The figure thus reaffirms the previously reported result that the model-error methods that included a target CFI value led to solutions that perturbed $\bOmega$ less than solutions from model-error methods that did not include a target CFI value.

[^more-sigma-heat-maps]: Heat-maps of both the $\bSigma$ and $\hat{\bSigma}$ matrices for the five model-error methods and conditions with ten orthogonal factors, weak factor loadings, Poor model fit, and five or ten items per factor are shown in \@ref(sigma-heat-maps).

```{r c129-heat-maps, out.width = "85%"}
#| fig.cap = "Correlation matrices with model error ($\\bm{\\Sigma}$) for each model-error method from the condition with ten orthogonal factors, five items per factor, weak factor loadings of 0.3, and Poor model fit. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."
if (make_plots) {
  # Function to make heatmap plots
  matrix_plot <- function(R, title="", fill_limits = "auto") {
    
    if (length(fill_limits) == 1) {
      if (fill_limits == "auto") {
        fill_limits = NULL
      }
    }
    
    R <- R %>%
      as_tibble(rownames = NA) %>%
      rownames_to_column(var = "Var1") %>%
      pivot_longer(cols = !matches("Var1"), 
                   names_to = "Var2", values_to = "value") %>%
      mutate(value = case_when(Var1 == Var2 ~ NA_real_,
                               TRUE ~ value),
             Var1 = paste0("V", formatC(as.numeric(str_extract(Var1, "[0-9]+")),
                                        width = 3,
                                        flag = "0")),
             Var2 = paste0("V", formatC(as.numeric(str_extract(Var2, "[0-9]+")),
                                        width = 3,
                                        flag = "0"))) %>%
      mutate(Var2 = fct_rev(as.factor(Var2)))
    
    ggplot(R, aes(x = Var1, y = Var2, fill = value)) +
      geom_tile() +
      colorspace::scale_fill_continuous_diverging("Purple-Green", 
                                                  limits = fill_limits) +
      # scale_fill_viridis_c(limits = fill_limits) +
      labs(title = title, fill = "Correlation") +
      theme(axis.text = element_blank(),
            axis.ticks = element_blank(), 
            axis.title = element_blank()) +
      coord_equal()
  }
  
  # Load RpopME matrices
  RpopME_matrices <- readRDS(here("../model-error-simulation/misc/RpopME_matrices.RDS"))
  
  mod <- simFA(Model = list(NFac = 10, NItemPerFac = 5, Model = "orthogonal"),
               Loadings = list(FacLoadRange = .4, FacLoadDist = "fixed"))
  
  out_129 <- RpopME_matrices$C129
  out_129$Rpop <- mod$Rpop
  
  # Compute estimated population matrices
  Rtheta_list <- purrrgress::pro_map(out_129[1:5], .f = function(R) {
    fout_129 <- factanal(covmat = R, 
                         factors = 10, 
                         rotation = "none", 
                         n.obs = 1000, 
                         control = list(nstart = 100))
    Fhat <- fout_129$loadings
    Rhat <- Fhat %*% t(Fhat)
    diag(Rhat) <- 1
    Rhat
  })
  
  p1 <- matrix_plot(out_129$RpopME_tkl_rmsea, 
                    title = latex2exp::TeX("$TKL_{RMSEA}$"),
                    fill_limits = c(-.3, .3))
  p2 <- matrix_plot(out_129$RpopME_tkl_cfi, 
                    title = latex2exp::TeX("$TKL_{CFI}$"),
                    fill_limits = c(-.3, .3))
  p3 <- matrix_plot(out_129$RpopME_tkl_rmsea_cfi, 
                    title = latex2exp::TeX("$TKL_{RMSEA/CFI}$"),
                    fill_limits = c(-.3, .3))
  p4 <- matrix_plot(out_129$RpopME_wb, 
                    title = "CB",
                    fill_limits = c(-.3, .3))
  p5 <- matrix_plot(out_129$RpopME_wb, 
                    title = "WB",
                    fill_limits = c(-.3, .3))
  
  c129_plot <- (p1 + p2 + p3 + p4 + p5) + 
    plot_layout(guides = "collect", nrow = 2) +
    plot_annotation()
  
  ggsave(filename = here("img/c129_R_plot_Sigma_only.png"),
         plot = c129_plot,
         dpi = 320,
         height = 5, width = 7.25)
}

knitr::include_graphics(here("img/c129_R_plot_Sigma_only.png"))
```

## Fit Indices Indicating Lack-of-Fit Between $\bSigma$ and $\bOmegaHat$

Although the lack-of-fit between $\bSigma$ and $\bOmega$ was of primary interest in this dissertation, the lack-of-fit between $\bSigma$ and $\bOmegaHat$ might also be of interest to some researchers. Recall that $\bOmegaHat$ denotes the implied population correlation matrix obtained by analyzing $\bSigma$ using the major-factor model. Thus, the fit index values based on $\bSigma$ and $\bOmegaHat$ represent the "best-case scenario" fit index values a researcher could expect to obtain because the lack-of-fit is due only to model approximation error without any sampling error.

Figures reporting the distributions of RMSEA, CFI, TLI, and CRMR values representing the lack-of-fit between $\bSigma$ and $\bOmegaHat$ (analogous to \@ref(fig:rmsea-distributions), \@ref(fig:cfi-distributions), \@ref(fig:tli-distributions), and \@ref(fig:crmr-distributions)) are provided in \@ref(sigma-hat-fit-indices). In addition to the distributions of the $\bOmegaHat$ fit indices, I was also interested in how they related to the corresponding fit indices indicating lack-of-fit between $\bSigma$ and $\bOmega$. In particular, I was interested in determining how $\textrm{RMSEA}_{\bOmegaHat}$ was related to RMSEA and how $\textrm{CFI}_{\bOmegaHat}$ was related to CFI.

### Differences between RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$

To better understand the relationship between the observed $\textrm{RMSEA}_{\bOmegaHat}$ and RMSEA values from the simulation study, \@ref(fig:rmsea-theta-vs-theta-hat) shows box-plots of the differences between $\textrm{RMSEA}_{\bOmegaHat}$ and RMSEA, denoted as 

\begin{equation}
\textrm{RMSEA}_{\Delta} =  \textrm{RMSEA} - \textrm{RMSEA}_{\bOmegaHat},
\end{equation}

conditioned on each of the model-error methods, factor loading strength, and level of model fit. The figure shows that the $\textrm{RMSEA}_{\bOmegaHat}$ values were almost always lower than the RMSEA values corresponding to the same $\bSigma$ matrix, resulting in positive values of $\textrm{RMSEA}_{\Delta}$. The vast majority of these differences were quite small, with a median difference of only 0.003. However, \@ref(fig:rmsea-theta-vs-theta-hat) shows that $\textrm{RMSEA}_{\Delta}$ was affected by the level of model fit such that $\textrm{RMSEA}_{\Delta}$ was largest when model fit was Poor and smallest when model fit was Very Good. In the most extreme instance, a $\bSigma$ matrix with Poor model fit had an $\textrm{RMSEA}_{\bOmegaHat}$ value of .006 and an RMSEA value of .308, indicating very different qualitative interpretations of model fit. Thus, although the differences between RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$ were often negligible, in some cases they were large enough to indicate completely different qualitative interpretations of model fit for RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$.

```{r rmsea-theta-vs-theta-hat, out.width = '100%'}
#| fig.cap = "Box-plots of the differences between RMSEA and $\\textrm{RMSEA}_{\\hat{\\Omega}}$ (denoted as $\\textrm{RMSEA}_{\\Delta}$) conditioned on model-error method, levels of model fit, and factor loading. Note that some outliers were omitted from the plot to aid visualization. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  rmsea_diff <- results_matrix %>%  
    filter(rmsea < 3) %>% 
    mutate(model_fit_rec = str_replace_all(model_fit_rec, " ", "~"),
           model_fit_rec = fct_relevel(model_fit_rec,
                                       "Fit:~Poor",
                                       "Fit:~Fair",
                                       "Fit:~Very~Good")) %>% 
    mutate(error_method_rec = fct_rev(error_method_rec)) %>% 
    ggplot(aes(x = rmsea - rmsea_thetahat, y = error_method_rec)) +
    geom_boxplot(width = .4, outlier.size = .4, outlier.alpha = .05, size = .4) +
    scale_x_continuous(limits = c(0, .075)) +
    scale_y_discrete(labels = scales::label_parse()) +
    labs(x = latex2exp::TeX("$RMSEA_{\\Delta}$"),
         y = "") +
    facet_grid(model_fit_rec ~ loading_rec,
               labeller = label_parsed) +
    theme_bw()
  
  ggsave(here("img/rmsea_diff.png"),
         rmsea_diff,
         dpi = 320,
         height = 5)
}

knitr::include_graphics(here("img/rmsea_diff.png"))
```

In addition to being affected by the level of model fit, \@ref(fig:rmsea-theta-vs-theta-hat) also shows that $\textrm{RMSEA}_{\Delta}$ was affected by the strength of the major common factor loadings. Moving across the figure from left to right, the differences between RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$ values tended to decrease as factor loadings increased from 0.3 to 0.8. The figure also shows that there was an interaction between model-error method and factor loading such that $\textrm{RMSEA}_{\Delta}$ decreased the most for the $\TKLrmsea$ and CB methods as factor loadings increased, compared to the alternative model-error methods.

Another important effect that can be seen in \@ref(fig:rmsea-theta-vs-theta-hat) is that the relationship between RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$ differed depending on the model-error method that was used. For instance, the CB method consistently produced $\bSigma$ matrices with small $\textrm{RMSEA}_{\Delta}$ values, whereas the other model-error methods often produced solutions with large $\textrm{RMSEA}_{\Delta}$ values. In theory, the CB method should have led to solutions with equal RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$ values because $\bOmegaHat = \bOmega$ when the CB method converges to a valid solution (see \@ref(population-model-fit)). However, in the present simulation study the CB method produced RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$ values that were often very close but not exactly equal (i.e., small but non-zero $\textrm{RMSEA}_{\Delta}$ values). The $\textrm{RMSEA}_{\Delta}$ values were largest when model fit was Poor and smallest when model fit was Very Good, as can be seen by moving from top to bottom of \@ref(fig:rmsea-theta-vs-theta-hat). Although the differences between RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$ were often non-zero, they were generally small enough to be of little concern to most researchers. It is likely that these discrepancies were due to the CB method converging to local minima, which Cudeck and Browne [-@cudeck1992] acknowledge is possible when the target RMSEA value is large.

The number of major common factors and the number of items per factor also had effects on $\textrm{RMSEA}_{\Delta}$. These effects are shown in \@ref(fig:rmsea-theta-vs-theta-hat-factors), which contains box-plots of $\textrm{RMSEA}_{\Delta}$ conditioned on number of factors and number of items per factor. The figure shows that the TKL-based methods were most affected by the number of factors and the number of items per factor, but that the direction of the effect depended on the particular method. For instance, the $\TKLrmsea$ method led to solutions with median $\textrm{RMSEA}_{\Delta}$ values that tended to increase along with the number of major factors when there were 15 items per factor. For conditions with only five items per factor, the median $\textrm{RMSEA}_{\Delta}$ first decreased as the number of factors increased from one to three, and then increased slightly as the number of factors increased further. In contrast to $\TKLrmsea$ method, the $\TKLrmseacfi$ and $\TKLcfi$ methods led to median $\textrm{RMSEA}_{\Delta}$ values that decreased as the number of factors increased for conditions with five items per factor. For conditions with 15 items per factor, the median $\textrm{RMSEA}_{\Delta}$ remained stable as the number of factors increased. For the CB and WB model-error methods, the number of factors had little effect on median $\textrm{RMSEA}_{\Delta}$. Finally, all of the model-error methods produced solutions with smaller median $\textrm{RMSEA}_{\Delta}$ values as the number of items per factor increased, with the exception of the $\TKLrmsea$ method for conditions with five or ten factors.

```{r rmsea-theta-vs-theta-hat-factors, out.width = '100%'}
#| fig.cap = "Box-plots of the differences between RMSEA and $\\textrm{RMSEA}_{\\hat{\\Omega}}$ (denoted as $\\textrm{RMSEA}_{\\Delta}$) conditioned on model-error method, number of factors, and number of items per factor. Note that some outliers were omitted from the plot to aid visualization. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  rmsea_diff_factors <- results_matrix %>%  
    filter(rmsea < 3) %>% 
    mutate(model_fit_rec = str_replace_all(model_fit_rec, " ", "~"),
           model_fit_rec = fct_relevel(model_fit_rec,
                                       "Fit:~Poor",
                                       "Fit:~Fair",
                                       "Fit:~Very~Good")) %>% 
    mutate(error_method_rec = fct_rev(error_method_rec)) %>% 
    ggplot(aes(x = rmsea - rmsea_thetahat, y = error_method_rec)) +
    geom_boxplot(width = .4, outlier.size = .4, outlier.alpha = .05, size = .4) +
    scale_x_continuous(limits = c(0, .075)) +
    scale_y_discrete(labels = scales::label_parse()) +
    labs(x = latex2exp::TeX("$RMSEA_{\\Delta}$"),
         y = "") +
    facet_grid(items_per_factor_rec ~ factors_rec,
               labeller = label_parsed) +
    theme_bw()
  
    ggsave(here("img/rmsea_diff_factors.png"),
         rmsea_diff_factors,
         dpi = 320,
         height = 3.5,
         width = 7)
}

knitr::include_graphics(here("img/rmsea_diff_factors.png"))
```

### Differences between CFI and $\textrm{CFI}_{\bOmegaHat}$

In addition to knowing how $\textrm{RMSEA}_{\Delta}$ values differed among the conditions in the simulation study, I was also interested in how the CFI and $\textrm{CFI}_{\bOmegaHat}$ values differed. As with RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$, the difference between CFI and $\textrm{CFI}_{\bOmegaHat}$ was denoted as

\begin{equation}
\textrm{CFI}_{\Delta} =  \textrm{CFI} - \textrm{CFI}_{\bOmegaHat}.
\end{equation}

```{r calculate-error-method-medians-cfi-delta, echo = FALSE, include = FALSE}
cfi_method_medians <- results_matrix %>% 
  group_by(error_method) %>% 
  summarise(median_cfi_delta = median(cfi - cfi_thetahat, na.rm = TRUE)) %>% 
  pivot_wider(names_from = error_method, values_from = median_cfi_delta)

results_matrix %>% 
  group_by(error_method, factors, items_per_factor) %>% 
  summarise(median_cfi_delta = median(cfi - cfi_thetahat, na.rm = TRUE)) %>% 
  pivot_wider(names_from = error_method, values_from = median_cfi_delta) %>% 
  apa_table(col.names = c("Factors", "Items/Factor", 
                          "$\\textrm{TKL}_{\\textrm{RMSEA}}$",
                          "$\\textrm{TKL}_{\\textrm{RMSEA/CFI}}$",
                          "$\\textrm{TKL}_{\\textrm{CFI}}$",
                          "CB",
                          "WB"),
            digits = c(0, 0, 2, 2, 2, 2, 2),
            align = "rrrrrrr",
            format.args = list(na_string = ""),
            col_spanners = list("Model-Error Method" = 3:7),
            caption = "A caption.")
```

\@ref(fig:cfi-theta-vs-theta-hat) contains box-plots of $\textrm{CFI}_{\Delta}$ conditioned on model-error method, level of model fit, and factor loading strength. Similar to the previous results for $\textrm{RMSEA}_{\Delta}$, the figure shows that $\textrm{CFI}_{\Delta}$ values were largest when model fit was Poor and smallest when model fit was Very Good. Moreover, the figure also shows differences among the model fit indices. In general, the $\TKLrmsea$ and WB methods led to the most extreme $\textrm{CFI}_{\Delta}$ values, whereas the $\TKLrmseacfi$, $\TKLcfi$, and CB methods often led to less extreme values, particularly in conditions with weak factor loadings or Poor model fit. In fact, the $\TKLrmseacfi$ and $\TKLcfi$ model-error methods sometimes led to $\textrm{CFI}_{\Delta}$ values that were less extreme than those from the CB method. As discussed in the previous section, this was somewhat surprising because the CB method was expected to produce solutions such that $\bOmega = \bOmegaHat$ and $\textrm{CFI}_{\Delta} = 0$ and suggests that the CB method often converged to local minima in conditions with weak factor loadings and Poor or Fair model fit.

```{r cfi-theta-vs-theta-hat, out.width = '100%'}
#| fig.cap = "Box-plots of the differences between CFI and $\\textrm{CFI}_{\\hat{\\bm{\\Omega}}}$ (denoted as $\\textrm{CFI}_{\\Delta}$) conditioned on model-error method, levels of model fit, and factor loading. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  cfi_diff <- results_matrix %>%  
    filter(rmsea < 3) %>% 
    mutate(model_fit_rec = str_replace_all(model_fit_rec, " ", "~"),
           model_fit_rec = fct_relevel(model_fit_rec,
                                       "Fit:~Poor",
                                       "Fit:~Fair",
                                       "Fit:~Very~Good")) %>% 
    mutate(error_method_rec = fct_rev(error_method_rec)) %>% 
    ggplot(aes(x = cfi - cfi_thetahat, y = error_method_rec)) +
    geom_boxplot(width = .4, outlier.size = .4, outlier.alpha = .05, size = .4) +
    # scale_x_continuous(limits = c(0, .075)) +
    scale_y_discrete(labels = scales::label_parse()) +
    labs(x = latex2exp::TeX("$CFI_{\\Delta}$"),
         y = "") +
    facet_grid(model_fit_rec ~ loading_rec,
               labeller = label_parsed) +
    theme_bw()
  
  ggsave(here("img/cfi_diff.png"),
         cfi_diff,
         dpi = 320,
         height = 5)
}

knitr::include_graphics(here("img/cfi_diff.png"))
```

In addition to the effects of factor loading and model fit, $\textrm{CFI}_{\Delta}$ was also affected by the number of factors and items per factor. To help understand these effects, \@ref(fig:cfi-theta-vs-theta-hat-nfac) shows box-plots of $\textrm{CFI}_{\Delta}$ conditioned on model-error method, number of factors, and number of items per factor. The figure shows that $\textrm{CFI}_{\Delta}$ tended to increase as the number of factors increased for the $\TKLrmsea$ and WB methods, but had only a small effect (or no effect at all) on $\cfiDelta$ for the other model-error methods. Considering the effect of number of items per factor, \@ref(fig:cfi-theta-vs-theta-hat-nfac) shows that the number of items per factor had almost no effect on the median $\cfiDelta$ values for each of the model-error methods except for the $\TKLrmsea$ method in conditions with ten factors. However, there was more variation in $\cfiDelta$ values for the $\TKLrmsea$ method in conditions with 15 items per factor compared to conditions with five items per factor. In contrast, the CB and WB methods led to less variable $\cfiDelta$ values as the number of items per factor increased from five to 15.

```{r cfi-theta-vs-theta-hat-nfac, out.width = '100%'}
#| fig.cap = "Box-plots of the differences between CFI and $\\textrm{CFI}_{\\hat{\\Omega}}$ (denoted as $\\textrm{CFI}_{\\Delta}$) conditioned on model-error method, number of factors, and number of items per factor. Note that some outliers were omitted from the plot to aid visualization. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  cfi_diff_factors <- results_matrix %>%  
    filter(rmsea < 3) %>% 
    mutate(model_fit_rec = str_replace_all(model_fit_rec, " ", "~"),
           model_fit_rec = fct_relevel(model_fit_rec,
                                       "Fit:~Poor",
                                       "Fit:~Fair",
                                       "Fit:~Very~Good")) %>% 
    mutate(error_method_rec = fct_rev(error_method_rec)) %>% 
    ggplot(aes(x = cfi - cfi_thetahat, y = error_method_rec)) +
    geom_boxplot(width = .4, outlier.size = .4, outlier.alpha = .05, size = .4) +
    scale_y_discrete(labels = scales::label_parse()) +
    labs(x = latex2exp::TeX("$CFI_{\\Delta}$"),
         y = "") +
    facet_grid(items_per_factor_rec ~ factors_rec,
               labeller = label_parsed) +
    theme_bw()
  
    ggsave(here("img/cfi_diff_factors.png"),
         cfi_diff_factors,
         dpi = 320,
         height = 3.5,
         width = 7)
}

knitr::include_graphics(here("img/cfi_diff_factors.png"))
```

Overall, the results of the present simulation study suggested that any of the model-error methods evaluated in this simulation study could provide a useful starting point for researchers who would like to generate $\bSigma$ matrices with particular $\RMSEA_{\bOmegaHat}$ or $\CFI_{\bOmegaHat}$ values. However, all of the model-error methods (including the CB methods) sometimes produced solutions with large $\rmseaDelta$ or $\cfiDelta$ values, particularly in conditions with Poor model fit and weak factor loadings. Thus, researchers interested in generating solutions with particular $\textrm{RMSEA}_{\bOmegaHat}$ or $\textrm{CFI}_{\bOmegaHat}$ values should check the fit indices of each solution and reject solutions with fit indices outside of the desired range. Because the fit indices based on $\bOmegaHat$ almost always indicated better fit than the indices based on $\bOmega$, it might also be useful to specify target RMSEA values that are slightly higher (or CFI values that are slightly lower) than the desired $\textrm{RMSEA}_{\bOmegaHat}$ or $\textrm{CFI}_{\bOmegaHat}$ values.

## TKL Method Recovery of Model Fit Indices

The TKL-based model-error methods were designed to find a correlation matrix with model error that had either an RMSEA value or a CFI value (or both) that were close to a specified value. In a previous section, I reported RMSEA and CFI values for all of the model-error methods (including the TKL-based methods) and compared them to their target values. When only one target model-fit index was used (i.e., $\TKLrmsea$ or $\TKLcfi$), the observed RMSEA and CFI values were very close to the target values. However, when both RMSEA and CFI fit index targets were used simultaneously with the $\TKLrmseacfi$ method, many solutions failed to have RMSEA and CFI values that were both very close to the target values. This could indicate that the optimization procedure was not working well and often failed to find an optimal solution. However, an alternative explanation is that some combinations of RMSEA and CFI might not have been possible for conditions with major factor loadings fixed at a particular value, or with a certain number of major common factors, etc. 

To determine whether the $\TKLrmseacfi$ was able to find near-optimal solutions, I first needed to find combinations of RMSEA and CFI values that were known to be possible. If the $\TKLrmseacfi$ method was able to produce solutions with RMSEA and CFI values that were close to these target RMSEA and CFI values, it would suggest that the $\TKLrmseacfi$ was working well. More importantly, if the $\TKLrmseacfi$ method was not able to produce solutions with fit indices close to these known-to-be-possible combinations of RMSEA and CFI, it would suggest that the $\TKLrmseacfi$ cannot be relied on to produce optimal or near-optimal solutions. 

To find RMSEA and CFI value combinations that were known to be possible, I used the implementation of the traditional TKL method implemented in the `simFA()` function to generate a correlation matrix with model error for every condition in the simulation design. Next, I computed the RMSEA and CFI values for each simulated correlation matrix. I then used those values as target RMSEA and CFI values for the $\TKLrmseacfi$ method and generated 50 correlation matrices with model error for each condition using the `noisemaker()` function.[^fit-recovery-code] 

[^fit-recovery-code]: Code for this simulation study is provided in \@ref(check-model-fit-recovery-code).

The results from this small simulation study are reported in \@ref(fig:fit-index-recovery), which shows the known-to-be-possible target values of RMSEA and CFI (indicated by solid black lines) and the observed RMSEA and CFI values from the $\TKLrmseacfi$ method for each condition. The figure shows that the observed RMSEA and CFI values were nearly identical to the target values for most conditions. The conditions where the observed fit indices had the most variability were conditions with one or three factors, five items, and weak factor loadings. However, even in those conditions many solutions had CFI and RMSEA values that were nearly identical to the target values. These results indicate that using the $\TKLrmseacfi$ method repeatedly with different initial values of $\mathbf{W}$,  $\epsilon$, and $\nu_{\textrm{e}}$ and then selecting the solution with RMSEA and CFI values closest to the target values seems to be an effective approach for obtaining optimal (or nearly-optimal) solutions.

```{r fit-index-recovery}
#| fig.cap = "Observed RMSEA and CFI values for the $\\textrm{TKL}_{\\textrm{RMSEA/CFI}}$ model-error method and the corresponding known-to-be-possible target RMSEA and CFI values (indicated by the black lines), conditioned on number of factors, number of items per factor, factor loading strength, and factor correlation. Note that some levels were omitted to conserve space."
if (make_plots) {
  noisy_data <- readRDS(here("data/noisy_data.RDS"))
  
  target_values <- noisy_data %>%
    filter(factor_corr != "Factor Cor.: 0.3",
           factor_loading != "Loading: 0.6") %>%
    select(factors, items_per_factor, factor_corr, factor_loading,
           target_rmsea, target_cfi) %>%
    distinct()
  
  noisy_data %>%
    filter(factor_corr != "Factor Cor.: 0.3",
           factor_loading != "Loading: 0.6") %>% 
    ggplot(aes(x = rmsea, y = cfi)) +
    geom_point(alpha = .3, size = 1) +
    geom_hline(aes(yintercept = target_cfi), data = target_values,
               size = .25) +
    geom_vline(aes(xintercept = target_rmsea), data = target_values,
               size = .25) +
    facet_grid(factors * factor_loading ~ items_per_factor * factor_corr) +
    labs(x = "RMSEA", y = "CFI") +
    theme_bw()
  
  ggsave(filename = "rmsea_cfi_recovery.png",
         path = here("img"),
         plot = last_plot(),
         dpi = 320,
         height = 10,
         width = 7)
}

knitr::include_graphics(here("img/rmsea_cfi_recovery.png"))
```

<!-- Need to add:
1. [x] Experiments showing that the TKL optimization methods can recover true model fit parameters.
2. [o] Number of times that the CB method *doesn't* produced an optimal solution (RMSEA_theta_hat should be 0).
3. [x] Experiments showing whether reducing the TKL penalty improves convergence.
4. [x] Discussion of "hat" fit indices? Figures and tables in Appendix, but do I need a discussion of them in the main body of the paper? Perhaps a question for Niels.

- Check that RMSEA always is reported with a leading zero, (i.e., 0.025) and that CFI is not (i.e., .95).
- Define terms at the beginning. Factor loadings are always major common factor loadings. Items per factor is more precisely indicators per factor.
-->

```{r}
if (make_plots) {
  cb_nonconverged <- results_matrix %>% 
    filter(error_method == "CB") %>% 
    mutate(delta = rmsea_thetahat - target_rmsea) %>%
    group_by(factors, items_per_factor_rec, loading_numeric, model_fit_rec) %>% 
    summarise(percent_nonconverged = mean(abs(delta) >= .01, na.rm = TRUE)) %>% 
    mutate(factors = as.factor(factors)) %>% 
    ggplot(aes(x = loading_numeric, y = percent_nonconverged, color = factors,
               group = factors, shape = factors)) +
    geom_point() +
    geom_line() +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    facet_grid(items_per_factor_rec ~ model_fit_rec) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    labs(x = "Loading", y = "Percent Non-Converged", color = "Factors",
         shape = "Factors") +
    theme_bw()
  
  ggsave(plot = cb_nonconverged,
         filename = here("img/cb_nonconverged.png"),
         dpi = 320,
         height = 4, 
         width = 6.5)
}

# knitr::include_graphics(here("img/cb_nonconverged.png"))
```


