
# Results {#results}

```{r read-results}
results_matrix <- readRDS(here("data/results_matrix_with_conditions.RDS"))
```

In the previous section, I described the simulation study I conducted to learn more about the behavior of different methods for generating error-perturbed population covariance (correlation) matrices. The simulation study included three model-error methods---the (single- and multiple-target) TKL method, the CB method, and the WB method---and was designed to answer two primary questions. 

First, I wanted to know whether different model-error methods led to different values of the CFI, TLI, and CRMR fit indices when used with the same error-free models and target RMSEA values. If the model-error methods led to systematically different values on the alternative fit indices when matched on RMSEA and all other characteristics, it would suggest that they are not exchangeable. In that case, researchers conducting simulation studies would have to consider which of the model-error methods most closely approximates the model error process they are trying to simulate. Moreover, such results would highlight the importance of reporting fit indices other than RMSEA when simulating imperfect models. If there were no meaningful differences among the methods, it would indicate that the choice of one particular model-error method over another (among the methods considered here) is not an important variable in the design of simulation studies.

A second purpose of the study was to evaluate the effectiveness of the proposed multiple-target method for generating correlation matrices with model error that had RMSEA and CFI values that were close to the specified target values. It was not expected that the algorithm would be able to produce correlation matrices with RMSEA and CFI values that were very close to the target values for all of the major-factor population models because of the relationship between RMSEA, CFI, and population model characteristics [@lai2016]. Therefore, I used the absolute deviation between the observed and target RMSEA and CFI values to compare the results from the multiple-target TKL method to the results from the CB and WB methods used in Study 1.

The remainder of the section is structured as follows. First, I report how many of the simulated matrices had properties that would make them unsuitable for use in a simulation study. XXX...

## Indefinite Matrices (CB)

```{r add-is-indefinite-variable}
results_matrix <- mutate(
  results_matrix,
  error = case_when(is.na(error) ~ " ",
                    TRUE ~ error)  
) %>% mutate(
  is_indefinite = str_detect(error, "indefinite")
)

num_cb <- filter(results_matrix, error_method == "CB") %>% nrow()
num_indefinite <- filter(results_matrix,
                         error_method == "CB",
                         is_indefinite == TRUE) %>% nrow()
percent_indefinite <- round((num_indefinite / num_cb) * 100, 1)
```

One drawback of the CB model-error method is that the resulting correlation matrix with model error can be indefinite (i.e. having one or more negative eigenvalues) when the specified target RMSEA value is large [@cudeck1992]. These matrices are undesirable because correlation and covariance matrices are, by definition, at least positive semi definite (i.e., having strictly non-negative eigenvalues). Of the `r num_cb` correlation matrices with model error that were generated using the CB method, `r num_indefinite` (`r percent_indefinite`%) were indefinite. However, indefinite solutions were much more common for some conditions of the simulation design than other. \@ref(fig:fig-percent-indefinite-matrices) shows the percent of indefinite CB solutions for each level of model fit, number of items per factor, number of factors, and factor loading strength are shown in \@ref(fig:fig-percent-indefinite-matrices). (Exact percentages are reported in \@ref(tab:tab-percent-indefinite-matrices)). The figure shows at least three notable trends. First, the percent of indefinite solutions increased as model fit degraded. Second, the percent of indefinite solutions increased as the total number of items increased (i.e., as the number of factors and items per factor increased). Finally, \@ref(fig:fig-percent-indefinite-matrices) shows that the percent of indefinite solutions increased as factor loadings increased. 

In the best-case scenarios, conditions corresponding to models with 25 items or fewer led to indefinite solutions very infrequently (in less than 1% of cases). On the other hand, conditions with Poor model fit and 45 items or more led to indefinite correlation matrices in more than 90% of cases. These results show that the CB method would be an inefficient way to simulate positive semi definite population correlation or covariance matrices with model error when input matrices are large and the target RMSEA value is relatively large. 

Although inefficient, a potential strategy for dealing with indefinite solutions when using the CB method is to simply generate solutions using the CB method until a sufficient number of positive semi definite solutions have been obtained. However, the amount of time taken by the CB method increases quickly as the number of items increase, making the oversampling strategy impractical for problems with many items. In fact, using the CB method to generate even a small number of solution matrices becomes impractical for large input matrices. This is shown in \@ref(fig:cb-completion-time), which is a plot of the completion time for the CB method when applied to a one-factor model with salient loadings fixed at .6 and the number of items varying between 5 and 120. Using a computer with an Intel Core i5-4570 3.20GHz CPU and 16GB of RAM, the CB method took just over 30 seconds to complete (on average) for an input correlation matrix with 65 items. For an input matrix with 115 items, the CB method took approximately four and a half minutes to complete. Such long completion times are often impractical for large simulation studies, particularly if indefinite solutions are discarded. In fact, I had to skip using the CB method in simulation conditions with 10 factors and 15 items per factor (150 items) because those conditions would have taken an impractical amount of time to complete. Timing a single example, the CB method took 15 minutes and 48 seconds to complete with a 150-item input correlation matrix. At that rate, it would have taken almost 11 days to complete one (out of 36) conditions of the simulation design with 150 items. 

(ref:cb-indefinite-table-caption) The percent of Cudeck and Browne (CB) model-error method solutions that were indefinite.

```{r tab-percent-indefinite-matrices}
cb_indefinite_table <- results_matrix %>%
  filter(error_method == "CB") %>%
  group_by(factors, items_per_factor, 
           loading_numeric, model_fit) %>%
  summarise(mean_indefinite = mean(is_indefinite)) %>%
  ungroup() %>%
  mutate(mean_indefinite = case_when(factors == 10 & items_per_factor == 15 ~ NA_real_,
                                     TRUE ~ mean_indefinite)) %>%
  pivot_wider(values_from = mean_indefinite,
              names_from = factors)

cb_indefinite_table %>%
  mutate(across(.cols = c(`1`:`5`), ~ . * 100)) %>%
  apa_table(col.names = c("Items/Factor", "Loading", "Model Fit", 
                          "1", "3", "5", "10"),
            digits = c(0, 1, 0, 1, 1, 1, 1),
            align = "rrlrrrr",
            col_spanners = list("Factors" = c(4, 7)),
            format.args = list("na_string" = "---"),
            caption = "(ref:cb-indefinite-table-caption)",
            label = "cb-indefinite",
            note = "The Cudeck-Browne method was not used for conditions with 10 major factors and 15 items per factor because it was prohibitively slow for those conditions.")
```

```{r fig-percent-indefinite-matrices, fig.cap = "The percent of Cudeck-Browne (CB) method solutions that were indefinite, conditioned on number of factors, factor loading, number of items per factor, and model fit."}
if (make_plots) {
  cb_percent_indefinite <- results_matrix %>%
    filter(error_method == "CB") %>%
    mutate(error= case_when(is.na(error) ~ " ",
                            TRUE ~ error),
           items_per_factor = as.factor(items_per_factor),
           factors = as.factor(factors),
           model_fit = factor(model_fit,
                              levels = c("Very Good", "Fair", "Poor"),
                              labels = c("Fit: Very Good", 
                                         "Fit: Fair", 
                                         "Fit: Poor"))) %>%
    group_by(factors, items_per_factor_rec, 
             loading_numeric, model_fit) %>%
    summarise(mean_indefinite = mean(
      str_detect(error, "indefinite")
    )) %>%
    ungroup() %>%
    ggplot(aes(y = mean_indefinite, x = loading_numeric,
               color = factors,
               shape = factors, 
               linetype = factors,
               group = factors)) +
    geom_line() + 
    geom_point() +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
    scale_x_continuous(breaks = c(.4, .6, .8)) +
    facet_grid(items_per_factor_rec ~ model_fit) +
    theme_bw() +
    labs(color = "Factors",
         fill = "Factors",
         linetype = "Factors",
         shape = "Factors",
         y = "Indefinite Solutions",
         x = "Factor Loading") +
    theme(legend.position = "bottom")
  
  ggsave(filename = here("img/cb_percent_indefinite.png"),
         plot = cb_percent_indefinite,
         dpi = "retina",
         width = 6,
         height = 5)
}

knitr::include_graphics(here("img/cb_percent_indefinite.png"))
```

```{r cb-completion-time, fig.cap = 'The amount of time (in minutes) taken to generate a single correlation matrix with model error using the CB method. Completion times were recorded 10 times for single-factor models with salient factor loadings fixed at .6 and number of items (\\emph{p}) varying between 5 and 115. The dashed black line is the loess regression line.', align = "center"}
if (make_plots) {
  set.seed(123)
  max_p <- 115
  cb_times <- data.frame(p = seq(5, max_p, by = 10), t = NA)
  safe_cb <- possibly(cb, otherwise = NA)
  
  t <- pblapply(
    X = 1:nrow(cb_times),
    FUN = function(i) {
      mod <- simFA(Model = list(NFac = 1, NItemPerFac = cb_times$p[i]),
                   Loadings = list(FacLoadRange = .6,
                                   FacLoadDist = "fixed"))
      t <- microbenchmark(
        safe_cb(mod, target_rmsea = 0.05),
        times = 10,
        unit = "milliseconds"
      )
      
      t
    }
  )
  
  names(t) <- cb_times$p
  times <- map_dfr(t, ~ .x$time)
  times <- times %>%
    as_tibble() %>%
    pivot_longer(cols = everything(),
                 names_to = "p",
                 values_to = "ns") %>%
    mutate(p = as.numeric(p)) %>%
    mutate(minutes = (ns * 1e-8) / 60) %>%
    select(-ns)
  
  times <- times %>% add_case(p = 150, minutes = 15.8)
  
  cb_time_plot <- ggplot(times, aes(x = p, y = minutes)) +
    geom_point(alpha = 0.3) +
    geom_smooth(size = .4, color =" black", linetype = "dashed", 
                method = "loess", level = 0, span = 0.35) +
    scale_x_continuous(breaks = seq(5, 150, by = 10)) +
    labs(x = "p",
         y = "Completion Time (minutes)",
         title = "CB method completion time",
         subtitle = "One-factor models with salient loadings of .6 and RMSEA = 0.05") +
    theme_bw()
  
  saveRDS(times, here("misc/cb_completion_times.RDS"))
  
  ggsave(filename = here("img", "cb_time_plot.png"),
         plot = cb_time_plot,
         dpi = 320,
         height = 5,
         width = 6)
}

knitr::include_graphics(path = here("img", "cb_time_plot.png"))
```

## L-BFGS-B Non-convergence (Genetic Algorithm)

The default optimization method for the multiple-target TKL method was L-BFGS-B. In most cases, this method worked well and converged relatively quickly to a solution. However, there were a small number of cases where the L-BGFS-B method failed to converge. Specifically, the L-BFGS-B method failed to converge 14 times ($<1$%) when the target CFI and RMSEA values were both used and when model fit was Poor. The L-BFGS-B non-convergence rates for the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ method by number of factors, number of items per factor, and model fit are shown in \@ref(tab:tab-l-bfgs-b-convergence). Non-convergence was also somewhat more likely for conditions with few factors compared to conditions with many factors.

Although non-convergence of the L-BFGS-B algorithm was rare, a question that arose was whether the genetic algorithm that was used as a fallback option led to similar results compared to cases where the L-BFGS-B algorithm converged. This question was difficult to answer statistically because non-convergence occurred so infrequently. To get a general sense of whether results for cases where the L-BFGS-B algorithm converged or did not converge were similar, I plotted the CFI and RMSEA values for all converged and non-converged cases in conditions where the multiple-target TKL method was used with target CFI and RMSEA values and model fit was Poor. \@ref(fig:comparison-of-converged-vs-non-converged) shows that cases where the L-BFGS-B algorithm did not converge (shown in black) led to similar RMSEA and CFI values compared to cases where the algorithm converged (shown in gray). Thus, using a genetic algorithm seemed to provide reasonable results in the few cases where the L-BFGS-B algorithm failed to converge (albeit much more slowly than the L-BFGS-B algorithm).

```{r comparison-of-converged-vs-non-converged, fig.cap = "RMSEA and CFI values for cases where the model-error method was $\\textrm{TKL}_{\\textrm{RMSEA/CFI}}$ and model fit was Poor. Grey dots indicate cases where the L-BFGS-G algorithm converged; black dots indicate cases that did not converge and where a genetic algorithm was used instead."}
if (make_plots) {
  nonconverged_data <- results_matrix %>%
    filter(error_method == "TKL (RMSEA/CFI)") %>%
    mutate(converged = 
             warning != "simpleWarning: `optim()` failed to converge, using `ga()` instead.\n") %>%
    mutate(converged = case_when(is.na(converged) ~ TRUE,
                                 TRUE ~ converged)) %>%
    mutate(converged = factor(converged, 
                              levels = c(TRUE, FALSE), 
                              labels = c("Yes", "No")))
  
  nonconverged_rmsea_cfi <- nonconverged_data %>%
  filter(model_fit == "Poor") %>%
  ggplot(aes(y = cfi, x = rmsea)) +
  geom_point(size = 1, alpha = 0.8) +
  gghighlight(converged == "No", use_direct_label = FALSE,
              unhighlighted_params = list(alpha = 0.1)) +
  geom_vline(xintercept = 0.09, size = .25, color = "grey10", linetype = "dashed") +
  geom_hline(yintercept = 0.90, size = .25, color = "grey10", linetype = "dashed") +
  theme_bw() +
  labs(x = "RMSEA", y = "CFI", color = "L-BFGS-B Convergence")

  ggsave(filename = here("img/nonconverged_rmsea_cfi.png"),
         dpi = 320,
         width = 4.25, 
         height = 4)
}

knitr::include_graphics(here("img/nonconverged_rmsea_cfi.png"))
```

(ref:bfgs-convergence-table-caption) The percent of cases in each combination of conditions where the L-BFGS-B algorithm did not converge after 100 random starts and genetic optimization was used instead.

```{r tab-l-bfgs-b-convergence, results='asis'}
bfgs_nonconvergence_table <- results_matrix %>%
  filter(error_method == "TKL (RMSEA/CFI)") %>%
  mutate(warning = case_when(is.na(warning) ~ " ",
                             TRUE ~ warning)) %>%
  group_by(factors, model_fit, items_per_factor) %>%
  summarise(mean_nonconvergence = mean(
    warning == "simpleWarning: `optim()` failed to converge, using `ga()` instead.\n"
  ) * 100) %>%
  ungroup() %>%
  pivot_wider(values_from = mean_nonconvergence,
              names_from = model_fit)

bfgs_nonconvergence_table %>%
  papaja::apa_table(col.names = c("Factors", "Items/Factor",
                                  "Very Good",
                                  "Fair",
                                  "Poor"),
                    digits = c(0, 0, 1, 1, 1),
                    align = "rrrrr",
                    font_size = "small",
                    col_spanners = list("Model Fit" = c(3, 5)),
                    label = "bfgs-convergence-table",
                    caption = "(ref:bfgs-convergence-table-caption)")
```

## Major Minor Factors (W Matrices)

Recall that the multiple-objective TKL method included an optional penalty that heavily penalized cases that had strong minor factors. Specifically, the penalty was applied if any minor factor had two or more absolute factor loadings greater than or equal to a specified value. The purpose of the penalty was to avoid introducing minor factors that might be more accurately characterized as major factors. To determine whether the penalty was effective at helping avoid major minor factors, I checked each of the minor factor loading ($\mathbf{W}$) matrices to determine whether any minor factor had two or more loadings greater than .3 (in absolute value). 

The percent of cases where the constraints on $\mathbf{W}$ were violated for each level of number of factors, number of items per factor, factor loading strength, and model fit are shown in \@ref(fig:w-major-factors-plot) and reported in \@ref(tab:tab-major-minor-factors). Only results for the $\textrm{TKL}_{\textrm{RMSEA}}$ were included because the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ and $\textrm{TKL}_{\textrm{CFI}}$ error methods seldom led to solutions that violated the constraints on $\mathbf{W}$. (Only 24 out of 90,000 cases had violated $\mathbf{W}$ constraints for the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ and $\textrm{TKL}_{\textrm{CFI}}$ methods combined.) \@ref(fig:w-major-factors-plot) shows that the $\mathbf{W}$ constraints were violated most often when model fit was Fair or Poor, factor loadings were relatively low, and there were many total items (i.e., many factors and items per factor).

To understand why certain conditions led to more $\mathbf{W}$ constraint violations than others, it is helpful to understand the relationship between those conditions and the TKL parameters ($\epsilon$ and $\nu_{\textrm{e}}$). \@ref(fig:eps-and-v-values-by-W-violations) shows the $\epsilon$ and $\nu_{\textrm{e}}$ values for each of the TKL model-error methods, conditioned on factor loading strength, number of factors, and number of items per factor. To conserve space, only conditions with Poor model fit and 1, 5, or 10 major factors were included in the figure. Overall, the figure shows that there was a trade-off between $\epsilon$ and $\nu_{\textrm{e}}$ such that higher values of $\nu_{\textrm{e}}$ were related to lower values of $\epsilon$ (and *vice versa*). Moreover, \@ref(fig:eps-and-v-values-by-W-violations) shows that the distributions of $\epsilon$ and $\nu_{\textrm{e}}$ differed depending on which TKL variant was used. The differences between the error-method variants were largest when the there were many items (i.e., many items and many items per factor) and when factor loadings were relatively weak. Under those circumstances, the $\textrm{TKL}_\textrm{RMSEA}$ method led to higher values of $\nu_{\textrm{e}}$ than the $\textrm{TKL}_\textrm{RMSEA/CFI}$ or $\textrm{TKL}_\textrm{CFI}$ methods. This effect suggests that higher values of $\nu_{\textrm{e}}$ were required to produce solutions with RMSEA values close to .09 when there are many items. On the other hand, the results for the $\textrm{TKL}_\textrm{RMSEA/CFI}$ and $\textrm{TKL}_\textrm{CFI}$ methods indicated that much lower $\nu_{\textrm{e}}$ values were required to obtain CFI values close to .90 when there were many items.

The apparent trade-off between $\epsilon$ and $\nu_{\textrm{e}}$ values makes sense when you consider their roles in the TKL method. A high $\nu_{\textrm{e}}$ value indicated that much of the unique variance would be assigned to the minor common factors. If the value of $\epsilon$ was also high, it indicated that the first few minor factors would account for most of the minor factor variance. Therefore, the $\mathbf{W}$ constraints were more likely to be violated when both $\epsilon$ and $\nu_{\textrm{e}}$ were high and less likely to be violated if either parameter was low. in \@ref(fig:eps-and-v-values-by-W-violations), which shows the values of $\epsilon$ and $\nu_{\textrm{e}}$ produced by the three TKL variants for conditions with Poor model fit, 10 factors, 15 items per factor, and factor loadings of .8. Each point (corresponding to a single case) was colored according to whether or not the $\mathbf{W}$ constraints were violated. The figure shows that the $\mathbf{W}$ constraints were violated when the values of $\epsilon$ and $\nu_{\textrm{e}}$ were both above some threshold, which only happened when the $\textrm{TKL}_\textrm{RMSEA}$ was used.

(ref:major-minor-factors) The percent of cases that violated the minor common factor loading constraints for each level of number of factors, items per factor, factor loading, and model fit when the $\textrm{TKL}_{\textrm{RMSEA}}$ method was used.

```{r tab-major-minor-factors, results='asis'}
percent_w_major_factors_table <- results_matrix %>%
  filter(error_method == "TKL (RMSEA)") %>%
  group_by(factors, items_per_factor, loading_numeric, model_fit) %>%
  mutate(w_has_major_factors = fn_value >= 1e06) %>%
  summarise(mean_w_major_factors =
              mean(w_has_major_factors, na.rm = TRUE) * 100) %>%
  ungroup() %>%
  pivot_wider(values_from = mean_w_major_factors,
              names_from = "model_fit")

percent_w_major_factors_table %>%
  apa_table(col.names = c("Factors", "Items per Factor", "Loading",
                          "Very Good", 
                          "Fair", 
                          "Poor"),
            col_spanner = list("Model Fit" = c(4, 6)),
            digits = c(0, 0, 1, 1, 1, 1),
            align = "rrlrrr",
            caption = "(ref:major-minor-factors)",
            label = "major-minor-factors",
            font_size = "small")
```

```{r w-major-factors-plot, fig.cap = "The percent of cases where the constraints on $\\mathbf{W}$ were violated when the $\\textrm{TKL}_{\\textrm{RMSEA}}$ model-error method was used, conditioned on number of factors, number of items per factor, factor loading, and model fit."}
if (make_plots) {
  percent_w_major_factors_table <- results_matrix %>%
    filter(!(error_method %in% c("CB", "WB"))) %>%
    group_by(factors, items_per_factor_rec, loading_numeric, 
             model_fit_rec, error_method_rec, error_method) %>%
    mutate(w_has_major_factors = fn_value >= 1e06) %>%
    summarise(mean_w_major_factors =
                mean(w_has_major_factors, na.rm = TRUE))
  
  w_major_factors_plot <- percent_w_major_factors_table %>%
    filter(error_method == "TKL (RMSEA)") %>%
    mutate(factors = as.factor(factors)) %>%
    ggplot(aes(y = mean_w_major_factors, 
               x = loading_numeric, 
               color = factors, 
               linetype = factors, 
               shape = factors, 
               group = factors)) +
    geom_line() +
    geom_point() +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    scale_y_continuous(label = scales::percent_format(accuracy = 1)) +
    scale_x_continuous(breaks = c(.4, .6, .8)) +
    facet_grid(items_per_factor_rec ~ model_fit_rec) +
    theme_bw() +
    labs(x = "Loadings", y = TeX("Cases with Violtated $W$ Constraints"), 
         color = "Factors", linetype = "Factors", shape = "Factors") +
    theme(legend.position = "bottom")
  
  ggsave(filename = here("img/major_minor_factors.png"),
         w_major_factors_plot,
         dpi = 320,
         width = 6, 
         height = 5)
}

knitr::include_graphics(here("img/major_minor_factors.png"))
```

```{r eps-and-v-values-by-W-violations, fig.cap = "Values of the TKL parameters ($\\epsilon$ and $\\nu_{\\textrm{e}}$) by model-error method, number of factors, number of items per factor, and factor loading strength when model fit was Poor. Results for conditions with three or five major factors were omitted to conserve space. TKL = Tucker, Koopman, and Linn."}
if (make_plots) {
  eps_and_nu_plot <- results_matrix %>%
    mutate(w_has_major_factors = factor(w_has_major_factors, 
                                        levels = c(FALSE, TRUE),
                                        labels = c("No", "Yes"))) %>%
    filter(model_fit == "Poor",
           factors %in% c(1,10),
           !(error_method %in% c("CB", "WB"))) %>%
    ggplot(aes(y = v, x = eps, color = error_method)) +
    geom_point(alpha = .1, size = .75) +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    scale_x_continuous(breaks = c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)) +
    facet_grid(factors_rec * items_per_factor_rec ~ loading_rec) +
    labs(y = TeX("$\\nu_e$"), x = TeX("$\\epsilon$"), color = "Error Method") +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +
    theme_bw() +
    theme(legend.position = "bottom")

  ggsave(filename = here("img/eps_and_nu_w_violations.png"),
         eps_and_nu_plot,
         dpi = 320,
         height = 7, width = 6)
}

knitr::include_graphics(here("img/eps_and_nu_w_violations.png"))
```

```{r w-violation-example, fig.cap = "The distribution of $\\epsilon$ and $\\nu_{\\textrm{e}}$ (and whether or not $\\mathbf{W}$ constraints were violated) for conditions with Poor model fit, 10 factors, 15 items per factor, and factor loadings of .8. TKL = Tucker, Koopman, and Linn"}
if (make_plots) {
  eps_and_nu_violation_plot <- results_matrix %>%
    mutate(w_has_major_factors = factor(w_has_major_factors, 
                                        levels = c(FALSE, TRUE),
                                        labels = c("No", "Yes"))) %>%
    filter(model_fit == "Poor",
           factors == 10,
           items_per_factor == 15,
           loading_numeric == 0.8,
           !(error_method %in% c("CB", "WB"))) %>%
    ggplot(aes(y = v, x = eps, color = w_has_major_factors)) +
    geom_point(alpha = .3) +
    scale_color_manual(values = c("#CCCCCC", "#440154FF")) +
    facet_grid(~ error_method_rec, labeller = label_parsed) +
    labs(y = TeX("$\\nu_{e}$"), x = TeX("$\\epsilon$"), 
         color = "W Constraints Violated") +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5), 
                                reverse = TRUE)) +
    theme_bw() +
    theme(legend.position = "bottom", panel.spacing.x = unit(14, units = "points"))

  ggsave(filename = here("img/eps_and_nu_violation_example.png"),
         eps_and_nu_violation_plot,
         dpi = 320,
         height = 3, width = 6)
}

knitr::include_graphics(here("img/eps_and_nu_violation_example.png"))
```

## Distributions of Fit Statistics

One of the primary questions the simulation study was intended to answer was whether the five model-error methods produced solutions with different fit index values when used with the same error-free models and target RMSEA and CFI values. In this section, I report the distributions of the RMSEA, CFI, TLI and CRMR model-fit indices for solutions produced by each of the five model-error methods ($\textrm{TKL}_{\textrm{RMSEA}}$, $\textrm{TKL}_{\textrm{CFI}}$, $\textrm{TKL}_{\textrm{RMSEA/CFI}}$, CB, and WB).

### RMSEA

Of the fit indices investigated in this study, the RMSEA value has been most often used as a measure of model fit when generating covariance or correlation matrices with model error. \@ref(fig:rmsea-distributions) shows box-plots summarizing the distributions of RMSEA values for each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. Notice that the $\textrm{TKL}_{\textrm{RMSEA}}$ and CB methods almost always produced solutions with RMSEA values that were very close to the target RMSEA values. This result makes sense because both methods use optimization to produce solutions with RMSEA values close to a specified target. However, a somewhat unexpected result was that the CB method occasionally produced solutions with RMSEA values that were much higher or lower than the target value, particularly in conditions with ten major factors.

After the $\textrm{TKL}_{\textrm{RMSEA}}$ and CB methods, \@ref(fig:rmsea-distributions) shows that the WB method was the next best model-error method in terms of producing solutions with RMSEA values close to the target values. In fact, the WB method produced solutions with median RMSEA values that were as close to the target values as those from $\textrm{TKL}_{\textrm{RMSEA}}$ and CB solutions. However, the WB method also led to more variable RMSEA values, particularly in conditions with Poor model fit and many factors.

The two methods that performed worst in terms of producing solutions with RMSEA values close to the targets were the $\textrm{TKL}_{\textrm{RMSEA/CB}}$ and $\textrm{TKL}_{\textrm{CB}}$ methods. \@ref(fig:rmsea-distributions) shows that these methods often led to RMSEA values that were lower than the target values, except when there were relatively few factors and strong factor loadings. The largest differences between the observed and target RMSEA values for the $\textrm{TKL}_{\textrm{RMSEA/CB}}$ and $\textrm{TKL}_{\textrm{CB}}$ methods occurred in conditions with Poor model fit and relatively weak factor loadings (.3). In those conditions, both methods led to RMSEA values that were considerably lower than the target values.

```{r rmsea-distributions, fig.cap = "Distributions of the RMSEA values for solutions produced by each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. The dashed lines indicate the target RMSEA value for each condition. Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.", fig.align='left', out.width=425}
if (make_plots) {
  rmsea_distributions <- 
    results_matrix %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = rmsea)) +
    geom_hline(aes(yintercept = target_rmsea), 
               color = "black", linetype = "dashed", size = .25) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = "RMSEA") +
    theme_bw() +
    theme(plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/rmsea_distributions.png"), 
    plot = rmsea_distributions,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/rmsea_distributions.png"))
```

### CFI

[What are we doing for a transition here?] The distributions of CFI values for the solutions produced using the five model-error methods are shown in \@ref(fig:cfi-distributions), conditioned on number of factors, model fit, and factor loading strength. As with \@ref(fig:rmsea-distributions), the middle levels of model fit and factor loading strength were omitted to converse space. \@ref(fig:cfi-distributions) shows that the results for CFI were nearly opposite to the results for RMSEA. Specifically, whereas the $\textrm{TKL}_{\textrm{RMSEA}}$, CB, and WB methods produced solutions with RMSEA values much closer to the target values than those produced by the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ or $\textrm{TKL}_{\textrm{CFI}}$ methods, the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ and $\textrm{TKL}_{\textrm{CFI}}$ produced solutions with CFI values that were closer to the target CFI values compared to the other model-error methods in most conditions. The differences between the methods that optimized for CFI ($\textrm{TKL}_{\textrm{RMSEA/CFI}}$ and $\textrm{TKL}_{\textrm{CFI}}$) and the other model-error methods were largest for conditions with Poor model fit, low factor loadings, and many factors (see the third row of \@ref(fig:cfi-distributions)). On the other hand, \@ref(fig:cfi-distributions) shows that all of the model-error methods produced similar CFI values (that were close to the target CFI value) for conditions with very good model fit and strong factor loadings.

A result in \@ref(fig:cfi-distributions) that is worth highlighting is that the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ and $\textrm{TKL}_{\textrm{CFI}}$ produced solutions with very similar CFI values in most conditions. These two methods also produced solutions with very similar RMSEA values in many conditions, as shown in \@ref(fig:rmsea-distributions). Indeed, the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ and $\textrm{TKL}_{\textrm{CFI}}$ methods led to much more similar results in terms of both RMSEA and CFI than the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA}}$ methods. Put another way, optimizing for CFI alone generally led to similar results compared to optimizing for both CFI and RMSEA, whereas optimizing for RMSEA alone generally led to solutions with much different RMSEA and CFI values compared to optimizing for both CFI and RMSEA. This suggests that CFI was more sensitive to small changes in parameter values than RMSEA.

The distributions of CFI values shown in \@ref(fig:cfi-distributions) also emphasized the importance of reporting multiple fit indices when simulating correlation or covariance matrices with model error. For instance, the $\textrm{TKL}_{\textrm{RMSEA}}$ and CB methods produced solutions with RMSEA values close to the target RMSEA value of .09 in conditions with Poor model fit and weak factor loadings. However, \@ref(fig:cfi-distributions) shows that those two model-error methods led to solutions with unacceptably low CFI values in the same conditions.

```{r cfi-distributions, fig.cap = "Distributions of the CFI values for solutions produced by each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. The dashed lines indicate the target CFI value for each condition. Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.", out.width=425}
if (make_plots) {
  cfi_distributions <- 
    results_matrix %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = cfi)) +
    geom_hline(aes(yintercept = target_cfi), 
               color = "black", linetype = "dashed", size = .25) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = "CFI") +
    theme_bw() +
    theme(plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/cfi_distributions.png"), 
    plot = cfi_distributions,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/cfi_distributions.png"))
```

### TLI

The distributions of TLI values for the solutions produced using the five model-error methods are shown in \@ref(fig:tli-distributions), conditioned on number of factors, model fit, and factor loading strength. Overall, the distributions of TLI values were quite similar to the distributions of CFI values shown in \@ref(fig:cfi-distributions). In particular, the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods tended to produce solutions with higher TLI values than the other model-error methods, except for conditions with a single factor, Poor model fit, and strong factor loadings. 

```{r tli-distributions, out.width="100%"}
#| fig.cap="Distributions of the TLI values for solutions produced by each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. The dashed lines indicate the threshold values of TLI that correspond to the targeted levels of model fit, according to Hu and Bentler (1999). Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  tli_distributions <- 
    results_matrix %>%
    mutate(tli = case_when(tli < 0 ~ 0, TRUE ~ tli)) %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = tli)) +
    geom_hline(aes(yintercept = target_cfi), 
               color = "black", linetype = "dashed", size = .25) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = "TLI") +
    theme_bw() +
    theme(panel.spacing.x = unit(10, units = "pt"),
          plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/tli_distributions.png"), 
    plot = tli_distributions,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/tli_distributions.png"))
```

Similar to CFI, the differences in TLI values resulting from the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods compared to the other model-error methods were most pronounced for conditions with many factors, weak factor loadings, and Poor model fit. This result (and the similar result for CFI) can be understood by thinking about how the relative and absolute fit indices differ in how they describe model fit. As an example, consider the an orthogonal model with ten major factors, five items per factor, and major common factor loadings fixed at .4. The population correlation matrix without model error for this condition ($\bm{\Omega}$) was $50 \times 50$ block-diagonal correlation matrix, with correlations of $0.16$ between items that load on the same factor and zero otherwise. To obtain a population correlation matrix ($\bm{\Sigma}$) with sufficient model error to indicate Poor model fit (based on an RMSEA value of 0.09), the elements of the $\mathbf{W}$ matrix had to be large enough to ensure that the square average squared difference between the off-diagonal elements of $\bm{\Omega}$ and $\bm{\Sigma}$ was 0.0081. Because the "noise" correlations from the minor common factors were relatively large compared to the non-zero elements of $\bm{\Omega}$, the $\bm{\Sigma}$ matrix did not have a clear factor structure. Thus, the major-factor model did not fit the $\bm{\Sigma}$ very well because the model was unable to account for the correlations between the items represented in the off-block-diagonal elements of $\bm{\Sigma}$ that were introduced by the $\mathbf{WW}^\prime$ matrix. Moreover, because the $\mathbf{WW}^\prime$ matrix increased the correlations between nearly all of the items that were uncorrelated in $\bm{\Omega}$, the independence model used as a baseline for computing CFI fit $\bm{\Sigma}$ relatively well.

```{r omega-wtw-sigma-example, out.width='100%'}
#| fig.cap = "The population correlation matrix without model error ($\\bm{\\Sigma}$), the matrix of item correlations due to the minor common factors ($\\mathbf{WW}^\\prime$), and the population matrix with model error ($\\bm{\\Sigma}$) for an orthogonal model with ten major factors, five items per factor, and major common factor loadings fixed at .4. The RMSEA and CFI values for this example were 0.09 and 0.25."

if (make_plots) {
  c129 <- readRDS(here("data/results_129.RDS"))
  
  Sigma <- c129[[1]]$sigma_tkl_rmsea$value$Sigma
  W <- c129[[1]]$sigma_tkl_rmsea$value$W
  Omega <- fungible::simFA(
    Model = list(NFac = 10, NItemPerFac = 5),
    Loadings = list(FacLoadDist = "fixed",
                    FacLoadRange = .3)
  )$Rpop
  
  p3 <- Sigma %>%
    as_tibble() %>%
    rownames_to_column() %>%
    pivot_longer(cols = V1:V50) %>%
    rename("var1" = "rowname", "var2" = "name") %>%
    mutate(var2 = as.numeric(str_extract(var2, "[0-9]+")),
           var1 = as.numeric(var1)) %>%
    mutate(value = case_when(var1 == var2 ~ NA_real_,
                             var1 != var2 ~ value)) %>%
    ggplot(aes(x = fct_rev(as.factor(var2)), y = var1, fill = value)) +
    geom_raster() +
    colorspace::scale_fill_continuous_diverging("Purple-Green", 
                                                limits = c(-.4, .4)) +
    labs(fill = "Correlation") +
    theme_void()
  
  p2 <- tcrossprod(W) %>%
    as_tibble() %>%
    rownames_to_column() %>%
    pivot_longer(cols = V1:V50) %>%
    rename("var1" = "rowname", "var2" = "name") %>%
    mutate(var2 = as.numeric(str_extract(var2, "[0-9]+")),
           var1 = as.numeric(var1)) %>%
    mutate(value = case_when(var1 == var2 ~ NA_real_,
                             var1 != var2 ~ value)) %>%
    ggplot(aes(x = fct_rev(as.factor(var2)), y = var1, fill = value)) +
    geom_raster() +
    colorspace::scale_fill_continuous_diverging("Purple-Green", 
                                                limits = c(-.4, .4)) +
    labs(fill = "Correlation") +
    theme_void()
  
  p1 <- Omega %>%
    as_tibble() %>%
    rownames_to_column() %>%
    pivot_longer(cols = V1:V50) %>%
    rename("var1" = "rowname", "var2" = "name") %>%
    mutate(var2 = as.numeric(str_extract(var2, "[0-9]+")),
           var1 = as.numeric(var1)) %>%
    mutate(value = case_when(var1 == var2 ~ NA_real_,
                             var1 != var2 ~ value)) %>%
    ggplot(aes(x = fct_rev(as.factor(var2)), y = var1, fill = value)) +
    geom_raster() +
    colorspace::scale_fill_continuous_diverging("Purple-Green", 
                                                limits = c(-.4, .4)) +
    labs(fill = "Correlation") +
    theme_void()
  
  Omega_plus_WtW <- p1 + plot_spacer() + p2 + plot_spacer() + p3 + 
    plot_layout(guides = "collect", widths = c(4, 1, 4, 1, 4)) &
    theme(plot.margin = margin(t = 15, l = 5, r = 5))
  
  ggsave(Omega_plus_WtW, filename = here("img/Omega_plus_WtW.png"),
         dpi = 320,
         width = 7,
         height = 2.1,
         scale = 1.5)
}

knitr::include_graphics(here("img/omega_plus_WtW_with_annotations.png"))
```

The effects of the model error introduced by the minor common factors can be seen in \@ref(fig:omega-wtw-sigma-example), which contains visual representations of $\bm{\Omega}$, $\textrm{WW}^\prime$, and $\bm{\Sigma}$ matrices. The $\mathbf{W}$ and $\bm{\Sigma}$ matrices shown in the figure were obtained using the $\textrm{TKL}_{\textrm{RMSEA}}$ method with a target RMSEA value of 0.09. The observed RMSEA and CFI values for $\bm{\Sigma}$ were 0.09 and 0.25. Notice in \@ref(fig:omega-wtw-sigma-example) how large the elements of the $\mathbf{WW}^\prime$ matrix were relative to the $\bm{\Omega}$ matrix and how many off-block-diagonal elements there were relative to the block diagonal elements. This helps explain the incompatibility between RMSEA and CFI for models with many factors, weak major common factor loadings, and target RMSEA and CFI values reflecting poor model fit. To get an RMSEA value in line with the target value, all of the elements of $\bm{\Omega}$ were perturbed. The perturbations to the off-block-diagonal elements of $\bm{\Omega}$ were important in terms of the resulting CFI value because they were incompatible with the major common factor model (i.e., they should have been zero under that model) and therefore degraded the fit of the major factor model for $\bm{\Sigma}$ while improving the fit of the independence model. The effect of perturbing these off-block-diagonal elements became more pronounced as the number of factors increased because the number of off-block-diagonal elements increased faster than the number of block-diagonal elements as the total number of items increased. This can be clearly seen in \@ref(fig:off-diag-elements), which shows the number of block diagonal elements and off-block-diagonal elements in the lower triangle of $\bm{\Sigma}$ as the number of factors increased for an orthogonal model with five items per factor.

```{r off-diag-elements, fig.align='center'}
#| fig.cap = "The number of block-diagonal elements in the lower-triangle of $\\bm{\\Sigma}$ compared to the number of off-block-diagonal elements for an orthogonal model with five items per factor."
if (make_plots) {
  count_diag_off_diag <- function(items_per_factor, factors) {
    p <- items_per_factor * factors
    block_diag_elements <- (items_per_factor * (items_per_factor - 1) / 2) * factors
    off_diag_elements <- (p * (p - 1)) / 2
    off_block_diag_elements <- off_diag_elements - block_diag_elements
    
    list("block_diag_elements" = block_diag_elements,
         "off_block_diag_elements" = off_block_diag_elements)
  }
  
  factors <- 1:10
  
  off_diag <- count_diag_off_diag(items_per_factor = 5, factors = factors)
  off_diag <- as.data.frame(off_diag)
  off_diag$k <- factors
  off_diag <- pivot_longer(off_diag, 
                           cols = block_diag_elements:off_block_diag_elements, 
                           names_to = "type", 
                           values_to = "elements")
  
  off_diag %>%
    mutate(type = factor(type, 
                         levels = c("block_diag_elements", 
                                    "off_block_diag_elements"),
                         labels = c("Block-Diagonal",
                                    "Off-Block-Diagonal"))) %>%
    ggplot(aes(x = k, y = elements, color = type, 
               linetype = type, shape = type)) +
    geom_point() +
    geom_line(aes(group = type)) +
    scale_y_continuous(label = scales::comma) +
    scale_x_continuous(breaks = c(0, 2, 4, 6, 8, 10)) +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    theme_minimal() +
    labs(y = "Elements in Lower Triangle",
         x = "Factors",
         color = "", linetype = "", shape = "") +
    theme(legend.position = "bottom")
  
  ggsave(filename = here("img/off-diagonal-elements.png"),
         dpi = 320,
         height = 3.5,
         width = 3.5)
}

knitr::include_graphics(here("img/off-diagonal-elements.png"))
```

### CRMR

The distributions of the CRMR model fit index by level of model fit, factor loading, number of major common factors, and model error method are represented as boxplots in \@ref(fig:crmr-distributions). The figure shows that in conditions with Very Good model fit, all of the model error methods led to CRMR values that were generally at or below 0.05, the rule-of-thumb CRMR threshold for good model fit given by Hu and Bentler [-@hu1999]. In general, all of the model fit methods led to similar CRMR distributions when model fit was Very Good. The largest differences in CRMR values between model error methods when model fit was Very Good occured when there were many factors and low factor loadings. In those conditions, the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ and $\textrm{TKL}_{\textrm{CFI}}$ model error methods led to lower CRMR values compared to the other methods.

Larger differences between the model error methods emerged when model fit was Poor. In those conditions, only the $\textrm{TKL}_{\textrm{RMSEA}}$ and WB methods consistently led to CRMR values that were near or above the threshold CRMR value for acceptable model fit given by Hu and Bentler [-@hu1999]. In fact, the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods often led to CRMR values that were less than the Hu and Bentler [-@hu1999] threshold for good model fit and were frequently the methods the led to the lowest CRMR values. The only conditions where the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods led to higher median CRMR values were those with one-factor models, salient factor loadings of 0.8, and Poor (target) model fit.

<!-- [TK: Need to discuss the fit indices in terms of theta hat values as well. Where best to put that material?] -->

```{r crmr-distributions, fig.cap="Distributions of the CRMR values for solutions produced by each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. Recommendations for threshold values of SRMR/CRMR are not as fine-grained as for some other fit indices, but threshold values of 0.05 and 0.08 were proposed by Hu and Bentler (1999) as more and less conservative upper-bounds for acceptable SRMR/CRMR values. The dashed lines indicate these two threshold values. Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.", out.width="100%"}
if (make_plots) {
  crmr_distributions <- 
    results_matrix %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    mutate(target_crmr = case_when(model_fit == "Poor" ~ 0.08,
                                   model_fit == "Very Good" ~ 0.05)) %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = crmr)) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    geom_hline(yintercept = 0.05, 
               color = "black", linetype = "dashed", size = .25) +
    geom_hline(yintercept = 0.08, 
               color = "black", linetype = "dashed", size = .25) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = "CRMR") +
    theme_bw() +
    theme(plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/crmr_distributions.png"), 
    plot = crmr_distributions,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/crmr_distributions.png"))
```

## Agreement of Fit Indices

In the previous section, I reported results in terms of the RMSEA, CFI, TLI, and CRMR values individually. However, we often consider fit indices jointly and are interested in whether fit indices for a correlation matrix "agree" with one another. Therefore, in this section I will report the results in terms of fit index agreement. Specifically, I will try to address two main questions. First, how well did model error methods do at producing solutions with RMSEA and CFI values that were close to the target values corresponding to a particular level of model fit? Second, to what extent did the model error methods produce solutions with fit index values that corresponded to the same qualitative interpretation of model fit? For both questions, I will focus on the agreement between RMSEA and CFI, both for the sake of simplicity and because RMSEA and CFI were the fit indices that I used as target values for the model error methods. Although not discussed here, results reporting the agreement between all of the fit indices included in this study (CFI, RMSEA, TLI, and CRMR) are included in \@ref(appendix-b).

To evaluate how well each of the model error-methods did at producing solutions that had RMSEA and CFI values that were close to the target values, I used the metric

\begin{equation}
D = |\textrm{RMSEA}_{\textrm{obs}} - \textrm{RMSEA}_{\textrm{target}}| + |\textrm{CFI}_{\textrm{obs}} - \textrm{CFI}_{\textrm{target}}|,
(\#eq:distance-between-observed-and-target)
\end{equation}

where $\textrm{RMSEA}_{\textrm{obs}}$ and $\textrm{CFI}_{\textrm{obs}}$ denote the observed RMSEA and CFI values for each solution. Small $D$ values indicated that the solution was good in the sense that the observed RMSEA and CFI values were close to the target RMSEA and CFI values. On the other hand, large $D$ values inidcated a poor solution with either one or both of the observed RMSEA and CFI values far from the corresponding target values.

Box-plots showing the distribution of $D$ values for each of the model-error methods conditioned on the number of major common factors, model fit, and factor loading strength are shown in \@ref(fig:d3-plot). The figure shows that all of the model-error method led to good results (i.e., small $D$ values) in conditions with Very Good model fit and strong factor loadings. Similarly, all of the model-error methods led to reasonably good results in conditions with one or three major factors, Poor model fit, and strong factor loadings. In the remaining conditions where the model-error methods did not all lead to small $D$ values, the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods led to smaller $D$ values than the other model-error methods. In particular, the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods led to much smaller $D$ values than the alternative model-error methods in conditions with many major common factors and weak factor loadings, especially when model fit was Poor. The CB method typically led to the next-smallest $D$ values in these conditions, followed by the $\textrm{TKL}_{\textrm{RMSEA}}$ and WB methods.

```{r d3-plot, out.width="100%"}
#| fig.cap = "The sum of the absolute differences between the observed and target RMSEA and CFI values ($D$), conditioned on number of factors, model fit, and factor loading strength. Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."
if (make_plots) {
  d_plot <- 
    results_matrix %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = d3)) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = TeX("$D$")) +
    theme_bw() +
    theme(plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/d_plot.png"), 
    plot = d_plot,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/d_plot.png"))
```

Looking at how far the observed RMSEA and CFI values differed from the target RMSEA and CFI values (in terms of $D$) gave one perspective on model fit agreement. I obtained a second perspective on model fit agreement by determining how often fit indices led to the same qualitative assessement of model fit using rule-of-thumb threshold values of RMSEA and CFI such as those reported by Hu and Bentler [-@hu1999]. To calculate the rates of qualitative model fit agreement, I first assigned each simulated correlation matrix into one of three qualitative model fit categories based on its observed RMSEA and CFI values. For RMSEA, a simulated correlation matrix was considered to have good model fit if the observed RMSEA value was less than or equal to .05. If the RMSEA was greater than 0.05 but less than .1, it was considered to have acceptable model fit. RMSEA values greater than .1 were considered to represent unacceptable model fit. Similarly, observed CFI values greater than .95 were considered to represent good model fit, CFI values between .95 and .90 were considered to represent acceptable model fit, and CFI values below .90 were considered to represent unacceptable model fit.

```{r calculate-agreement-rate, echo = FALSE, include = FALSE}
agreement_rates <- results_matrix %>% mutate(
  rmsea_subj_fit = case_when(rmsea <= 0.05 ~ "Good",
                             rmsea > 0.05 & rmsea <= 0.1 ~ "Acceptable",
                             rmsea > 0.1 ~ "Unacceptable"),
  cfi_subj_fit = case_when(cfi >= .95 ~ "Good",
                           cfi < .95 & cfi >= .9 ~ "Acceptable",
                           cfi < .9 ~ "Unacceptable")
) %>% mutate(
  fit_agreement = (rmsea_subj_fit == cfi_subj_fit)
) %>% group_by(error_method) %>% 
  summarise(mean_fit_agreement = mean(fit_agreement, na.rm = TRUE)) %>%
  pivot_wider(names_from = error_method, values_from = mean_fit_agreement)

colnames(agreement_rates) <-
  c("tkl_rmsea", "tkl_cfi", "tkl_rmsea_cfi", "cb", "wb")
```

The percent of simulated correlation matrices with model error that led to fit indices indicating qualitative agreement on model fit are shown in \@ref(fig:fit-agreement-fig), conditioned on number of factors, factor loading strength, target model fit, and model-error method. The figure shows that no single model-error method always led to the highest rates of qualtative fit agreement. 

XXX ----

Overall, the $\textrm{TKL}_{\textrm{CFI}}$ model-error method led to the highest rate of qualitative fit index agreement (`r scales::percent(agreement_rates$tkl_cfi, accuracy = .1)`), followed by the CB (`r scales::percent(agreement_rates$cb, accuracy = .1)`), $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ (`r scales::percent(agreement_rates$tkl_rmsea_cfi, accuracy = .1)`), $\textrm{TKL}_{\textrm{RMSEA}}$ (`r scales::percent(agreement_rates$tkl_rmsea, accuracy = .1)`), and WB (`r scales::percent(agreement_rates$wb, accuracy = .1)`) methods. 

Breaking these results out further by level of target model fit, 

XXX ----

... For conditions with target fit indices that corresponded to Very Good model fit, the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods almost always led to qualitative model fit agreement rates of 100%. The other model-error methods led to similar rates of qualitative model fit agreement in conditions with Very Good target model fit and strong factor loadings (i.e., Loading = .8), but led to much lower rates of agreement in conditions with Very Good model fit and weaker factor loadings.

Results for conditions with Fair or Poor target model fit were less straightforward. When target model fit was Fair, the $\textrm{TKL}_{\textrm{CFI}}$ method often led to the highest rates of qualitative fit agreement of the model-error methods, particularly when factor loadings were weak and when there were many major common factors. When target model fit was Poor, all of the model error methods had low qualitative fit agreement rates except when factor loadings were .8

```{r fit-agreement-fig, out.width='100%'}
#| fig.cap = "The percent of cases where the observed RMSEA and CFI values led to the same qualitative evaluation of model fit based on the threshold values suggested by Hu and Bentler (1999)."

if (make_plots) {
  results_matrix %>%
    mutate(
      rmsea_subj_fit = case_when(rmsea <= 0.05 ~ "Good",
                                 rmsea > 0.05 & rmsea <= 0.1 ~ "Acceptable",
                                 rmsea > 0.1 ~ "Unacceptable"),
      cfi_subj_fit = case_when(cfi >= .95 ~ "Good",
                               cfi < .95 & cfi >= .9 ~ "Acceptable",
                               cfi < .9 ~ "Unacceptable")
    ) %>% mutate(
      fit_agreement = (rmsea_subj_fit == cfi_subj_fit)
    ) %>% group_by(factors, loading_rec, model_fit_rec, error_method) %>%
    summarize(agreement_rate = mean(fit_agreement, na.rm = TRUE)) %>%
    ggplot(aes(y = agreement_rate, x = factors, color = error_method, 
               shape = error_method, linetype = error_method, 
               group = error_method)) +
    scale_y_continuous(label = scales::percent) +
    scale_x_continuous(breaks = 1:10) +
    geom_point() +
    geom_line() +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    labs(y = "Fit Index Agreement", x = "Factors",
         shape = "Error Method", color = "Error Method", linetype = "Error Method") +
    facet_grid(model_fit_rec ~ loading_rec) +
    theme_bw() +
    theme(legend.position = "bottom")
  
  ggsave(filename = here("img/fit_index_agreement.png"),
         height = 6,
         width = 7,
         dpi = 320)
}

knitr::include_graphics(here("img/fit_index_agreement.png"))
```

(ref:fit-agreement-table-caption) The percent of cases where the observed RMSEA and CFI values led to the same qualitative evaluation of model fit based on the threshold values suggested by Hu and Bentler [-@hu1999].

```{r fit-agreement-table}
fit_agreement_table <- results_matrix %>%
  mutate(
    rmsea_subj_fit = case_when(rmsea <= 0.05 ~ "Good",
                               rmsea > 0.05 & rmsea <= 0.1 ~ "Acceptable",
                               rmsea > 0.1 ~ "Unacceptable"),
    cfi_subj_fit = case_when(cfi >= .95 ~ "Good",
                             cfi < .95 & cfi >= .9 ~ "Acceptable",
                             cfi < .9 ~ "Unacceptable")
  ) %>% mutate(
    fit_agreement = (rmsea_subj_fit == cfi_subj_fit)
  ) %>% group_by(factors, loading_numeric, model_fit, error_method) %>%
  summarize(agreement_rate = mean(fit_agreement, na.rm = TRUE)) %>%
  pivot_wider(names_from = error_method, 
              values_from = agreement_rate) %>%
  ungroup()

fit_agreement_table %>%
    mutate(across(.cols = c(`TKL (RMSEA)`:WB), ~ . * 100)) %>%
  apa_table(col.names = c("Factors", "Loading", "Model Fit", 
                          "$\\textrm{TKL}_{\\textrm{RMSEA}}$", 
                          "$\\textrm{TKL}_{\\textrm{CFI}}$", 
                          "$\\textrm{TKL}_{\\textrm{RMSEA/CFI}}$", 
                          "CB", "WB"),
            digits = c(0, 1, 0, 1, 1, 1, 1, 1),
            align = "rrlrrrrr",
            col_spanners = list("Model-Error Method" = c(4, 8)),
            format.args = list("na_string" = "---"),
            caption = "(ref:fit-agreement-table-caption)",
            label = "fit-agreement-table",
            note = "TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.")
```

It was both interesting and unexpected that the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods led to such similar results, both in terms of $D$ and in terms of observed $RMSEA$ and $CFI$ values, whereas the $\textrm{TKL}_{\textrm{RMSEA}}$ method often led to results that were more similar to those of the CB and WB methods. The similarity between the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ results suggested that the CFI had more influence in \@ref(eq:rmsea-cfi-obj-function) than RMSEA, even when the fit indices were equally weighted. A possible explanation is provided in \@ref(fig:rmsea-cfi-distributions), which shows observed RMSEA and CFI values for solutions from the $\textrm{TKL}_{\textrm{RMSEA}}$, $\textrm{TKL}_{\textrm{CFI}}$, and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods, conditioned on number of factors and model fit. The figure shows that solutions generated using the $\textrm{TKL}_{\textrm{RMSEA}}$ method had little variability in terms of RMSEA values. On the other hand, the CFI values for those solutions often had much more variability, particularly in conditions with weak factor loadings. When the $\textrm{TKL}_{\textrm{CFI}}$ method was used, solutions had RMSEA and CFI values that were generally constrained to small range within each condition. These results indicated that the range of possible CFI was much larger for solutions with a fixed RMSEA value than the range of RMSEA values for solutions with a fixed CFI value in many conditions. Therefore, the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ might have led to such similar results because including a CFI target constrained the range of RMSEA values. Put another way, incorporating a target RMSEA value in addition to a target CFI value might have had little effect compared to using only a target CFI value because making small changes to CFI often led to large changes in RMSEA.

```{r rmsea-cfi-distributions, out.width='100%'}
#| fig.cap = "RMSEA and CFI values for the TKL-based model-error methods, conditioned on number of factors and model fit."
if (make_plots) {
  results_matrix %>%
    filter(rmsea < 3, str_detect(error_method, "TKL"),
           factors != 3, loading_numeric != .6) %>%
    ggplot(aes(x = rmsea, y = cfi)) +
    geom_jitter(alpha = .1, size = 1, 
                aes(color = error_method, shape = error_method)) +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    facet_grid(model_fit_rec~ factors_rec * loading_rec, scales = "fixed") +
    labs(x = "RMSEA", y = "CFI", color = "Error Method", shape = "Error Method") +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +
    theme_bw() +
    theme(legend.position = "bottom",
          panel.spacing.x = unit(8, units = "points"))
  
  ggsave(filename = here("img/rmsea-cfi-distributions.png"),
         dpi = 320,
         scale = 1.1,
         height = 5, width = 7)
}

knitr::include_graphics(path = here("img/rmsea-cfi-distributions.png"))
```

To better understand the conditional distribution of CFI values for solutions with fixed RMSEA values, I used the $\textrm{TKL}_{\textrm{RMSEA}}$ method to generate solutions with a range of target RMSEA values for each of a subset of conditions from the main simulation study. Specifically, I generated 100 solutions for each of 16 target RMSEA values equally-spaced between .025 and .1 for each condition of the main simulation design with uncorrelated major common factors. (Conditions with correlated major common factors were omitted to make the number of conditions manageable.) The results can be seen in Figure \@ref(fig:rmsea-conditional-cfi), which shows the CFI and RMSEA values for each condition. Solid black lines in the figure indicate where RMSEA is equal to $1 - \textrm{CFI}$.

```{r rmsea-conditional-cfi, out.width='100%'}
#| fig.cap = "Observed CFI and RMSEA values for solutions generated using the $\\textrm{TKL}_{\\textrm{RMSEA}}$ method. For each combination of number of factors, number of items per factor, and factor loading strength, 100 solutions were generated for each of 16 target RMSEA values equally-spaced between 0.025 and 0.1. The solid black line indicates where CFI and $1 - \\textrm{RMSEA}$ were equal."

if (make_plots) {
  # Show possible CFI values for a range of RMSEA values
  reps <- 100
  
  factors <- unique(results_matrix$factors)
  items_per_factor <- unique(results_matrix$items_per_factor)
  loading <- unique(results_matrix$loading_numeric)
  target_rmsea <- seq(0.025, 0.1, by = 0.005)
  
  conditions_matrix <- expand.grid(
    factors = factors,
    items_per_factor = items_per_factor,
    loading = loading,
    target_rmsea = target_rmsea
  )
  
  set.seed(666)
  rmsea_cfi_vals <- pro_map_dfr(
    .x = 1:nrow(conditions_matrix), 
    .f = function(condition) {
      mod <- simFA(
        Model = list(NFac = conditions_matrix$factors[condition],
                     NItemPerFac = conditions_matrix$items_per_factor[condition],
                     Model = "orthogonal"),
        Loadings = list(FacLoadDist = "fixed",
                        FacLoadRange = conditions_matrix$loading[condition])
      )
      
      map_dfr(.x = 1:reps,
              .f = function(x, target_rmsea) {
                sol <- noisemaker(mod = mod, 
                                  method = "TKL", 
                                  target_rmsea = target_rmsea,
                                  target_cfi = NULL,
                                  tkl_ctrl = list(penalty = 1e6,
                                                  NWmaxLoading = 2,
                                                  WmaxLoading = .3))
                w_constraints_violated <- sum(sol$W[,1] >= .3) > 2
                c(rmsea = sol$rmsea, cfi = sol$cfi, 
                  w_constraints_violated = w_constraints_violated)
              }, target_rmsea = conditions_matrix$target_rmsea[condition])
    }
  )
  
  rmsea_cfi_vals <- bind_cols(
    rmsea_cfi_vals,
    conditions_matrix[rep(1:nrow(conditions_matrix), each = reps),]
  )
  
  rmsea_cfi_vals <- rmsea_cfi_vals %>%
    mutate(
      factors_rec = factor(factors, 
                           labels = levels(results_matrix$factors_rec)),
      loading_rec = factor(loading, 
                           labels = levels(results_matrix$loading_rec)),
      items_per_factor_rec = factor(items_per_factor, 
                                    labels = levels(results_matrix$items_per_factor_rec))
    )
  
  saveRDS(rmsea_cfi_vals, file = here("data/rmsea_conditional_cfi.RDS"))
  # rmsea_cfi_vals <- readRDS(here("data/rmsea_conditional_cfi.RDS"))
  
  rmsea_cfi_vals %>%
    ggplot(aes(x = rmsea, y = cfi)) +
    geom_point(size = 1, alpha = .1) +
    geom_abline(aes(slope = -1, intercept = 1), size = .5, alpha = .5) +
    facet_grid(loading_rec * items_per_factor_rec ~ factors_rec) +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +
    labs(x = "RMSEA", y = "CFI") +
    theme_bw() +
    theme(legend.position = "bottom",
          panel.spacing.x = unit(14, units = "points"))
  
  ggsave(filename = here("img/rmsea_conditional_cfi.png"),
         height = 7,
         width = 7,
         scale = 1.15,
         dpi = 320)
}

knitr::include_graphics(
  path = here("img/rmsea_conditional_cfi.png")
)
```

## TKL Method Recovery of Model Fit Indices

The TKL-based model error methods were designed to find a correlation matrix with model error that had either an RMSEA value or a CFI value (or both) that were close to a specified value. In a previous section, I reported RMSEA and CFI values for all of the model-error methods (including the TKL-based methods) and compared them to their target values. When only one target model-fit index was used (i.e., $\textrm{TKL}_{\textrm{RMSEA}}$ or $\textrm{TKL}_{\textrm{CFI}}$), the observed RMSEA and CFI values were very close to the target values. However, when both RMSEA and CFI fit index targets were used simulaneously with the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ method, many solutions failed to have RMSEA and CFI values that were both very close to the target values. This could indicate that the optimization procedure was not working well and often failed to find an optimal solution. However, an alternative explanation is that some combinations of RMSEA and CFI might not have been possible for conditions with major factor loadings fixed at a particular value, or with a certain number of major common factors, etc. 

To determine whether the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ was able to find near-optimal solutions, I first needed to find combinations of RMSEA and CFI values that were known to be possible. If the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ method was able to produce solutions with RMSEA and CFI values that were close to these target RMSEA and CFI values, it would suggest that the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ was working well. More importantly, if the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ method was not able to produce solutions with fit indices close to these known-to-be-possible combinations of RMSEA and CFI, it would suggest that the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ cannot be relied on to produce optimal or near-optimal solutions. 

To find RMSEA and CFI value combinations that were known to be possible, I used the implementation of the traditional TKL method implemented in the `simFA()` function to generate a correlation matrix with model error for every condition in the simulation design. Next, I computed the RMSEA and CFI values for each simulated correlation matrix. I then used those values as target RMSEA and CFI values for the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ method and generated 50 correlation matrices with model error for each condition using the `noisemaker()` function.[^fit-recovery-code] 

[^fit-recovery-code]: Code for this simulation study is provided in Appendix TK.

The results from this small simulation study are reported in \@ref(fig:fit-index-recovery), which shows the known-to-be-possible target values of RMSEA and CFI (indicated by solid black lines) and the observed RMSEA and CFI values from the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ method for each condition. The figure shows that the observed RMSEA and CFI values were nearly identical to the target values for most conditions. The conditions where the observed fit indices had the most variability were conditions with one or three factors, five items, and weak factor loadings. However, even in those conditions many solutions had CFI and RMSEA values that were nearly identical to the target values. This suggests that using the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ method repeatedly with different initial values of $\mathbf{W}$,  $\epsilon$, and $\nu_{\textrm{e}}$ and then selecting the solution with RMSEA and CFI values closest to the target values might be an effective approach for obtaining optimal (or nearly-optimal) solutions.

```{r fit-index-recovery}
#| fig.cap = "Observed RMSEA and CFI values for the $\\textrm{TKL}_{\\textrm{RMSEA/CFI}}$ model-error method and the corresponding known-to-be-possible target RMSEA and CFI values (indicated by the black lines), conditioned on number of factors, number of items per factor, factor loading strength, and factor correlation. Note that some levels were omitted to conserve space."
if (make_plots) {
  noisy_data <- readRDS(here("data/noisy_data.RDS"))
  
  target_values <- noisy_data %>%
    filter(factor_corr != "Factor Cor.: 0.3",
           factor_loading != "Loading: 0.6") %>%
    select(factors, items_per_factor, factor_corr, factor_loading,
           target_rmsea, target_cfi) %>%
    distinct()
  
  noisy_data %>%
    filter(factor_corr != "Factor Cor.: 0.3",
           factor_loading != "Loading: 0.6") %>% 
    ggplot(aes(x = rmsea, y = cfi)) +
    geom_point(alpha = .3, size = 1) +
    geom_hline(aes(yintercept = target_cfi), data = target_values,
               size = .25) +
    geom_vline(aes(xintercept = target_rmsea), data = target_values,
               size = .25) +
    facet_grid(factors * factor_loading ~ items_per_factor * factor_corr) +
    labs(x = "RMSEA", y = "CFI") +
    theme_bw()
  
  ggsave(filename = "rmsea_cfi_recovery.png",
         path = here("img"),
         plot = last_plot(),
         dpi = 320,
         height = 10,
         width = 7)
}

knitr::include_graphics(here("img/rmsea_cfi_recovery.png"))
```

<!-- Need to add:
1. Experiments showing that the TKL optimization methods can recover true model fit parameters.
2. Number of times that the CB method *doesn't* produced an optimal solution (RMSEA_theta_hat should be 0).
3. Experiements showing whether reducing the TKL penalty improves convergence.
4. Discussion of "hat" fit indices? Figures and tables in Appendix, but do I need a discussion of them in the main body of the paper? Perhaps a question for Niels.
-->

## Figures to Add and Discuss

```{r rmsea-conditional-cfi, out.width='100%'}
#| fig.cap = "Observed CFI and RMSEA values for solutions generated using the $\\textrm{TKL}_{\\textrm{RMSEA}}$ method. For each combination of number of factors, number of items per factor, and factor loading strength, 100 solutions were generated for each of 16 target RMSEA values equally-spaced between 0.025 and 0.1. The solid black line indicates where CFI and $1 - \\textrm{RMSEA}$ were equal."

if (make_plots) {
  # Show possible CFI values for a range of RMSEA values
  reps <- 50
  
  factors <- unique(results_matrix$factors)
  items_per_factor <- unique(results_matrix$items_per_factor)
  loading <- unique(results_matrix$loading_numeric)
  target_rmsea <- seq(0.025, 0.1, by = 0.005)
  
  conditions_matrix <- expand.grid(
    factors = factors,
    items_per_factor = items_per_factor,
    loading = loading,
    target_rmsea = target_rmsea
  )
  
  set.seed(666)
  rmsea_cfi_vals <- pro_map_dfr(
    .x = 1:nrow(conditions_matrix), 
    .f = function(condition) {
      mod <- simFA(
        Model = list(NFac = conditions_matrix$factors[condition],
                     NItemPerFac = conditions_matrix$items_per_factor[condition],
                     Model = "orthogonal"),
        Loadings = list(FacLoadDist = "fixed",
                        FacLoadRange = conditions_matrix$loading[condition])
      )
      
      map_dfr(.x = 1:reps,
              .f = function(x, target_rmsea) {
                sol <- noisemaker(mod = mod, 
                                  method = "TKL", 
                                  target_rmsea = target_rmsea,
                                  target_cfi = NULL,
                                  tkl_ctrl = list(penalty = 1e6,
                                                  NWmaxLoading = 2,
                                                  WmaxLoading = .3))
                w_constraints_violated <- sum(sol$W[,1] >= .3) > 2
                c(rmsea = sol$rmsea, cfi = sol$cfi, 
                  w_constraints_violated = w_constraints_violated)
              }, target_rmsea = conditions_matrix$target_rmsea[condition])
    }
  )
  
  rmsea_cfi_vals <- bind_cols(
    rmsea_cfi_vals,
    conditions_matrix[rep(1:nrow(conditions_matrix), each = reps),]
  )
  
  rmsea_cfi_vals <- rmsea_cfi_vals %>%
    mutate(
      factors_rec = factor(factors, 
                           labels = levels(results_matrix$factors_rec)),
      loading_rec = factor(loading, 
                           labels = levels(results_matrix$loading_rec)),
      items_per_factor_rec = factor(items_per_factor, 
                                    labels = levels(results_matrix$items_per_factor_rec))
    )
  
  saveRDS(rmsea_cfi_vals, file = here("data/rmsea_conditional_cfi.RDS"))
  # rmsea_cfi_vals <- readRDS(here("data/rmsea_conditional_cfi.RDS"))
  
  rmsea_cfi_vals %>%
    ggplot(aes(x = rmsea, y = cfi)) +
    geom_point(size = 1, alpha = .1) +
    geom_abline(aes(slope = -1, intercept = 1), size = .5, alpha = .5) +
    facet_grid(loading_rec * items_per_factor_rec ~ factors_rec) +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +
    labs(x = "RMSEA", y = "CFI") +
    theme_bw() +
    theme(legend.position = "bottom",
          panel.spacing.x = unit(14, units = "points"))
  
  ggsave(filename = here("img/rmsea_conditional_cfi.png"),
         height = 7,
         width = 7,
         scale = 1.15,
         dpi = 320)
}

knitr::include_graphics(
  path = here("img/rmsea_conditional_cfi.png")
)
```

These specific matrices were chosen because they had large differences between CFI and RMSEA, but the W matrix didn't violate the constraints and the CB matrix wasn't indefinite.

<!-- Begin Figure c129 -->
\begin{sidewaysfigure}[ht]

{ \centering \includegraphics[width=\linewidth]{img/c129_R_plots_with_annotation.png} }

\caption{Correlation matrices with model error ($\mathbf{\Sigma}$) and the implied correlation matrix obtained by fitting the population major-factor model to $\mathbf{\Sigma}$ ($\hat{\mathbf{\Sigma}}$) for each model-error method. Results are shown for a set of solution matrices from the condition with ten orthogonal factors, five items per factor, weak factor loadings of .3, and Poor model fit. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.}

\label{fig:c129-with-annotation}

\end{sidewaysfigure}
<!-- End Figure c129 -->
