
The essential problem was that the addition of the discrete penalty term made the objective function non differentiable, which made it very difficult for the L-BFGS-B optimization algorithm to



[^sigma-vs-sigma-hat] 

[^sigma-vs-sigma-hat]: The results reported in this section are for fit indices relating to the lack of fit between $\bm{\Sigma}$ and $\mathbf{\Omega}$. Figures showing analogous results for fit indices calculated using $\hat{\mathbf{\Omega}}$ --- the model-implied correlation matrix obtained by analyzing $\mathbf{\Sigma}$ --- are provided in \@ref(sigma-hat-fit-indices).

FOR DISCUSSION: 

With the exception of the CB method, none of the model-error methods were expected to produce the same fit index values for $\bm{\Omega}$ and $\hat{\bm{\Omega}}$. Although in theory it might be possible to modify the TKL model-error methods to try to generate a $\bm{\Sigma}$ matrix corresponding to particular $\textrm{RMSEA}_{\hat{\theta}}$ or $\textrm{CFI}_{\hat{\theta}}$ values, doing so would require fitting a factor model at each iteration of the optimization procedure and thus would likely be impractical for most purposes. Understanding how the observed $\textrm{RMSEA}_{\hat{\theta}}$ or $\textrm{CFI}_{\hat{\theta}}$ values related the observed RMSEA and CFI values in the simulation study would be...


----- START OF CUT MATERIAL FROM MODEL FIT AGREEMENT SECTION --------

The frequent disagreement between RMSEA and CFI can be also be understood by considering how the relative and absolute fit indices differ in how they describe model fit. As an example, consider an orthogonal model with ten major factors, five items per factor, and major common factor loadings fixed at 0.4. The population correlation matrix without model error for this condition ($\bOmega$) would be a $50 \times 50$ block-diagonal correlation matrix, with correlations of $0.16$ between items that load on the same factor, and zero otherwise. To obtain a population correlation matrix ($\bSigma$) with sufficient model error to indicate Poor model fit (based on an RMSEA value of 0.09), the elements of the $\mathbf{W}$ matrix would have to be large enough to ensure that the square average squared difference between the off-diagonal elements of $\bOmega$ and $\bSigma$ was 0.0081. Because the "noise" correlations from the minor common factors would be relatively large compared to the non-zero elements of $\bOmega$, the $\bSigma$ matrix would not have a clear factor structure. Thus, the major-factor model would not fit the $\bSigma$ very well because the model would be unable to account for the correlations between the items represented in the off-block-diagonal elements of $\bSigma$ introduced by the $\mathbf{WW}^\prime$ matrix. 

In addition to affecting the fit of the hypothesized model, large non-zero off-block-diagonal elements of $\bSigma$ also negatively affect the fit of the baseline (independence) model because the independence model implies that $\bSigma$ should be an identity matrix. However, unless the hypothesized model leads to a much smaller minimized discrepancy function value than the baseline model, the resulting CFI value will be relatively small. This is demonstrated in \@ref(fig:cfi-as-factors-increase), which shows the minimized discrepancy function values for the hypothesized and baseline models as the number of major factors increased from one to ten, along with the corresponding CFI and RMSEA values. The hypothesized models (i.e., the population models without model error) were all orthogonal models with salient major factor loadings of 0.4, 15 items per factor, and between 1 and 10 common factors. The $\bSigma$ matrices were generated using the $\TKLrmsea$ method with a target RMSEA value of 0.09. To interpret Panel A of the figure, recall that the $\textrm{CFI} = 1 - F_t / F_b$ as defined in \autoref{eq:cfi}, where $F_t / F_b$ is the ratio of the minimized discrepancy function values for the hypothesized and baseline models. Panel A of \@ref(fig:cfi-as-factors-increase) shows that both $F_t$ and $F_b$ increased as the number of factors increased, holding everything else constant. However, $F_t$ grew fast enough that $F_t / F_b$ decreased as the number of factors increased, resulting in lower CFI values as RMSEA remained fixed (as shown in Panels B and C). To keep CFI constant as the number of factors increased, $F_t$ would have needed to increase at the same rate as $F_b$. For instance, Panel A includes a line indicating the values of $F_t$ required to produce a CFI value of 0.90 for each number of factors.

```{r cfi-as-factors-increase, out.width = "100%"}
#| fig.cap = "Panel A: Minimized discrepancy function values, CFI, and RMSEA values for $\\Sigma$ matrices generated from orthogonal models with salient major factor loadings of 0.4, 15 items per factor, and between 1 and 10 factors. The $\\Sigma$ matrices were generated using the $\\textrm{TKL}_{\\textrm{RMSEA}}$ model-error method with a target RMSEA value of 0.09. The line in the left-most panel labeled ``Target for CFI = 0.90'' indicates the value of $F_t$ that would be needed to obtain a CFI value of 0.90, given the value of $F_b$. Panels B and C: Observed CFI and RMSEA values for each simulated $\\Sigma$ matrix in Panel A."

if (make_plots) {
  # Show why CFI degrades as the dimensions of Sigma increase
  factors <- 1:10
  out <- cbind("factors" = factors, "Ft" = NA, "Fb" = NA, "cfi" = NA, "rmsea" = NA)
  
  set.seed(42)
  out <- lapply(
    X = factors, 
    FUN = function(factor_num) {
      m1 <- simFA(Model = list(NFac = factor_num, NItemPerFac = 15),
                  Loadings = list(FacLoadRange = .4,
                                  FacLoadDist = "fixed"))
      
      error_mod <- noisemaker(mod = m1, method = "TKL", target_rmsea = 0.09)
      
      Omega <- m1$Rpop
      Sigma <- error_mod$Sigma
      p <- nrow(Sigma)
      Ft <- log(det(Omega)) - log(det(Sigma)) + sum(diag(Sigma %*% solve(Omega))) - p
      Fb <- -log(det(Sigma))
      
      c(factors = factor_num, 
        Ft = Ft, 
        Fb = Fb, 
        cfi = cfi(Sigma, Omega), 
        rmsea = rmsea(Sigma, Omega, k = factor_num))
    }
  )
  
  out <- out %>% 
    bind_rows() %>% 
    mutate(target = .1 * Fb)
  
  out <- out %>% 
    pivot_longer(-factors, names_to = "func_type")
  
  labels <- list("Ft" = TeX("$\\mathit{F}_t$"),
                 "Fb" = TeX("$\\mathit{F}_b$"),
                 "target" = TeX("Target $\\mathit{F}_t$ for CFI=0.90"))
  
  p1 <- out %>% 
    filter(func_type %in% c("Fb", "Ft", "target")) %>% 
    ggplot(aes(x = factors, y = value, color = func_type, linetype = func_type)) + 
    geom_point(size = 1) +
    geom_line() +
    scale_x_continuous(breaks = c(2,4,6,8,10)) +
    scale_color_brewer(palette = "Dark2", type = "qual", labels = labels) +
    scale_linetype_discrete(labels = labels) +
    guides(color = guide_legend(title = ""), 
           linetype = guide_legend(title = "")) +
    labs(x = "Factors", y = "Minimized Discrepancy Function Value", 
         color = "Model", linetype = "Model") +
    theme_bw() +
    theme(legend.position = "bottom")
  
  p2 <- out %>% 
    filter(func_type == "cfi") %>% 
    ggplot(aes(x = factors, y = value)) + 
    geom_point(size = 1) +
    geom_line() +
    scale_x_continuous(breaks = c(2,4,6,8,10)) +
    labs(y = "CFI", x = "Factors") +
    theme_bw()
  
  p3 <- out %>% 
    filter(func_type == "rmsea") %>% 
    ggplot(aes(x = factors, y = value)) + 
    geom_point(size = 1) +
    geom_line() +
    scale_x_continuous(breaks = c(2,4,6,8,10)) +
    scale_y_continuous(limits = c(0, 0.1)) +
    labs(y = "RMSEA", x = "Factors") +
    theme_bw()
  
  design = "12\n13\n44"
  cfi_factors_plot <- p1 + p2 + p3 + 
    guide_area() + 
    plot_layout(design = design, guides = "collect", heights = c(4,4,1)) +
    plot_annotation(tag_levels = "A") &
    theme(plot.tag = element_text(size = 9))
  
  ggsave(filename = here("img/cfi_factors_plot.png"),
         plot = cfi_factors_plot,
         dpi = 320,
         height = 4.5,
         width = 7)
}

knitr::include_graphics(here("img/cfi_factors_plot.png"))
```

The effects of the model error introduced by the minor common factors can be seen in \@ref(fig:omega-wtw-sigma-example), which contains visual representations of the $\bOmega$, $\mathbf{WW}^\prime$, and $\bSigma$ matrices. The $\mathbf{WW}^\prime$ and $\bSigma$ matrices shown in the figure were obtained using the $\TKLrmsea$ method with a target RMSEA value of 0.09. The observed RMSEA and CFI values for $\bSigma$ were 0.09 and 0.25, respectively. Notice that the elements of the $\mathbf{WW}^\prime$ matrix in \@ref(fig:omega-wtw-sigma-example) were large (in absolute value) relative to the $\bOmega$ matrix. Also notice that $\bOmega$ had off-block-diagonal elements relative to the number of block-diagonal elements. Both of these features helped to explain the incompatibility between RMSEA and CFI for models with many factors, weak major common factor loadings, and target RMSEA and CFI values reflecting poor model fit. To get an RMSEA value close to the target value, all of the elements of $\bOmega$ had to be perturbed by making the elements of $\mathbf{WW}^\prime$ relatively large (in absolute value). The changes to the off-block-diagonal elements of $\bOmega$ were important in terms of the resulting CFI value because they were incompatible with the major common factor model (i.e., they should have been zero under that model). Therefore, they degraded the fit of the major factor model for $\bSigma$ while improving the fit of the independence model. Moreover, perturbing the off-block-diagonal elements had a larger effect on CFI as the number of factors increased because the number of off-block-diagonal elements increased more quickly than the number of block-diagonal elements as the total number of items increased. This can be seen in \@ref(fig:off-diag-elements), which shows the number of block diagonal elements and off-block-diagonal elements in the lower triangle of $\bSigma$ as the number of factors increased for an orthogonal model with five items per factor.

```{r omega-wtw-sigma-example, out.width='100%'}
#| fig.cap = "The population correlation matrix without model error ($\\bm{\\Sigma}$), the matrix of item correlations due to the minor common factors ($\\mathbf{WW}^\\prime$), and the population matrix with model error ($\\bm{\\Sigma}$) for an orthogonal model with ten major factors, five items per factor, and major common factor loadings fixed at .4. The RMSEA and CFI values for this example were 0.09 and 0.25."

if (make_plots) {
  c129 <- readRDS(here("data/results_129.RDS"))
  
  Sigma <- c129[[1]]$sigma_tkl_rmsea$value$Sigma
  W <- c129[[1]]$sigma_tkl_rmsea$value$W
  Omega <- fungible::simFA(
    Model = list(NFac = 10, NItemPerFac = 5),
    Loadings = list(FacLoadDist = "fixed",
                    FacLoadRange = .3)
  )$Rpop
  
  p3 <- Sigma %>%
    as_tibble() %>%
    rownames_to_column() %>%
    pivot_longer(cols = V1:V50) %>%
    rename("var1" = "rowname", "var2" = "name") %>%
    mutate(var2 = as.numeric(str_extract(var2, "[0-9]+")),
           var1 = as.numeric(var1)) %>%
    mutate(value = case_when(var1 == var2 ~ NA_real_,
                             var1 != var2 ~ value)) %>%
    ggplot(aes(x = fct_rev(as.factor(var2)), y = var1, fill = value)) +
    geom_raster() +
    colorspace::scale_fill_continuous_diverging("Purple-Green", 
                                                limits = c(-.4, .4)) +
    labs(fill = "Correlation") +
    theme_void()
  
  p2 <- tcrossprod(W) %>%
    as_tibble() %>%
    rownames_to_column() %>%
    pivot_longer(cols = V1:V50) %>%
    rename("var1" = "rowname", "var2" = "name") %>%
    mutate(var2 = as.numeric(str_extract(var2, "[0-9]+")),
           var1 = as.numeric(var1)) %>%
    mutate(value = case_when(var1 == var2 ~ NA_real_,
                             var1 != var2 ~ value)) %>%
    ggplot(aes(x = fct_rev(as.factor(var2)), y = var1, fill = value)) +
    geom_raster() +
    colorspace::scale_fill_continuous_diverging("Purple-Green", 
                                                limits = c(-.4, .4)) +
    labs(fill = "Correlation") +
    theme_void()
  
  p1 <- Omega %>%
    as_tibble() %>%
    rownames_to_column() %>%
    pivot_longer(cols = V1:V50) %>%
    rename("var1" = "rowname", "var2" = "name") %>%
    mutate(var2 = as.numeric(str_extract(var2, "[0-9]+")),
           var1 = as.numeric(var1)) %>%
    mutate(value = case_when(var1 == var2 ~ NA_real_,
                             var1 != var2 ~ value)) %>%
    ggplot(aes(x = fct_rev(as.factor(var2)), y = var1, fill = value)) +
    geom_raster() +
    colorspace::scale_fill_continuous_diverging("Purple-Green", 
                                                limits = c(-.4, .4)) +
    labs(fill = "Correlation") +
    theme_void()
  
  Omega_plus_WtW <- p1 + plot_spacer() + p2 + plot_spacer() + p3 + 
    plot_layout(guides = "collect", widths = c(4, 1, 4, 1, 4)) &
    theme(plot.margin = margin(t = 15, l = 5, r = 5))
  
  ggsave(Omega_plus_WtW, filename = here("img/Omega_plus_WtW.png"),
         dpi = 320,
         width = 7,
         height = 2.1,
         scale = 1.5)
}

knitr::include_graphics(here("img/omega_plus_WtW_with_annotations.png"))
```

```{r off-diag-elements, fig.align='center', out.width='60%'}
#| fig.cap = "The number of block-diagonal elements in the lower-triangle of $\\bm{\\Sigma}$ compared to the number of off-block-diagonal elements for an orthogonal model with five items per factor."
if (make_plots) {
  count_diag_off_diag <- function(items_per_factor, factors) {
    p <- items_per_factor * factors
    block_diag_elements <- (items_per_factor * (items_per_factor - 1) / 2) * factors
    off_diag_elements <- (p * (p - 1)) / 2
    off_block_diag_elements <- off_diag_elements - block_diag_elements
    
    list("block_diag_elements" = block_diag_elements,
         "off_block_diag_elements" = off_block_diag_elements)
  }
  
  factors <- 1:10
  
  off_diag <- count_diag_off_diag(items_per_factor = 5, factors = factors)
  off_diag <- as.data.frame(off_diag)
  off_diag$k <- factors
  off_diag <- pivot_longer(off_diag, 
                           cols = block_diag_elements:off_block_diag_elements, 
                           names_to = "type", 
                           values_to = "elements")
  
  off_diag %>%
    mutate(type = factor(type, 
                         levels = c("block_diag_elements", 
                                    "off_block_diag_elements"),
                         labels = c("Block-Diagonal",
                                    "Off-Block-Diagonal"))) %>%
    ggplot(aes(x = k, y = elements, color = type, 
               linetype = type, shape = type)) +
    geom_point() +
    geom_line(aes(group = type)) +
    scale_y_continuous(label = scales::comma) +
    scale_x_continuous(breaks = c(0, 2, 4, 6, 8, 10)) +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    theme_minimal() +
    labs(y = "Elements in Lower Triangle",
         x = "Factors",
         color = "", linetype = "", shape = "") +
    theme_bw() +
    theme(legend.position = "bottom")
  
  ggsave(filename = here("img/off-diagonal-elements.png"),
         dpi = 320,
         height = 3.5,
         width = 3.5)
}

knitr::include_graphics(here("img/off-diagonal-elements.png"))
```

----- END OF CUT MATERIAL FROM MODEL FIT AGREEMENT SECTION --------

For instance, in a test using an orthogonal model with 50 items (five factors, with ten items per factor) the CB method had a median completion time of almost 24 seconds over 10 trials, compared to less than a second for each of the TKL-based methods.^[wb-method-time]
[^wb-method-time]: The WB method had a median completion time of about nine seconds, but that timing includes the relatively computationally-expensive setup steps that need only be conducted once per model.

In addition to leading to the lowest average $D$ values and the highest rate of qualitative fit agreement, the $\TKLcfi$ method also avoided several issues that plagued the 

In addition to producing solutions with low $D$ values and high rates of qualitative fit agreement, there were at least two additional reasons to prefer the $\TKLcfi$ and $\TKLrmseacfi$ model-error methods to the alternatives. First, the TKL-based model-error methods (and the WB method) never produced indefinite $\bSigma$ matrices. As noted in \@ref(indefinite-matrices), indefinite $\bSigma$ matrices are unacceptable candidates for population correlation matrices with model error because all correlation and covariance matrices are at least positive semi-definite by definition [@wothke1993; @lorenzo-seva2020a; @kracht2022]. In contrast, the CB method often led to indefinite $\bSigma$ matrices, particularly in conditions with many factors, many items per factor, and Poor model fit. Even worse, the CB method became very slow as the number of factors and items increased. As a result, the CB method is likely an impractical choice for researchers who wish to generate large $\bSigma$ matrices with Poor model fit.

Both methods led to solutions without any major minor factors, despite the failure of the objective function penalty. On the other hand, the $\TKLrmsea$ method often led to solutions with major minor factors in some conditions. Although it is not possible to explicitly check for major minor factors with the CB and WB methods, neither method is able to avoid the problem. In conditions where the $\TKLrmsea$ method is likely to produce solutions with major minor factors, the CB method tended to produce solutions that were indefinite. Moreover, neither the CB or WB method provides any mechanism to avoid producing solutions that are better conceptualized as having more than the specified number of major common factors.

<!---
[] I recommend using the $\TKLcfi$ method because it led to the best agreement rates. However, using the $\TKLrmseacfi$ method should lead to very similar results.
[] Using the $\TKLcfi$ or $\TKLrmseacfi$ methods has at least three other advantages as well compared to the alternative model-error methods:
  [] Both methods led to solutions without any major minor factors, despite the failure of the objective function penalty. On the other hand, the $\TKLrmsea$ method often led to solutions with major minor factors in some conditions. Although it is not possible to explicitly check for major minor factors with the CB and WB methods, neither method is able to avoid the problem. In conditions where the $\TKLrmsea$ method is likely to produce solutions with major minor factors, the CB method tended to produce solutions that were indefinite. Moreover, neither the CB or WB method provides any mechanism to avoid producing solutions that are better conceptualized as having more than the specified number of major common factors.
  
[] Finally, I demonstrated that the $\TKLrmseacfi$ method was able to generate solutions with RMSEA and CFI values that were extremely close to the specified target RMSEA values ...

[] Limitations:
  [] None of the model error methods were able to generate solutions with some combinations of RMSEA and CFI values. Further work should be done to determine whether certain combination of RMSEA and CFI are, in fact, impossible for a particular $\bOmega$ value.
  [] When I designed the optimization procedure for the TKL methods, I assumed that researchers would not have any a priori beliefs about plausible ranges of $\epsilon$ and $\nu_e$. However, a committee member suggested that researchers might consider certain parameter values implausible in practice. For instance, for an orthogonal model with all salient major factor loadings fixed at 0.8, a $\nu_e$ value of .90 would indicate that 94.4% of the variance for each item was reliable, which is highly-implausible in most contexts. To allow for researchers to include their a priori beliefs about plausible ranges of $\nu_e$ and $\eps$, the L-BFGS-B box-constraints in the TKL optimization procedure could be specified by a user. This change would allow users to specify upper and lower bounds on either $\epsilon$ or $nu_e$, or both simultaneously. The implementations of the TKL methods in the `noisemaker` package have been updated to enable this functionality.
  [] It might also be the case that researchers are more interested in producing solutions with RMSEA and CFI values that fall within a particular range, rather than finding RMSEA and CFI values that are as close as possible to the provided target values. Future work should be done to develop and implement an appropriate objective function that will penalize solutions with RMSEA and CFI values that fall outside of provided ranges. [Need to think about whether this makes sense or not.]
  [] 
--->


<!---
Find a way to work in: all methods led to similar results (in terms of RMSEA, CFI, CRMR, and TLI) in conditions with few factors, strong factor loadings, and very good model fit. However, the methods led to very different results in conditons with many major factors, relatively weak factor loadings, and poor model fit.

1. Model-error methods led to different outcomes in terms of RMSEA and CFI values.
  a. RMSEA
    i. The model-error methods that only used RMSEA targets led to (unsurprisingly) similar results in terms of RMSEA. In particular, the TKL_RMSEA, and CB methods both produced solutions with RMSEA values that were very close to the target RMSEA values. The distributions of observed RMSEA values for the solutions produced by the WB method were centered around the target RMSEA value, but were more variable than the RMSEA distributions for the TKL_RMSEA and CB methods.
    ii. On the other hand, the TKL_CFI, and TKL_RMSEA_CFI methods typically led to observed RMSEA values that were not as close to the target RMSEA values. In many cases, TKL_CFI, and TKL_RMSEA_CFI led to RMSEA values that were somewhat lower than the target values, particularly in conditions with many factors. 
  b. CFI
    i. When evaluated on CFI, the model-error methods again fell into groups according to whether or not they incorporated a CFI target value. 
    ...
    
2. When assessing which of these methods to use in simulation work, there are other considerations. (A) Completion time; (B) Non-convergence; (C) Indefinite Matrices.

Future Research:

1. More flexible targets; target fit index ranges rather than fit index values.
2. Marsh et al. in search of golden rules: use more varied (realistic) model error methods to establish rule-of-thumb cutoff values.
3. The proposed model-error methods will give researchers the ability to generate more realistic population models with model error and user-specified fit index values. This could help us better understand how fit indices 
4. User-specified bounds of v and epsilon for noisemaker.
5. What do my results say about which model-error method should be preferred?
--->

Covariance structure models such as the common factor model are widely used in psychological research. To investigate the properties of these models and to understand how well various parameter estimation methods are able to estimate population parameter values, researchers often conduct Monte Carlo simulation studies by simulating data from a variety of population models, often incorporating sampling variability to help understand the role of sample size. However, generating data directly from population models has been criticized as unrealistic because the assumption that a population model will fit perfectly in the population will almost always be violated in practice. This discrepancy is known as "model error", and might be due to non-linearities in the relationships between factors and indicators, or due to the effects of large number of common factors that are too weak and too numerous to be of practical interest. Incorporating the effects of model error in Monte Carlo simulation studies is important because it makes simulated data sets more representative of empirical data sets, which are unlikely to have population covariance matrices that are perfectly fit by any simple covariance structure model. Therefore, the inclusion of model error should lead to results that generalize better to empirical settings. To that end, a few model-error methods have been proposed for simulating error-perturbed covariance matrices from the population covariance matrices implied by a covariance structure model. For instance, the Tucker, Koopman, and Linn (TKL; 1969), Cudeck and Browne (1992), and Wu and Browne (2015) model-error methods have variously been proposed.

Although the incorporation of model error in Monte Carlo simulation studies of covariance structure models (particularly the common factor analysis model) has become more common, there are no published comparisons of the model-error methods. Additionally, it is not clear how users should choose parameter values for each of the model-error methods to produce solutions with a particular degree of model fit. The CB and WB model-error methods each have a single user-specified parameter that allows the user to specify a desired root mean square error of approximation (RMSEA) value with varying precision. The TKL method has two user-specified parameters that are often chosen by trial-and-error to produce solutions with RMSEA values that are reasonably close to a desired value. This presents at least two problems: First, it is difficult and time-consuming to select TKL parameter values that lead to acceptable RMSEA values over the wide variety of common factor models often used in Monte Carlo simulation studies. Second, RMSEA alone is not sufficient for a complete understanding of the degree of model fit. In the following dissertation, I address these issues as follows. First, I review the psychometric literature on model error, introduce the TKL, CB, and WB model-error methods, and describe four commonly-used fit indices that provide 



- Model error methods don't provide an easy way to specify the degree of model misfit to add. Although researchers have often used the root mean square error of approximation (RMSEA) value as the sole indicator of model misfit, no fit index is 

<!-- Resolution; How does this work solve the problem? -->

<!-- Epilogue; Broader picture and description of impact -->

Simulating data sets 

Incorporating the effects of model error in Monte Carlo simulation studies is important because it makes simulated data sets more representative of empirical data sets, which are unlikely to have population covariance matrices that are perfectly fit by any simple covariance structure model. To that end, a few model-error methods have been proposed for simulating error-perturbed covariance matrices from the population covariance matrices implied by a covariance structure model. For instance, the Tucker, Koopman, and Linn (TKL; 1969), Cudeck and Browne (1992), and Wu and Browne (2015) model-error methods have variously been proposed.

Although the incorporation of model error in Monte Carlo simulation studies of covariance structure models (particularly the common factor analysis model) has become more common, there are no published comparisons of the model-error methods. Additionally, it is not clear how users should choose parameter values for each of the model-error methods to produce solutions with a particular degree of model fit. The CB and WB model-error methods each have a single user-specified parameter that allows the user to specify a desired root mean square error of approximation (RMSEA) value with varying precision. The TKL method has two user-specified parameters that are often chosen by trial-and-error to produce solutions with RMSEA values that are reasonably close to a desired value. This presents at least two problems: First, it is difficult and time-consuming to select TKL parameter values that lead to acceptable RMSEA values over the wide variety of common factor models often used in Monte Carlo simulation studies. Second, RMSEA alone is not sufficient for a complete understanding of the degree of model fit. In the following dissertation, I address these issues as follows. First, I review the psychometric literature on model error, introduce the TKL, CB, and WB model-error methods, and describe four commonly-used fit indices that provide 


```{r}
if (make_plots) {
  cb_nonconverged <- results_matrix %>% 
    filter(error_method == "CB") %>% 
    mutate(delta = rmsea_thetahat - target_rmsea) %>%
    group_by(factors, items_per_factor_rec, loading_numeric, model_fit_rec) %>% 
    summarise(percent_nonconverged = mean(abs(delta) >= .01, na.rm = TRUE)) %>% 
    mutate(factors = as.factor(factors)) %>% 
    ggplot(aes(x = loading_numeric, y = percent_nonconverged, color = factors,
               group = factors, shape = factors)) +
    geom_point() +
    geom_line() +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    facet_grid(items_per_factor_rec ~ model_fit_rec) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    labs(x = "Loading", y = "Percent Non-Converged", color = "Factors",
         shape = "Factors") +
    theme_bw()
  
  ggsave(plot = cb_nonconverged,
         filename = here("img/cb_nonconverged.png"),
         dpi = 320,
         height = 4, 
         width = 6.5)
}

# knitr::include_graphics(here("img/cb_nonconverged.png"))
```





### Percent of Cases with Qualitative Fit Index Agreement {#fit-index-agreement-table}

(ref:fit-agreement-table-caption) The percent of cases where the observed RMSEA and CFI values led to the same qualitative evaluation of model fit based on the threshold values suggested by Hu and Bentler [-@hu1999].

```{r fit-agreement-table}
fit_agreement_table <- results_matrix %>%
  mutate(
    rmsea_subj_fit = case_when(rmsea <= 0.05 ~ "Good",
                               rmsea > 0.05 & rmsea <= 0.1 ~ "Acceptable",
                               rmsea > 0.1 ~ "Unacceptable"),
    cfi_subj_fit = case_when(cfi >= .95 ~ "Good",
                             cfi < .95 & cfi >= .9 ~ "Acceptable",
                             cfi < .9 ~ "Unacceptable")
  ) %>% mutate(
    fit_agreement = (rmsea_subj_fit == cfi_subj_fit)
  ) %>% group_by(factors, loading_numeric, model_fit, error_method) %>%
  summarize(agreement_rate = mean(fit_agreement, na.rm = TRUE)) %>%
  pivot_wider(names_from = error_method, 
              values_from = agreement_rate) %>%
  ungroup()

fit_agreement_table %>%
    mutate(across(.cols = c(`TKL (RMSEA)`:WB), ~ . * 100)) %>%
  apa_table(col.names = c("Factors", "Loading", "Model Fit", 
                          "$\\textrm{TKL}_{\\textrm{RMSEA}}$", 
                          "$\\textrm{TKL}_{\\textrm{CFI}}$", 
                          "$\\textrm{TKL}_{\\textrm{RMSEA/CFI}}$", 
                          "CB", "WB"),
            digits = c(0, 1, 0, 1, 1, 1, 1, 1),
            align = "rrlrrrrr",
            col_spanners = list("Qualitative Fit Agreement (\\%)" = c(4, 8)),
            format.args = list("na_string" = "---"),
            caption = "(ref:fit-agreement-table-caption)",
            label = "fit-agreement-table",
            font_size = "small",
            note = "TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.")
```
