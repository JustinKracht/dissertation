---
# Required Information ------------------------------------
title: "Make Some Noise: Methods for Generating Data From Imperfect Factor Models"
author: "Justin D. Kracht"
month: "July"
year: "2022"
advisor: "Niels G. Waller"
phd: true

# Rendering options ---------------------------------------
knit: "bookdown::render_book"
output: 
  gopherdown::thesis_pdf:
    latex-engine: xelatex

# Fonts ---------------------------------------------------
mainfont: "Times New Roman"
sansfont: "Arial"
monofont: "Courier"

# Link highlighting ---------------------------------------
link-citations: true
colored-not-bordered-links: true
urlcolor-hex: "000000"   # web addresses
citecolor-hex: "000000"  # citations
linkcolor-hex: "000000"  # links to sections in your thesis

# Bibliography/References ---------------------------------

bibliography: ["bib/dissertation.bib"]
csl: "csl/apa7.csl"

# ----------------- Create List of tables and Figures --------------------------

lot: true
lof: true

# ----------------- Optional Frontmatter --------------------------------------

acknowledgements: >
  `r if(knitr:::is_latex_output()) gopherdown::inc("pre/00-acknowledgements.Rmd")`
dedication: >
  `r if(knitr:::is_latex_output()) gopherdown::inc("pre/00-dedication.Rmd")`
abstract: >
  `r if(knitr:::is_latex_output()) gopherdown::inc("pre/00-abstract.Rmd")`
---

```{r setup, echo=FALSE, cache=FALSE, message=FALSE}
# Load libraries
library(papaja)
library(fungible)
library(tidyverse)
library(bookdown)
library(microbenchmark)
library(patchwork)
library(latex2exp)
library(gghighlight)
library(kableExtra)
library(here)
library(scales)
library(purrrgress)
library(texreg)
library(noisemaker) # devtools::install_github("JustinKracht/noisemaker")

# Set knitr chunk options
knitr::opts_chunk$set(
  echo = FALSE, 
  message = FALSE,
  warning = FALSE,
  cache = TRUE, 
  fig.align = "center", 
  dpi = 320
)

# Flags indicating whether or not new plots and tables should be generated
make_plots <- FALSE # Should new plots be generated?
make_tabs <- FALSE # Should new tables be generated?
```

```{r r-citations, include = FALSE, eval = FALSE}
# Generate a bib file citing all of the R packages that were used
sim_deps <- renv::dependencies(here("../model-error-simulation"))$Package
writing_deps <-  renv::dependencies(here())$Package

package_deps <- c(unique(c(sim_deps, writing_deps)), "thesisdown")
create_bib(file = here("bib/r-refs.bib"),
           append = TRUE,
           x = package_deps)
```

<!--chapter:end:index.Rmd-->

# Introduction {#intro}

Covariance structure models (also called structural equation models) are widely used in psychological research [@bollen1989]. These models allow a structured covariance matrix to be represented as a matrix-valued function of a vector of parameters such that $\bm{\Omega}= \bm{\Omega}(\bm{\gamma})$, where $\bm{\Omega}$ is a $p \times p$ covariance matrix and $\bm{\gamma}$ is a vector of $h$ free model parameters. Stated another way, covariance structure models attempt to represent the structural connections between a set of unobserved latent variables (factors) and a set of observed variables that are indicators of the latent variables. When assessing model fit or estimating the dispersion of the estimated structural parameters, $\hat{\bm{\gamma}}$, researchers traditionally assume that the model holds perfectly in the population. It is assumed that there exists some vector $\bm{\gamma} = \bm{\gamma}_0$ such that the population covariance matrix $\bm{\Omega}$ can be perfectly reproduced (i.e., $\bm{\Omega} = \bm{\Omega}(\bm{\gamma}_0)$). If the model fits perfectly in the population, differences between the sample covariance matrix and the corresponding model-implied covariance matrix can only be due to sampling variability. 

However, the assumption that the covariance structure is correctly specified in the population will nearly always be violated in practice [@maccallum1991; @maccallum2001; @browne2002; @meehl2002; @cudeck1991; @tomarken2003]. After all, "no mathematical model will fit real-world phenomena exactly" [@maccallum2001, p. 503] due, for example, to non-linearities in the relationships between factors and indicators, or to the effect of numerous minor factors of little theoretical interest [@cudeck1992]. The idea that all models are imperfect representations of reality has been expressed many ways, perhaps most succinctly by Box's aphorism that "all models are wrong, but some are useful" [-@box1987, p. 424]. Further interesting discussions of the idea that models are only useful approximations of reality (and are often useful *because* they are approximations) can be found in both scientific and popular literature [@carroll1894; @eco1994; @borges1998; @gelman2008; @steele2008; @nester1996].

Recognizing that all models are literally false, and thus no covariance structure model will fit perfectly in the population, several authors have suggested that estimation error for these models can be attributed to two sources: error due to sampling variability and error due to model misfit [@tucker1969; @cudeck1991; @browne1992]. This second type of error has been variously referred to as *model error* [@tucker1967evaluation; @tucker1969; @maccallum2001], *error of approximation* [@cudeck1991], *specification error* [@satorra2015], or *adventitious error* [@wu2015]. Although some of these terms are related to specific views regarding the nature of model error [c.f. @wu2015; @tucker1969], all of the terms refer to the discrepancy between the model-implied covariance matrix, $\bm{\Omega}$, and the error-perturbed population covariance matrix, $\bm{\Sigma} = \bm{\Omega} + \mathbf{E}$ (where $\mathbf{E}$ is a symmetric matrix representing the effects of model error). In this proposal, the term "model error" will generally be used to describe the discrepancy between $\bm{\Sigma}$ and $\bm{\Omega}$.

Acknowledging model error is important because it can have significant implications for estimating covariance structure models. For instance, consider the traditional Chi-square test of exact model fit. Given a sample covariance matrix $\mathbf{S}$, the minimum objective function value for a hypothesized model $\hat{F} = F(\mathbf{S}, \bm{\Omega}(\hat{\bm{\gamma}}))$ obtained by minimizing a discrepancy function $F(\mathbf{S}, \bm{\Omega}(\bm{\gamma}))$ is assumed to follow a central Chi-square distribution when multiplied by $n = N - 1$, where $N$ denotes the sample size [@olsson2004].[^ml-vs-ols] However, the test requires two stringent assumptions that are unlikely to both be satisfied in empirical settings: (a) that the observed variables are multivariate normal, and (b) that the model fits perfectly in the population [i.e., there is no model error\; @browne1984]. If (b) is not satisfied, then the test statistic $n \hat{F}$ will not follow a central Chi-square distribution and will lead to incorrect tests [@olsson2004]. Moreover, Chi-square tests of exact fit are sensitive to sample size, with large sample sizes leading to almost certain rejection of the model, even with small amounts of misspecification [@yuan2004; @bentler1980; @tomarken2003]. In any case, testing a model that is known not to be perfectly true against the null hypothesis of perfect fit would seem to have limited usefulness [@browne1992; @steiger2007]. 

[^ml-vs-ols]: The maximum likelihood discrepancy function is commonly used, but other common discrepancy functions (e.g., generalized least squares, weighted least squares, asymptotically distribution free) will converge to the same minimum discrepancy values when the model is correctly specified and the observed variables are multivariate normal [@olsson2004].

Model error also has implications for covariance structure modeling beyond global tests of model fit. For instance, traditional methods of computing confidence intervals for model parameters assume that all error is sampling error (i.e., the model fits perfectly in the population). Thus, confidence intervals produced using these methods are overly-optimistic when model error is present [@wu2015]. Simulation studies have shown that the presence of model error can also impact parameter estimation for exploratory factor analysis [@briggs2003], dimensionality identification [@kracht2022], the behavior of confidence regions and fungible parameter estimates for structural equation models [@pek2012], and is important in other contexts as well [@trichtinger2020; @beauducel2016; @dewinter2016; @gnambs2016; @hsu2015].

These simulation studies represent a recent trend toward incorporating model error in Monte Carlo simulation studies involving covariance structure models. Including model error in these studies is important for at least two reasons. First, the addition of model error makes simulated data sets more representative of empirical data sets, which almost certainly do not have population covariance matrices that are perfectly fit by any simple covariance structure model. Therefore, the inclusion of model error should lead to results that are more generalizable to empirical settings compared to Monte Carlo studies that do not include model error. Second, incorporating model error allows researchers to evaluate the robustness of methods when covariance structure models do not hold exactly.

Recognizing the importance of incorporating model error in Monte Carlo simulation studies, various authors have introduced methods for generating population covariance matrices with imperfect model fit. The three most popular of these methods were proposed by (a) Tucker, Koopman, and Linn [TKL\; -@tucker1969], (b) Cudeck and Browne [CB\; -@cudeck1992], and (c) Wu and Browne [WB\; -@wu2015].

## Model-Error Methods

### The Tucker, Koopman, and Linn Method {#tkl-method}

One of the first model-error methods was developed by @tucker1969 in the context of common factor analysis. The common factor analysis model in terms of $p$ manifest variables and $k$ common factors can be written as

\begin{equation}
\bm{\Omega} = \bm{\Lambda}\bm{\Phi}\bm{\Lambda}^\prime + \bm{\Psi},
(\#eq:cfm)
\end{equation}

\noindent where $\bm{\Omega}$ is the model-implied $p \times p$ covariance matrix, $\bm{\Lambda}$ is the $p \times k$ factor-pattern matrix, $\bm{\Phi}$ is the $k \times k$ common factor covariance matrix, and $\bm{\Psi}$ is the $p \times p$ diagonal matrix containing the uniqueness variances. For convenience, we can define all observed variables and common factors as having means of zero and standard deviations of one so that $\bm{\Omega}$ has a unit diagonal (i.e., is a correlation matrix). @tucker1969 proposed extending the common factor model to include many minor common factors of small effect in addition to the major common factors to represent "unsystematic or unknown aspects of the process that generates the data" [@cudeck1992, p. 358]. The model they proposed can be written as

\begin{equation}
\bm{\Sigma} = \bm{\Lambda}\bm{\Phi}\bm{\Lambda}^\prime + \bm{\Psi} + \mathbf{WW}^\prime,
(\#eq:cfm-tkl)
\end{equation}

\noindent where $\bm{\Sigma}$ is the $p \times p$ error-perturbed population covariance matrix, $\mathbf{W}$ is the $p \times q$ ($q \gg k$) matrix of minor common factor loadings, and all other terms are as previously defined. For convenience, we can define the observed variables to be in standard score form such that $\bm{\Sigma}$ has a unit diagonal. The combined influence of the $q$ minor common factors represents the reliable common variance (and covariance) not accounted for by the major common factors and is considered to be due to model error. Although @tucker1969 do not give a specific recommendation for the number of minor common factors to incorporate, values of $q$ between 50 and 150 are common [@kracht2022; @trichtinger2020; @briggs2003; @jung2008regularized; @cai2013].

When using the TKL method, it is common to specify the amount of model error to include as the proportion of the uniqueness factor variances, or "uniqueness variances", that is reapportioned to the minor common factors. The proportion of the uniqueness variances that is reapportioned and how that variance is distributed among the minor common factors are determined by two user-specified parameters, $\nu_{\textrm{e}} \in [0,1]$ and $\epsilon \in [0,1]$, respectively. To create the matrix of minor factor loadings ($\mathbf{W}$), a $p \times q$ provisional matrix, $\mathbf{W}^*$, is first created such that the $i$th column of $\mathbf{W}^*$ ($i \in [1..q]$)[^integer-range-notation] consists of $p$ independent samples from $\mathcal{N}(0, (1 - \epsilon)^{2(i-1)})$, where $\mathcal{N}(0, (1 - \epsilon)^{2(i-1)})$ denotes a normal distribution with a mean of zero and a variance of $(1 - \epsilon)^{2(i-1)}$. Because the standard deviation of the $i$th column of $\mathbf{W}^*$ is given by $(1 - \epsilon)^{i - 1}$, values of $\epsilon$ close to zero result in columns with relatively equal variance, corresponding to approximately equipotent minor factors. On the other hand, values of $\epsilon$ close to one result in error variance primarily being distributed to the first minor factor, with the remaining variance distributed to the other minor factors in a decreasing geometric sequence. 

[^integer-range-notation]: When $a$ and $b$ are integers, the notation $[a..b]$ is used here to denote the interval of integers between $a$ and $b$, inclusive.

To ensure that the minor common factors account for the specified proportion of uniqueness variance ($\nu_{\textrm{e}}$), $\mathbf{W}^*$ is then scaled to create $\mathbf{W}$. This scaling is done in several steps. First, a $p \times p$ diagonal matrix $\bm{\Psi}^*$ is created such that

\begin{equation}
\bm{\Psi}^* = \mathbf{I}_p - \dg(\bm{\Lambda} \bm{\Phi} \bm{\Lambda}^\prime),
(\#eq:psi-star)
\end{equation} 

\noindent where $\dg(\bm{\Lambda}\bm{\Phi}\bm{\Lambda}^\prime)$ is the diagonal matrix formed from the diagonal entries in $\bm{\Lambda}\bm{\Phi}\bm{\Lambda}^\prime$ and $\mathbf{I}_p$ denotes a $p \times p$ identity matrix. Then the matrix $\mathbf{W}$ is formed using

\begin{equation}
\mathbf{W} = (\dg(\mathbf{W}^* \mathbf{W}^{* \prime})^{-1} \bm{\Psi}^* \nu_{\textrm{e}})^{1/2} \mathbf{W}^*.
(\#eq:W-matrix)
\end{equation}

\noindent This process ensures that the $q$ minor common factors account for the specified proportion of the variance not accounted for by the major common factors. The $\mathbf{W}$ matrix can then be used to create the diagonal matrix of unique variances, $\bm{\Psi} = \mathbf{I}_p - \dg(\bm{\Lambda} \bm{\Phi}\bm{\Lambda}^\prime + \mathbf{WW}^\prime)$. The $\bm{\Lambda}$, $\bm{\Psi}$, and $\mathbf{W}$ matrices are then used to construct the population correlation matrix (with model error), $\bm{\Sigma}$, as shown in \autoref{eq:cfm-tkl}. The TKL method is one of the most widely-used methods for generating correlation matrices with imperfect model fit [@lorenzo-seva2020; @kracht2022; @chung2019; @beauducel2016; @lorenzo-seva2016; @dewinter2016; @gnambs2016; @myers2015a].[^1] 

[^1]: @tucker1969 was cited 204 times as of July 28, 2022, according to citation counts provided by Web of Science.

Although the original TKL method is still most commonly used in simulation studies, at least two variants of this method have since been developed. First, @hong1999 introduced a variation of the TKL method that allowed for minor common factors that were correlated with each other and with the major common factors. In Hong's model, the population covariance matrix can be written as

\begin{equation}
\bm{\Sigma} = \mathbf{L} \mathbf{C} \mathbf{L}^\prime + \bm{\Psi},
\end{equation}

\noindent where $\mathbf{L} = \begin{bmatrix} \bm{\Lambda} & \mathbf{W} \end{bmatrix}$ is the super matrix containing the major and minor factor loadings, and $\bm{\Psi}$ is the diagonal matrix of uniqueness variances, as defined in \autoref{eq:cfm}. The matrix $\mathbf{C}$ is the correlation matrix for the major and minor factors such that

\begin{equation}
\mathbf{C} = \begin{bmatrix} \bm{\Phi} & \bm{\Upsilon} \\ \bm{\Upsilon}^\prime & \bm{\Gamma} \end{bmatrix},
\end{equation}

\noindent where $\bm{\Phi}$ is the $k \times k$ major factor correlation matrix, $\bm{\Upsilon}$ is the $k \times q$ matrix of correlations between the major and minor factors, and $\bm{\Gamma}$ is the $q \times q$ minor factor correlation matrix.

Hong's [-@hong1999] article has been cited a number of times since its publication[^2], but only one published study has applied Hong's method [@porritt2015]. In Porritt's [-@porritt2015] simulation study, all major and minor factor correlations were fixed at .3, following the example given by Hong [-@hong1999]. However, neither Porritt [-@porritt2015] nor Hong [-@hong1999] directly compared Hong's method with the original TKL method. Therefore, it remains unclear whether modeling major-minor factor correlations is important when simulating covariance matrices with model error.

[^2]: @hong1999 was cited twelve times as of March 11, 2021, according to Web of Science.

A second variant of the TKL method was recently introduced by Trichtinger and Zhang [-@trichtinger2020]. Specifically, Trichtinger and Zhang's method adapted the TKL method to allow the simulation of covariance matrices with model error for multivariate time series data. Trichtinger and Zhang [-@trichtinger2020] first introduced a novel test statistic appropriate for P-technique factor analysis [@cattell1947]. They then used their adapted TKL method to generate data with model error in a Monte Carlo simulation study to evaluate the empirical characteristics of their test statistic. Although their adapted TKL method has not yet been used in any other published simulation studies, it demonstrates that it is possible to extend the TKL method beyond the simple common factor model to other types of covariance structure models.

### The Cudeck and Browne (CB; 1992) Method {#cb}

An alternative to the TKL method for modeling imperfect model fit in simulation studies was described by Cudeck and Browne [-@cudeck1992]. These authors agreed with @tucker1969 that no simple factor analysis model is likely to fit exactly in the population and that Monte Carlo simulation studies conducted to evaluate statistical methods should incorporate model error to test robustness against imperfect model fit. However, Cudeck and Browne wanted a model-error method that was more flexible than the TKL method and that allowed the user to specify the desired discrepancy function value to control the amount of model error. Therefore, Cudeck and Browne proposed a new method of generating data from models with imperfect fit, building on the work of @tucker1969. They developed their new approach to satisfy three desiderata: First, that the approach is general to covariance structure models and not only factor analysis models; Second, that the approach allows the pre-specification of the amount of model error in terms of the maximum likelihood or least squares discrepancy function value, $F(\bm{\Sigma}, \bm{\Omega}(\bm{\gamma})) = \delta$; Third, that the minimizer of the discrepancy function is a specified vector of model parameters, $\bm{\gamma} = \bm{\gamma_0}$. 

The Cudeck and Browne method (referred to as CB hereafter) works as follows. A $p \times p$ population covariance matrix is defined as the sum

\begin{equation}
\bm{\Sigma} = \bm{\Omega}(\bm{\gamma}) + \mathbf{E},
(\#eq:cb)
\end{equation}

\noindent where $\bm{\Omega}(\bm{\gamma})$ is a matrix-valued function of a vector of parameters, $\bm{\gamma}$, and $\mathbf{E}$ is a $p \times p$ symmetric matrix such that \autoref{eq:cb} is positive definite. Moreover, let $\bm{\Sigma}_0 = \bm{\Omega}(\bm{\gamma}_0) + \mathbf{E}$ be the population covariance matrix for a particular vector of model parameters, $\bm{\gamma}_0$. The CB method works by finding an $\mathbf{E}$ matrix such that the discrepancy function $F(\bm{\Sigma}_0, \bm{\Omega}(\bm{\gamma}_0))$ is minimized at $\bm{\gamma}_0$ and the minimum is equal to a pre-specified value, $\delta$. Cudeck and Browne (1992) considered discrepancy functions of the form

\begin{equation}
F(\bm{\Sigma}, \bm{\Omega}(\bm{\gamma})) = \frac{1}{2} \tr [\mathbf{Z}^{-1}(\bm{\Sigma} - \bm{\Omega}(\bm{\gamma}))^2],
(\#eq:disc-fun)
\end{equation}

\noindent where the fixed $p \times p$ matrix $\mathbf{Z}$ does not depend on $\mathbf{E}$. Note that when $\mathbf{Z} = \mathbf{I}_p$, \autoref{eq:disc-fun} is the discrepancy function for ordinary least squares. The discrepancy function for ordinary-theory maximum likelihood can be written as

\begin{equation}
F_{\textrm{ML}}(\bm{\Sigma}, \bm{\Omega}(\bm{\gamma})) = \ln |\bm{\Omega}(\bm{\gamma})| - \ln |\bm{\Sigma}| + \tr [\bm{\Sigma} \bm{\Omega}(\bm{\gamma})^{-1}] - p,
(\#eq:disc-ml)
\end{equation}

\noindent where $|\bm{\Sigma}|$ is the determinant of $\bm{\Sigma}$. Cudeck and Browne [-@cudeck1992] showed that the minimizer of \autoref{eq:disc-fun} is the same as the minimizer of \autoref{eq:disc-ml} when $\mathbf{Z} = \bm{\Omega}(\bm{\gamma}_{\textrm{ML}})$, where $\bm{\gamma}_{\textrm{ML}}$ is the minimizer of the maximum likelihood discrepancy function in \autoref{eq:disc-ml}. Thus, \autoref{eq:disc-fun} is a general form of the discrepancy function that is equivalent to either the least squares discrepancy function or the maximum likelihood discrepancy function, depending on the value of $\mathbf{Z}$. 

Recall that one objective of the CB method is to ensure that the discrepancy function $F(\bm{\Sigma}_0, \bm{\Omega}(\bm{\gamma}))$ is minimized when $\bm{\gamma} = \bm{\gamma}_0$. To do this, it is necessary to find an $\mathbf{E}$ matrix such that the gradient $\partial F(\bm{\Sigma}_0, \bm{\Omega}(\bm{\gamma})) / \partial \bm{\gamma} = \bm{0}$. Cudeck and Browne [-@cudeck1992] showed the gradient can be written as

\begin{equation}
\frac{\partial F(\bm{\Sigma}_0, \bm{\Omega}(\bm{\gamma}))}{\partial \bm{\gamma}} = \mathbf{B}^\prime \tilde{\mathbf{e}}
(\#eq:gradient-of-F)
\end{equation}

\noindent where $\mathbf{B}$ is a $\frac{1}{2}(p^2 + p) \times h$ matrix that depends on both $\mathbf{Z}$ and $\dot{\bm{\Sigma}}_i = [\partial \bm{\Sigma}(\bm{\gamma})/ \partial \gamma_i]$, and $\tilde{\mathbf{e}} = \textrm{vecs} \: [ \bm{\Sigma}_0 - \bm{\Omega}(\bm{\gamma}) ]$ is a $\frac{1}{2}(p^2 + p) \times 1$ vector.[^defn-of-B] The $\vecs$ operator is defined such that for a symmetric matrix, $\mathbf{A}$, $\vecs \mathbf{A} = \begin{bmatrix} a_{11} & a_{12} & a_{22} & a_{13} & \dots & a_{pp} \end{bmatrix}^\prime$. Put another way, the $\vecs \mathbf{A}$ operator returns a vector of the stacked upper-diagonal elements of $\mathbf{A}$ (including the diagonal). Written in this form, the gradient can be set equal to the null vector, $\mathbf{B}^\prime \mathbf{\tilde{e}} = \bm{0} |_{\bm{\gamma = \bm{\gamma}_0}}$ and solved for $\tilde{\mathbf{e}} = \vecs \tilde{\mathbf{E}}$. To find a suitable $\mathbf{\tilde{e}}$, let $\mathbf{y}$ be a non-null $\frac{1}{2}(p^2 + p) \times 1$ vector. Then the difference $\mathbf{\tilde{e}} = \mathbf{y} - \mathbf{B(\mathbf{B}^\prime \mathbf{B})^{-1} \mathbf{B}^\prime \mathbf{y}}$ gives an $\mathbf{\tilde{e}}$ vector such that 

\begin{equation}
\mathbf{B}^\prime \mathbf{\tilde{e}} = \bm{0} |_{\bm{\gamma = \bm{\gamma}_0}}. 
(\#eq:proj-y-on-B)
\end{equation}

\noindent In plain language, $\mathbf{\tilde{e}}$ is the residual vector between $\mathbf{y}$ and the projection of $\mathbf{y}$ onto the column space of $\mathbf{B}$ given by $\mathbf{B(\mathbf{B}^\prime \mathbf{B})^{-1} \mathbf{B}^\prime \mathbf{y}}$. This residual vector is orthogonal to the column space of $\mathbf{B}$ [@strang2016, p. 208], so \autoref{eq:proj-y-on-B} is satisfied. Note that the $\mathbf{y}$ vector is arbitrary and that different $\mathbf{y}$ vectors correspond to different $\tilde{\mathbf{e}}$ values.[^ml-vs-ols-y-choice]

[^defn-of-B]: For a complete description of how to obtain the $\mathbf{B}$ matrix, see Cudeck and Browne [-@cudeck1992]. 

[^ml-vs-ols-y-choice]: According to Cudeck and Browne [-@cudeck1992], $\mathbf{y}$ is arbitrary when maximum likelihood is used, but must be chosen more carefully when OLS is used to avoid indefinite solutions.

After finding $\tilde{\mathbf{e}}$ (and correspondingly, $\tilde{\mathbf{E}}$) such that $F(\bm{\Sigma}_0, \bm{\Omega}(\bm{\gamma}))$ is minimized at $\bm{\gamma} = \bm{\gamma}_0$, the next step is to ensure that the minimum is equal to a specified value, $\delta$. The population Root Mean Square Error of Approximation [RMSEA\; @steiger1990] value is related to the objective function value by $\varepsilon = \sqrt{F(\bm{\Sigma}_0, \bm{\Omega}(\bm{\gamma}))/df}$, where $\varepsilon$ and $df$ denote the RMSEA and model degrees of freedom, respectively. Therefore, $\delta$ is generally selected to produced a desired RMSEA value such that $\delta = \varepsilon^2 df$. Cudeck and Browne defined the error matrix as $\mathbf{E} = \kappa \tilde{\mathbf{E}}$, where $\kappa$ is a scaling term that is chosen so that the value of the objective function at its minimum is equal to $\delta$. When $\kappa = 0$, $\bm{\Sigma} = \bm{\Omega}$, whereas larger values of $\kappa$ lead to larger discrepancy function values. Cudeck and Browne (1992) furthermore proved that $\bm{\gamma}_0$ is the global minimizer of $F(\bm{\Sigma}_0, \bm{\Omega}(\bm{\gamma}))$ as long as $\kappa$ is not too large (pp. 360--361). Thus, their second desideratum (i.e., that $\bm{\gamma}_0$ be the minimizer of the objective function) and third desideratum (i.e., that the value of the discrepancy function at its minimum is $\delta$) are both satisfied.

The CB method is appealing for use in simulation studies for a number of reasons. First, it allows the user to specify a desired RMSEA value. Second, unlike the TKL method, the CB method does not have any tuning parameters that need to be chosen (other than the target RMSEA). Third, the CB method is easily extendable to many types of covariance structure models. Likely because of these advantages, the CB method has been used in a relatively large number of Monte Carlo simulation studies [@lai2017; @lai2018; @lai2019a; @lai2020; @lai2020b; @lai2020a; @lai2020c; @montoya2020; @xia2021].

Although it is appealing for simulation work, the CB method carries with it an assumption about the nature of model error that might or might not be considered reasonable for empirical data sets. Recall that a requirement of the CB method is that the discrepancy function $F(\bm{\Sigma}_0, \bm{\Omega}(\bm{\gamma}_0))$ is minimized when $\bm{\gamma} = \bm{\gamma}_0$. When a model is mis-specified, there does not exist any $\bm{\gamma}_0$ such that $\bm{\Sigma}_0 = \bm{\Omega}(\bm{\gamma}_0)$. However, the maximum likelihood parameter estimate $\hat{\bm{\gamma}}_{\textrm{ML}}$ is still consistent toward the minimizer of the maximum likelihood discrepancy function under mild regularity conditions [@wu2015; @shapiro1983, Theorem 5.4; @shapiro2007, Section 5.3]. Moreover, if a sample covariance matrix $\mathbf{S}$ is an unbiased estimator of $\bm{\Sigma}_0$, then the expected value of $\mathbf{S}$ is $\bm{\Sigma}_0$. Taken together, these properties indicate that maximum likelihood parameter estimates converges to $\bm{\gamma}_0$ as $N \to \infty$ under the CB framework. 

This result reflects the view that the "true" population parameter values are simply the parameter values obtained by fitting a model to $\bm{\Sigma}$ using a particular discrepancy function. In this view, the discrepancy between the $\bm{\Sigma}$ and the model-implied covariance matrix obtained from analyzing $\bm{\Sigma}$ (i.e., $\hat{\bm{\Omega}}$) is of primary interest, not the discrepancy between $\bm{\Sigma}$ and the implied covariance matrix for some ideal model ($\bm{\Omega}$). Because the CB method ensures that $\bm{\gamma}_0$ is a minimizer of the discrepancy function, $\hat{\bm{\Omega}} = \bm{\Omega}$. Thus, in this view it makes sense that the population parameters ($\bm{\gamma}_0$) will be perfectly recovered as $N \to \infty$.

### The Wu and Browne (WB; 2015) Method

A third model-error method was introduced by Wu and Browne [-@wu2015] and is unique among the three approaches because it represents model error as a random effect rather than as a fixed quantity. Moreover, the TKL and CB model-error methods were developed for use in simulation studies. In contrast, the WB method was motivated by Wu and Browne's development of a method for estimating confidence intervals for model parameters, taking into account variability due to model error.

Before describing Wu and Browne's [-@wu2015] estimation method, it will be useful to define two important terms. The term *model discrepancy*, as used by Wu and Browne [-@wu2015], corresponds to what has been previously referred to as model error (i.e., the difference between the error-perturbed and model-implied population covariance matrices). The term *adventitious error* is used to describe the process underlying model discrepancy [@wu2015]. In Wu and Browne's [-@wu2015] conceptualization, model discrepancy arises from differences between two populations: an operational population from which the observed sample is representative, and a theoretical general population. The theory (as represented by the covariance structure model) is hypothesized to hold exactly in the theoretical general population, but not in the operational population [@wu2015; @wu2015a]. The general population might also be referred to as the *ideal* population. This is not the terminology used by Wu and Browne [-@wu2015], but more clearly reflects their view of the general population as being "...contained in the Platonic Aether" [attributed to Michael C. Edwards in @wu2015a, p. 620] as opposed to "a mundane collection of people that can be reached through more complicated designs" [@wu2015a, p. 621].

To illustrate their conception of adventitious error, Wu and Browne [-@wu2015] give the hypothetical example of researchers who are interested in validating the structure of a depression scale hypothesized to hold for all U.S. adults under some general measurement condition. The researchers attempt to validate the hypothesized structure by randomly selecting adults in Chicago on some particular summer day. Thus, the operational population (adults in Chicago on a particular day) differs from the theoretical general (or ideal) population. It is the difference between these two populations that Wu and Browne call adventitious error. In a follow-up article, Wu and Browne [-@wu2015a] clarify that adventitious error is not simply an issue of sub-populations that might be resolved by using alternative models or designs. They state:

> Although we believe that the theoretical population is an ideal in nature, we are not critical of it. Instead, we maintain that, just like statistical models, it is a construction by the human mind in order to understand the world and is necessarily a simplified description of the reality. The usefulness of the concept of a general population lies in its explanatory power but not in its tangible existence as a group of people or a set of measurement outcomes [@wu2015a, p. 619].

Building upon this view, Wu and Browne [-@wu2015] argued that the discrepancy between the operational and ideal population models is a source of variation that is not reflected in traditional methods for fitting covariance structures to sample covariance matrices. These approaches generally fit a covariance structure $\bm{\Omega}(\bm{\gamma})$ to a sample covariance matrix $\mathbf{S}$ by minimizing a discrepancy function $F(\mathbf{S}, \bm{\Omega}(\bm{\gamma}))$. For instance, a common choice of discrepancy function is the maximum likelihood discrepancy function,

\begin{equation}
F_{\textrm{ML}}(\mathbf{S}, \bm{\Omega}(\bm{\gamma})) = \ln |\bm{\Omega}(\bm{\gamma})| - \ln |\mathbf{S}| + \tr [\mathbf{S} \bm{\Omega}(\bm{\gamma})^{-1}] - p.
(\#eq:disc-ml-wb)
\end{equation}

\noindent Note that \autoref{eq:disc-ml-wb} is equivalent to \autoref{eq:disc-ml} with $\mathbf{S}$ substituted for $\bm{\Sigma}$. The discrepancy function is called "maximum likelihood" because minimizing the function is equivalent to maximizing the likelihood function for the Wishart distribution, $\textrm{W}_p(\bm{\Omega}(\bm{\gamma}) / n, n)$. This Wishart distribution is the sampling distribution of $\mathbf{S}$ under the assumption that $\bm{\Sigma} = \bm{\Omega}(\bm{\gamma})$ for some $\bm{\gamma} = \bm{\gamma}_0$, and the assumption of normality [@wu2015]. The asymptotic distribution of the maximum likelihood parameter estimate, $\hat{\bm{\gamma}}_{\textrm{ML}}$ can then be derived [e.g., @shapiro2007, Theorem 5.5, p. 249], and confidence intervals for parameters can be constructed [@joreskog1969].

Unfortunately, using maximum likelihood estimation requires assumptions that are unlikely to hold in many applied settings. First, the maximum likelihood discrepancy function is derived using normal distribution theory and can be sensitive to violations of normality [@browne1988]. However, maximum likelihood estimates can also be derived using a model that does not require any distributional assumptions [@howe1955; @mulaik2009foundations, pp. 214-215] and normal theory methods have been shown to be robust under certain circumstances [@browne1988]. A more troublesome issue highlighted by Wu and Browne [-@wu2015] is that using the maximum likelihood discrepancy function defined in \autoref{eq:disc-ml-wb} implicitly assumes that the theorized model holds perfectly in the population and that all differences between the sample and population covariance matrices are attributable to sampling error [@briggs2003; @wu2015]. Because adventitious error is not considered as a source of random variation, the variability estimates (and therefore confidence intervals) of parameter estimates and test statistics will be underestimated when using the traditional approach to model estimation [@wu2015].

Unlike the traditional approach, Wu and Browne's [-@wu2015] estimation method accounts for variability due to adventitious error by modeling adventitious error as a random effect with a distribution. The estimated dispersion parameter of this distribution can then be used as a measure of model misspecification. The statistical basis for their model is as follows. First, under the assumption of normality, the sample covariance matrix $\mathbf{S}$ has a Wishart distribution such that

\begin{equation}
(\mathbf{S} | \bm{\Sigma}) \sim \textrm{W}_p(\bm{\Sigma}/n, n),
(\#eq:S-dist)
\end{equation}

\noindent where $n$ is the degrees of freedom. As opposed to the traditional method where $\mathbf{S}$ is assumed to be an unbiased estimator of the model-implied population covariance matrix $\bm{\Omega}(\bm{\gamma})$, here $\mathbf{S}$ is instead assumed to be an unbiased estimator of the error-perturbed population covariance matrix, $\bm{\Sigma}$. $\bm{\Sigma}$ is then assumed to follow an inverse-Wishart distribution such that

\begin{equation}
(\bm{\Sigma} | \bm{\Omega}, m) \sim \textrm{W}^{-1}_p (m \bm{\Omega}, m),
(\#eq:Sigma-dist)
\end{equation}

\noindent where $m$ is a continuous precision parameter such that $m > p-1$ [@wu2015]. Wu and Browne [-@wu2015] also introduced the inverse of this precision parameter, $v = 1/m \in [0, (p-1)^{-1})$, which gives the dispersion of the adventitious error and can also be interpreted as a measure of misspecification. In particular, Wu and Brown [-@wu2015, p. 580] show that $v \approx \varepsilon^2$, where $\varepsilon$ denotes the RMSEA. Because $v$ has an upper bound of $1/(p-1)$, Wu and Browne suggested using $\sqrt{\tilde{v}} = (m - p + 1)^{-1/2}$ as the criterion of model admissibility. Specifically, they stated that $\sqrt{\tilde{v}} = 0.05$ is indicative of good model fit, values of $\sqrt{\tilde{v}}$ between 0.05 and 0.08 are indicative of acceptable model fit, and values above 0.08 are indicative of unacceptable model fit.

Wu and Browne's [-@wu2015] model therefore has two sets of parameters, $\bm{\gamma}$ and $v$, that require estimation. They showed that these parameters can be estimated by maximizing the likelihood function corresponding to the marginal distribution of the sample covariance matrix, $(\mathbf{S} | \bm{\Omega}, m)$, with the population covariance matrix $\bm{\Sigma}$ integrated out. Wu and Browne's choice of a conjugate distribution for $\bm{\Sigma}$ means that the marginal distribution of the sample covariance matrix follows a Type II matrix-variate beta distribution [@gupta2000, Chapter 5],

\begin{equation}
(\mathbf{S} | \bm{\Omega}, m) \sim \mathbf{B}^{\textrm{II}}_{p}\left(\frac{n}{2}, \frac{m}{2}, \frac{m}{n}\bm{\Omega} \right),
(\#eq:matrix-variate-beta)
\end{equation}

\noindent from which the probability density function and log-likelihood functions can be obtained [see @wu2015 for additional details]. The parameter estimate obtained by maximizing the beta marginal likelihood (or equivalently, minimizing the negative twice beta log-likelihood) is referred to as the maximum beta likelihood estimate (MBLE).

In addition to showing how to estimate $\bm{\gamma}$ and $v$, Wu and Browne [-@wu2015] derived sampling distributions and confidence intervals for $\hat{\bm{\gamma}}$ and $\hat{v}$. An important advantage of Wu and Browne's method over more traditional estimation methods is that their confidence intervals for $\hat{\bm{\gamma}}$ account for the amount of model error indicated by $\hat{v}$. They provided simulation evidence showing that confidence interval coverage rates for both estimated parameters were close to their nominal values, at least when $n$ and $m$ were relatively large and when $(\bm{\Sigma} | \bm{\Omega}, m) \sim \textrm{W}^{-1}_p (m \bm{\Omega}, m)$. Moreover, they also showed that coverage rates for 95% confidence intervals estimated using their method were much closer to the nominal levels compared to confidence intervals estimated using the traditional approach. Although these results were obtained from $\bSigma$ matrices simulated according to the process assumed by Wu and Browne's model, they claim that their method is robust regarding the distribution of model error when model error is small and that their method will reject the covariance structure when model error is not small [@wu2015a].

Thus far, this section has focused on Wu and Browne's [-@wu2015] method for estimating parameters and constructing confidence intervals for covariance structure models. However, as noted previously, Wu and Browne's model assumes that a covariance matrix with imperfect model fit is a random sample from an inverse-Wishart distribution, as shown in \autoref{eq:Sigma-dist}. Given a model-implied population covariance matrix $\bm{\Sigma}$ and some chosen value of the precision parameter $m$, covariance matrices with imperfect fit can be easily sampled from an appropriate inverse-Wishart distribution using statistical software such as R [@R-base], MATLAB [@matlab], or Julia [@bezanson2017julia]. The degree of model error (in terms of RMSEA) can be controlled to some extent by choosing an appropriate value of $m$, which Wu and Browne showed has a relationship with RMSEA such that $\sqrt{1/m} = \sqrt{v} \approx \varepsilon$ [@wu2015, Eq. 16, p. 580].

## Population Model Fit Indices {#population-model-fit}

In addition to being able to generate error-perturbed covariance matrices, it is important to be able to quantify the lack-of-fit between an error-perturbed population covariance matrix and a model-implied covariance matrix. Before describing the various model-fit indices that have been used to quantify model error, it will be helpful to first describe two different perspectives of model fit as it relates to model error. In the first, model error is quantified as the lack-of-fit between $\bm{\Sigma}$ and $\bm{\Omega}$. In the second, model error is quantified as the lack-of-fit between $\bm{\Sigma}$ and $\hat{\bm{\Omega}}$, the implied covariance matrix from fitting a particular model to $\bm{\Sigma}$. These two perspectives are equivalent in the special case when $\hat{\bm{\Omega}} = \bm{\Omega}$, which occurs when the CB method is used (because $\bm{\gamma}_0$ is required to be a minimizer of the chosen discrepancy function). For the purposes of this dissertation, I will focus primarily on the first model fit perspective. Unless specified, all model fit indices reflect the first perspective, indicating the lack-of-fit between between $\bm{\Sigma}$ and $\bm{\Omega}$. Where I discuss a model fit index indicating the lack-of-fit between $\bm{\Sigma}$ and $\hat{\bm{\Omega}}$, a "$\hat{\bm{\Omega}}$" subscript is used to make it clear that model fit is being considered from the second perspective. I.e., I use "RMSEA" to indicate the root mean square error of approximation between $\bm{\Sigma}$ and $\bm{\Omega}$, and "$\textrm{RMSEA}_{\hat{\bm{\Omega}}}$" to indicate the root mean square error of approximation between $\bm{\Sigma}$ and $\hat{\bm{\Omega}}$.

As described in the previous section, the population RMSEA value is often used to describe the lack-of-fit between population covariance matrices due to model error. Other fit indices such as the Tucker-Lewis Index [TLI\; @tucker1973], the Comparative Fit Index [CFI\; @bentler1990], and the Standardized Root Mean Square Residual [SRMR\; @hu1999] have also be used for this purpose. Although far from an exhaustive list, these fit indices are among the most popular in the psychometric literature and can be divided into two general categories [@hu1999]. First, RMSEA and SRMR are absolute fit indices that report model fit in terms of the difference between a particular covariance matrix and a model-implied covariance matrix, without using a reference or baseline model for comparison. In the population context, absolute fit indices answer the question, "How different is $\bm{\Sigma}$ to $\bm{\Omega}$?" On the other hand, the TLI and CFI are incremental fit indices, which reflect the improvement of fit of a particular model compared to a (restricted, nested) baseline model. The null or independence model is often used as the baseline model, answering the question, "How well is my model doing, compared with the worst model that there is?" [@miles2007, p. 870]. 

RMSEA, CFI, TLI, and SRMR are generally defined in terms of the sample covariance matrix, but can be expressed in their population form (i.e., with reference to $\bm{\Sigma}$) as follows. Let $F_h$ and $F_b$ be the minimized discrepancy function values for the hypothesized and baseline models (respectively) at the population level. Furthermore, let $df_h$ and $df_b$ be the degrees of freedom for the hypothesized and baseline models. Then the population RMSEA value is given by

\begin{equation}
\textrm{RMSEA} = \varepsilon = \sqrt{\frac{F_h}{df_h}},
(\#eq:rmsea)
\end{equation}

\noindent the population CFI value is given by

\begin{equation}
\textrm{CFI} = 1 - \frac{F_h}{F_b},
(\#eq:cfi)
\end{equation}

\noindent and the population TLI value is given by

\begin{equation} 
\textrm{TLI} = 1 - \frac{F_h / df_h}{F_b / df_b}.
(\#eq:tli)
\end{equation}

\noindent Unlike RMSEA, CFI, and TLI, the population SRMR does not depend on discrepancy function values and is given by

\begin{equation}
\textrm{SRMR} = \sqrt{ \frac{1}{p (p + 1) / 2} \sum_{i \leq j} \left(  \frac{\sigma_{ij} - \omega_{ij}}{\sqrt{\sigma_{ii} \sigma_{jj}}} \ \right) ^2 },
(\#eq:srmr)
\end{equation}

\noindent where $p$ is the number of observed variables, and $\sigma_{ij}$ and $\omega_{ij}$ are the $i,j$th elements of $\bm{\Sigma}$ and $\bm{\Omega}$, respectively [@maydeu-olivares2017; @xia2019; @pavlov2021]. Note that when $\bm{\Sigma}$ and $\bm{\Omega}$ are constrained to be correlation matrices with unit diagonals, the correlation root mean residual [CRMR\; @bollen1989a; @ogasawara2001] is appropriate:

\begin{equation}
\textrm{CRMR} = \sqrt{ \frac{1}{p (p - 1) / 2} \sum_{i < j} \left( \sigma_{ij} - \omega_{ij} \ \right) ^2 }.
(\#eq:crmr)
\end{equation}

Although all of the fit indices discussed here have been used to quantify model error in simulation studies [e.g., @lai2020b; @lai2020c; @lai2017; @xia2016a; @nguyen2022], the population RMSEA is perhaps the most widely-used. Indeed, many simulation studies using the TKL, CB, or WB model-error methods have grouped simulated covariance matrices into model fit categories based on highly-cited "rule-of-thumb" RMSEA cutoff values. For instance, Lorenzo-Seva and Ferrando [-@lorenzo-seva2020] used RMSEA values of 0.065 to represent models with fair fit, citing Browne and Cudeck [-@browne1992]. @myers2015a used RMSEA values of 0.025, 0.065, and 0.090 to categorize models as having very good fit, fair fit, and poor fit (respectively), citing Browne and Cudeck [-@browne1992], Steiger [-@steiger1989ezpath], and @jackson2009. According to Lai and Green [-@lai2016, p. 220]: 

> The most widely used cutoffs for RMSEA yield the following interpretations: (a) Values less than .05 (Browne & Cudeck, 1992) or .06 (Hu & Bentler, 1999) suggest 'good' fit; (b) values between .05 and .10 suggest 'acceptable' fit (Browne & Cudeck, 1992; MacCallum, Browne, and Sugawara, 1996); and (c) values larger than .10 suggest 'bad' fit (Browne & Cudeck, 1992).

\noindent However, rule-of-thumb cutoffs based on other common fit indices such as CFI can lead to conclusions about model fit that disagree with those indicated by RMSEA. 

The problem of disagreement among fit indices was explored in detail by Lai and Green [-@lai2016], who derived necessary and sufficient conditions for disagreement between CFI and RMSEA (when used with common cutoff values). For instance, Lai and Green (2016) showed that "good" RMSEA values and "bad" CFI values (i.e., RMSEA $\leq 0.05$ and CFI $\leq 0.90$) will occur when $df_h \leq 400 F_b \leq 10 df_h$, where $df_h$ denotes the model degrees of freedom and $F_b$ is the discrepancy function value for the baseline null (independence) model. Lai and Green [-@lai2016] also showed that $F_b = -\ln |\mathbf{S}|$ for an observed correlation matrix and that $\ln |\mathbf{S}|$ will be large (and $-\ln |\mathbf{S}|$ small) when the elements of $\mathbf{S}$ are close to zero. Thus, models will have "good" RMSEA but "bad" CFI when the elements of $\mathbf{S}$ are small (but large enough that $F_b > df_h / 400$) and when the model degrees of freedom value is large (but small enough that $df_h < 400 F_b$; Lai and Green, 2016). Lai and Green [-@lai2016] concluded that disagreement between CFI and RMSEA does not reflect a failure on the part of either fit index. Rather, disagreement occurs because the two fit indices evaluate model fit from different perspectives and the cutoffs used to make qualitative judgments about model fit are arbitrary. 

Unfortunately, disagreement between fit indices is a problem for researchers attempting to simulate covariance matrices with some particular qualitative level of model fit. Consider, for instance, a simulated covariance matrix with RMSEA = 0.04 and CFI = 0.78.[^model-fit-example] An RMSEA value of 0.04 indicates good model fit based on most rule-of-thumb cutoff values [@browne1992]. However, CFI values less than 0.90 are seldom considered to indicate acceptable fit [@lai2016]. How should this covariance matrix (and others like it) be categorized in a simulation study? No straightforward answer is apparent. The essential problem, as stated by @kline2011, is that "*there is no such thing as a magical, single-number summary that says everything worth knowing about model fit*" (p. 193). Therefore, a common recommendation is to report multiple fit indices to get a more complete picture of model fit [@kline2011; @lai2016]

[^model-fit-example]: This is not just a hypothetical example. Simulated correlation matrices leading to similar RMSEA and CFI values were encountered in simulations conducted by Kracht and Waller (2020).

Despite the recommendation to report multiple fit indices, only a handful of simulation studies that incorporate error-perturbed covariance matrices report fit indices other than RMSEA [@kracht2022; @lai2020b; @lai2020; @lai2019a; @lai2018; @lai2017; @lai2016]. The reliance on a single model fit index (usually RMSEA) when generating error-perturbed covariance matrices can be explained by the characteristics of the TKL, CB, and WB model-error methods. 

In the case of the TKL method, little is known about which values of the TKL parameters ($\epsilon$ and $\nu_\textrm{e}$) are reasonable for generating data that are representative of empirical data. Without clear guidance about which parameter values to use, researchers using the TKL method in simulation studies have often relied on RMSEA values to determine whether resulting correlation matrices were representative of empirical data sets. For instance, Briggs and MacCallum [-@briggs2003] used the TKL method to generate data sets they categorized as having either good or moderate model fit. In their simulation study, error-perturbed covariance matrices were retained or rejected based on their RMSEA values. A matrix was retained if the RMSEA resulting from a maximum likelihood factor analysis fell within the range .030--.049 for the good fit condition and within the range .070--.089 for the moderate model fit condition. 

Other researchers have also used RMSEA as a measure of model misfit when using the TKL method in Monte Carlo simulation studies [@lorenzo-seva2020; @kracht2022; @gnambs2016; @myers2015a; @preacher2013; @preacher2002; @nguyen2022]. One difficulty with this approach is that it can be challenging to choose values of $\epsilon$ and $\nu_\textrm{e}$ that lead to desired RMSEA values while varying factor model characteristics (e.g., number of factors, factor salience, factor correlations, etc.). Manually choosing parameters that routinely lead to desired RMSEA values and desired values of another fit statistic (such as CFI) is even more challenging. Thus, researchers who use the TKL method in simulation studies generally select the values of $\epsilon$ and $\nu_\textrm{e}$ based on only RMSEA.

Researchers who choose to instead generate error-perturbed covariance matrices using the CB method often use RMSEA as a measure of model misfit because the CB method allows them to specify a desired discrepancy function value (and therefore a desired RMSEA value). However, the CB method has no other user-specified parameters than can be changed to influence model fit indices that measure other aspects of model fit (such as CFI). Lai and Green [-@lai2016] used the CB method to generate covariance matrices with specified RMSEA and CFI values, but did so by updating the factor loadings and uniqueness variances in the population model while holding RMSEA constant. Although this approach is effective, it has the significant drawback of not allowing researchers to systematically vary model parameters across experimental conditions. 

Similar to the CB method, the WB method allows the user to choose a desired RMSEA value but does not provide an easy way to specify a desired a CFI value (or any alternative fit index). The WB method is also less precise than the CB method in the sense that it only allows the user to specify a distribution for the model-perturbed covariance matrices and thus does not guarantee that the RMSEA value for a simulated covariance matrix will be very close to the target value.

<!--chapter:end:chapters/01-chap1.Rmd-->


# A Method for Generating Error-Perturbed Covariance Matrices with Specified Levels of Model Fit

Given that model fit cannot be fully described by a single fit index, it is desirable to have a model fit method that would allow for the simultaneous specification of multiple fit indices (e.g., RMSEA and CFI). To date, RMSEA has been used almost exclusively to quantify the amount of model error introduced by a model-error method, and is the only fit index that can be specified in advance when using the CB and WB methods [@briggs2003; @cudeck1992; @wu2015]. 

In this section, I propose a procedure based on the TKL method that uses optimization to find parameter values that lead to error-perturbed covariance matrices with RMSEA and CFI values that are close to user-specified target values. RMSEA and CFI are used because they are among the most commonly-used model fit indices and reflect different aspects of model fit [absolute and incremental model fit\; @kline2011]. However, in theory the procedure could be extended to work with any two fit indices.[^more-than-two] The proposed procedure also allows the user to impose constraints on the number of large minor factor loadings to ensure a clear delineation between major and minor factors in the TKL framework. Because the optimization procedure allows for RMSEA and CFI targets and constraints on factor loadings, I will refer to it as the "multiple-target optimization method" or "the multiple-target method".

[^more-than-two]: In fact, the procedure could easily be extended to include three or more fit indices. However, the marginal benefit of including additional fit indices is unlikely to be large and could make optimization more difficult.

The proposed procedure uses the limited memory Broyden-Fletcher-Goldfarb-Shanno optimization algorithm with box constraints [L-BFGS-B\; @byrd1995] to minimize the objective function

\begin{equation}
G(\nu_\textrm{e}, \epsilon) = b_1 \frac{\left[ \varepsilon - \varepsilon_T \right]^2}{\varepsilon_T^2} + b_2 \frac{\left[ \textrm{CFI} - \textrm{CFI}_T \right]^2}{\left( 1 - \textrm{CFI}_T \right)^2} + \mathbf{1}_{\bm{W}} \lambda,
(\#eq:rmsea-cfi-obj-function)
\end{equation}

\noindent where $0 \leq \nu_\textrm{e} \leq 1$, $0 \leq \epsilon \leq 1$, $b_1$ and $b_2$ are user-specified weights constrained to sum to one. Setting $b_1 = b_2 = 0.5$ places equal weight on RMSEA and CFI, whereas unequal weights can be used to indicate a preference for one fit index over the other. If either weight is set to zero, the corresponding fit index has no effect on optimization. Thus, the optimization procedure seeks to find values of $\nu_\textrm{e}$ and $\epsilon$ such that the weighted sum of the mean squared error between the observed and target RMSEA and the mean squared error between the observed and target CFI is minimized.

The right-most term in \autoref{eq:rmsea-cfi-obj-function} consists of a user-defined penalty, $\lambda$, and an indicator function, $\mathbf{1}_{\bm{W}}$. The indicator function $\mathbf{1}_{\bm{W}}$ is equal to one whenever a user-specified number of (absolute) minor factor loadings are greater than some threshold value for any minor factor, and zero otherwise. The addition of this penalty term attempts to ensure a clear delineation between major and minor factors. For instance, a user could specify that no more than two minor factor loadings should be greater than 0.3 in absolute value. If the penalty term is set to some large value (e.g., $\lambda = 100$), TKL parameters that lead to models with too many large minor factor loadings will be heavily penalized. 

Empirical testing of the multiple-target method showed that some combinations of target fit index values and constraints on the $\mathbf{W}$ matrix sometimes led to non-convergence when using the L-BFGS-B algorithm. In particular, non-convergence was observed when using relatively high target RMSEA values and relatively low target CFI values, and when there were only a small number of factors. Non-convergence can often be remedied by restarting optimization with different starting parameter values. If non-convergence still occurs after trying multiple different starting values, global optimization using a genetic algorithm [GA\; @holland1975] can be used to minimize the objective function in \autoref{eq:rmsea-cfi-obj-function}. 

GAs take inspiration from biological evolutionary processes and natural selection to find candidate solutions that have a high level of fitness (i.e., that lead to high/low objective function values during maximization/minimization). Many variations of GAs have been proposed [@scrucca2013], but the procedure for most GAs can be generally described as follows. First, a random set of candidate solutions is generated and the fitness (i.e., objective function value) for each solution is evaluated. Next, a pair of "parent" solutions are selected (with replacement) from the set such that solutions with higher fitness have a higher selection probability. The parent solutions then produce two offspring with user-defined crossover and mutation probabilities. If crossover occurs, the children are formed as combinations of their parents using some crossover function. If crossover does not occur, the children are formed as exact copies of their parents. Similarly, if mutation occurs, the child solution is randomly perturbed. This process continues until there are as many children in the new generation as there were parents in the previous generation. Once the new generation is formed, selection occurs again and the process continues until a fixed number of generations have passed or until some stopping criterion is reached [@mitchell1996; @scrucca2013].

Although GAs work well for many problems where derivative-based methods have difficulty [e.g., when the objective function is not smooth or when there are local optima\; @scrucca2013], a downside is that they can be relatively slow compared to derivative-based optimization methods when applied to more "well-behaved" optimization problems. Therefore, the proposed optimization procedure first attempts to use the L-BFGS-B method to find a solution (with multiple random starts if convergence does not occur). If convergence still does not occur after multiple random starts using L-BFGS-B, a GA is used to find a solution. The advantage of this approach is that the procedure quickly produces a solution when the user-specified values make the problem well-suited for derivative-based optimization. If the derivative-based optimization fails, the procedure can still find a solution using the GA, albeit somewhat more slowly.

It is important to note that the multiple-target method will not necessarily produce solutions with RMSEA and CFI values that are exactly equal to the target values. For some models, it can be difficult---potentially impossible---to obtain particular combinations of RMSEA and CFI values (e.g., when modeling correlation matrices with many items, a low $\epsilon_T$ value and a high $\textrm{CFI}_T$ value). Moreover, the method is not guaranteed to find solutions that are global minima. Thus, users should check solutions to make sure that the model fit values are sufficiently close to the target values for their purposes. These issues will be investigated more thoroughly in the simulation study that is described in the next section.

<!--chapter:end:chapters/02-chap2.Rmd-->


# Methods

I conducted a simulation study to investigate and compare characteristics of the multiple-target TKL model-error method, the CB model-error method, and the WB model-error method. Moreover, the simulation study was also conducted to evaluate the effectiveness of the proposed multiple-target TKL method. Two questions were of particular interest. First, how did the model-error methods differ in terms of the characteristics of the error-perturbed covariance matrices they produced? Specifically, how did the model-error methods differ in terms of model fit statistics (e.g., RMSEA, SRMR, CFI, TLI) for the error-perturbed covariance matrices they generated? Second, how well did the model-error methods work in terms of producing error-perturbed covariance matrices with RMSEA (and CFI) values that were close to the target values? Answering these questions should be helpful to researchers who are planning Monte Carlo simulation studies involving common factor models and would like to understand how their choice of model-error method is likely to affect the characteristics of simulated, error-perturbed covariance matrices.

In the simulation study, I compared model-error methods by generating error-perturbed covariance matrices using a variety of population models and comparing the results based on several model fit indices. For simplicity, I used covariance matrices with unit diagonals (i.e., correlation matrices). Moreover, I focused on four model fit indices: the RMSEA, the CFI, the Tucker-Lewis Index [TLI\; @tucker1973], and the Correlation Mean Squared Residual [CRMR\; @bollen1989; @ogasawara2001]. Although many other fit indices have been proposed [see @marsh2005], the selected fit indices are among the most commonly-used and include both measures of absolute fit (RMSEA, CRMR) and incremental fit [CFI, TLI\; @kline2011].

Because the relationship between fit indices is affected by model characteristics [@lai2016], I included a variety of distinct population models created by systematically varying: (a) the number of major factors (Factors $\in \{1, 3, 5, 10\}$), (b) the number of items per factor (Items/Factor $\in \{5, 15 \}$), (c) the correlation between factors ($\phi \in \{0, .3, .6\}$), and (d) the strength of the salient item factor loadings (Loadings $\in \{0.4, 0.6, 0.8 \}$). Each item loaded on only a single factor and factor loadings were fixed at values representing weak, moderate, and strong factor loadings, respectively [@hair2018]. Examples of factor loading matrices corresponding to the weak, moderate, and strong factor loading conditions are shown in \@ref(tab:factor-loadings). The factor loading and factor correlation values used in the simulation study were intended to represent a range of values representative of values observed in empirical research. For instance, in a confirmatory factor analysis of sub-tests from the Ball Aptitude Battery believed to measure aspects of intelligence, @neuman2000 reported estimated factor loadings between 0.26 and 0.95 and factor correlations between .18 and .73.

```{r loading-tables, results='asis'}
cat(readLines(here::here("tab", "loading_tables.txt")),
    sep = '\n')
```

Forming a fully-crossed design from the levels of Factors, Items/Factor, and factor correlation ($\phi$) would have resulted in 4 (Factors) $\times$ 2 (Items/Factor) $\times$ 3 ($\phi$) $\times$ 3 (Loadings) $= 72$ unique conditions. However, the 12 conditions with one factor and factor correlations greater than zero were invalid because it was nonsensical to have correlated factors for one-factor models. For the 60 remaining conditions, I computed the model-implied population correlation matrix ($\bm{\Omega}$) corresponding to the population common factor model indicated by the condition. To generate the $\bm{\Omega}$ matrices, I used the `simFA()` function in the R *fungible* library [Version 1.0.8\; @R-fungible][^r-pkgs]. The `simFA()` function computes population correlation matrices for common factor models by taking the model parameters (e.g., factor loadings, number of items per factor, factor correlations) as arguments and then using the equation for the common factor model (i.e., \autoref{eq:cfm}) to produce the population correlation matrix corresponding to the specified model.

[^r-pkgs]: Additionally, the following R [@R-base] packages were used either in the simulation study or to create this manuscript: *bookdown* [Version 0.24; @R-bookdown; @R-bookdown], *colorspace* [Version 2.0.3\; @R-colorspace_a; @R-colorspace_b; @R-colorspace_c; ], *crayon* [Version 1.5.0; @R-crayon], *devtools* [Version 2.4.3; @R-devtools], *dplyr* [Version 1.0.8; @R-dplyr], *gghighlight* [Version 0.3.2; @R-gghighlight], *gopherdown* [Version 0.2.1; @R-gopherdown], *here* [Version 1.0.1; @R-here], *kableExtra* [Version 1.3.4; @R-kableExtra], *knitr* [Version 1.37; @R-knitr], *latex2exp* [Version 0.5.0; @R-latex2exp], *MBESS* [Version 4.8.0; @R-MBESS2], *microbenchmark* [Version 1.4.9; @R-microbenchmark], *papaja* [Version 0.1.0.9997; @R-papaja], *parallel* [Version 4.1.3; @R-base], *patchwork* [Version 1.1.1; @R-patchwork], *pbmcapply* [Version 1.5.0; @R-pbmcapply], *purrr* [Version 0.3.4; @R-purrr], *purrrgress* [Version 0.0.1; @R-purrrgress], *rmarkdown* [Version 2.13; @R-rmarkdown_a; @R-rmarkdown_b], *scales* [Version 1.1.1; @R-scales], *stringr* [Version 1.4.0; @R-stringr], *thesisdown* [Version 0.2.0.9000; @R-thesisdown], *tidyr* [Version 1.2.0; @R-tidyr], *tidyverse* [Version 1.3.1; @R-tidyverse], and *xfun* [Version 0.30; @R-xfun].

Having generated model-implied population correlation matrices without model error, the next step in the simulation procedure was to generate population correlation matrices with model error ($\bm{\Sigma}$) for each of the 60 population factor models using the multiple-target TKL, CB, and WB model-error methods. Each model-error method was repeated with random starting conditions 500 times for each of three target RMSEA values ($\varepsilon_T \in \{0.025, 0.065, 0.090\}$) and each of the 60 population factor models. The target RMSEA values were chosen to represent models with very good, fair, and poor model fit, following the convention used by @myers2015a and @maccallum2001. To generate these $\bSigma$ matrices, I wrote R code to implement each of the model error methods described in this section. Moreover, I created an R package (*noisemaker*) that provides a convenient, simple, and unified interface for generating correlation matrices with model error.[^tkl-cb-credit] R code for all of the model-error method implementations discussed in this dissertation is provided in \@ref(noisemaker-code).

[^tkl-cb-credit]: The TKL and CB implementations provided in the *noisemaker* package are based on implementations of those methods provided in the *fungible* [@R-fungible] and *MBESS* [@R-MBESS2] packages, respectively.

Although the TKL method has so far been discussed as a single model-error method, several variations of the multiple-target TKL method were included in the simulation study. Specifically, I generated error-perturbed covariance matrices for each condition with the multiple-target method using (a) only target RMSEA values (denoted as $\textrm{TKL}_{\textrm{RMSEA}}$), (b) equally-weighted target RMSEA and CFI values ($\textrm{TKL}_{\textrm{RMSEA/CFI}}$), and (c) only target CFI values ($\textrm{TKL}_{\textrm{CFI}}$). I used target CFI values (denoted as $\textrm{CFI}_\textrm{T}$) corresponding to the same subjective levels of model fit as the previously-mentioned target RMSEA values [@hu1999; @marcoulides2017], forming conditions with Very Good ($\textrm{RMSEA}_\textrm{T} = 0.025$, $\textrm{CFI}_\textrm{T} = 0.99$), Fair ($\textrm{RMSEA}_\textrm{T} = 0.065$, $\textrm{CFI}_\textrm{T} = 0.95$), and Poor ($\textrm{RMSEA}_\textrm{T} = 0.090$, $\textrm{CFI}_\textrm{T} = 0.90$) model fit. Additionally, each of the multiple-target TKL methods included a penalty term, $\lambda = 1,000,000$, to heavily penalize solutions where any minor common factor had more than two loadings greater than 0.3 in absolute value. Throughout the rest of this dissertation I often refer to the $\TKLrmsea$, $\TKLcfi$, and $\TKLrmseacfi$ methods collectively as the "TKL-based" methods because they all utilize the original TKL method, albeit with $\nu_\textrm{e}$ and $\epsilon$ values selected via optimization.

For all of the TKL-based methods, the L-BFGS-B optimization procedure was restarted with new starting values of $\nu_\textrm{e}$ if it failed to converge within 1,000 iterations. Starting values were randomly generated using $\nu_{\textrm{e}0} \sim \mathcal{U}(.2, .9)$ and $\epsilon_0 \sim \mathcal{U}(0, .8)$, where $\nu_{\textrm{e}0}$ and $\epsilon_0$ denote the starting values of $\nu_{\textrm{e}}$ and $\epsilon$ and $\mathcal{U}(a, b)$ denotes a uniform distribution on the interval $[a, b]$. These distributions were chosen because initial testing indicated that the multiple-target TKL method was more likely to result in a converged solution if the range of the starting values were somewhat restricted. 

In addition to randomly initializing the starting values of $\nu_{\textrm{e}}$ and $\epsilon$, the values of the $\mathbf{W}^*$ matrix were also randomly initialized at each repetition. In the multiple-target TKL method, the $p \times q$ provisional matrix $\mathbf{W}^*$ (defined in \@ref(tkl-method)) was initialized such that each column consisted of $p$ samples from a standard normal distribution. The $\mathbf{W}$ matrix was then obtained as follows. Let $\epsilon_j$ and $\nu_{\textrm{e}j}$ denote the proposed values of $\epsilon$ and $\nu_{\textrm{e}}$ at iteration $j$ of the multiple-target TKL optimization procedure. At the $j$th iteration, the columns of the $\mathbf{W}^*$ were scaled using $\mathbf{W}^*_j = \mathbf{W}^* \mathbf{V}$, where $\mathbf{V}$ denotes a $q \times q$ diagonal matrix with diagonal elements $(1-\epsilon_j)^0, (1-\epsilon_j)^1, \dots, (1-\epsilon_j)^{q-1}$. Then, $\mathbf{W}^*_j$ was scaled as described in \@ref(tkl-method) to form a $\mathbf{W}_j$ matrix to ensure that the proportion of variance accounted for by the $q = 50$ minor common factors was equal to $\nu_{\mathrm{e}j}$.

Repeating each of the TKL method variations 500 times with random starting values was important because the multiple-target TKL method is not guaranteed to find the global optimum for a particular problem. Moreover, the results of the multiple-target TKL method depend on the $\mathbf{W}^*$ matrix. Therefore, evaluating the multiple-target TKL method with a single set of starting values and a single $\mathbf{W}^*$ matrix might have led to results that were idiosyncratic and not representative of the method's general performance. Another reasonable question is why $\mathbf{W}^*$ was not held fixed over the 500 repetitions with random starting values to find the global optimum parameter values *for that particular* $\mathbf{W}^*$ *matrix*. However, the purpose of the multiple-target TKL method was to produce solutions with fit index values that were "close enough" to the target values to be reasonable over the entire space of $\mathbf{W}^*$ matrices, not to find the best solution for one particular $\mathbf{W}^*$ matrix.

As with the multiple-target TKL methods, the CB and WB methods were also repeated 500 times for each condition of the simulation design. For the CB method, each repetition produced slightly different results due to differences in the randomly-generated $\mathbf{y}$ vector (see \ref{cb}). For the WB method, each repetition represented an independent sample from the inverse Wishart distribution associated with each condition. Therefore, a large number of samples were required to ensure that the simulation results for the CB and WB methods represented the full range of potential outcomes. Unlike the CB model-error method, the WB model-error method did not allow precise control of RMSEA values and produced RMSEA values that were only approximately equal to the target value. Simply using a target RMSEA value to solve for a value of $v$ was unlikely to result in $\mathbf{\Sigma}$ matrices that had RMSEA values close to the target RMSEA value, unless the target RMSEA value was relatively small. 
<!-- This was because $v = \varepsilon^2 + o_\textrm{p}(\varepsilon^2)$ [@wu2015], where $o_\textrm{p}(\varepsilon^2)$ (read as "little-o-p-$\varepsilon^2$") indicates that the difference between $v$ and $\varepsilon^2$ is bounded in probability at the rate $\varepsilon^2$ [@vaart1998, p. 12].  -->

To resolve this issue, I developed a method to find a value of $v$ such that the median RMSEA value from the resulting error-perturbed covariance matrices was close to the target RMSEA value. The method is as follows. First, initial values of $v$ were obtained by squaring each element in a sequence of 20 equally-spaced RMSEA values ranging from 0.01 to 0.095. For each $v$ value, 50 error-perturbed covariance matrices were sampled from the inverse Wishart distribution and the median RMSEA value was computed. Next, $v$ was regressed on the linear and squared median RMSEA terms. The fitted model was then used to predict an appropriate $v$ value such that the median RMSEA value for simulated, error-perturbed covariance matrices was close to the target value. An example is shown in \@ref(fig:make-wb-mod-plot), which shows the $\sqrt{v} = \varepsilon_\textrm{T}$ values plotted against the observed median $\varepsilon$ values for 50 simulated $\bSigma$ matrices corresponding to an orthogonal, five-factor model with salient factor loadings sampled uniformly from 0.3 and 0.7. The dashed orange curve indicates the $\sqrt{v}$ values related to each median $\varepsilon$ value, as predicted by the fitted regression model.

<!-- [***COME BACK TO THIS; Wu and Browne p. 580***] -->

```{r make-wb-mod-plot, fig.align = "center", warning = FALSE, message = FALSE, out.width='70%'}
#| fig.cap = "The relationship between $\\sqrt{\\tilde{v}}$ and $\\epsilon$. Each point indicates the observed median $\\varepsilon$ value for 50 simulated $\\bSigma$ matrices corresponding to an orthogonal, five-factor model with salient factor loadings sampled uniformly from 0.3 and 0.7. The solid gray line indicates where the target RMSEA and median RMSEA values (from 50 samples) would be equal. The dashed orange line indicates the predicted value of $\\sqrt{v}$ that will produce a given RMSEA value."

if (make_plots) {
  set.seed(123)
  
  reps <- 50
  NFac <- 5
  mod <- fungible::simFA(Model = list(NFac = NFac, NItemPerFac = 5), Seed = 123)
  target_rmsea <- seq(0.01, 0.095, length.out = 20)
  vtilde <- target_rmsea^2
  
  rmsea_vec <- numeric(reps)
  median_rmsea <- numeric(length(target_rmsea))
  
  for (i in seq_along(target_rmsea)) {
    for (rep in 1:reps) {
      Rwb <- noisemaker::wb(mod, target_rmsea = target_rmsea[i],
                            adjust_target = FALSE)
      rmsea_vec[rep] <- noisemaker::rmsea(Rwb$Sigma, mod$Rpop, k = NFac)
    }
    median_rmsea[i] <- median(rmsea_vec)
  }
  
  wb_data <- data.frame(v = vtilde, median_rmsea = median_rmsea)
  
  wb_plot <- ggplot(wb_data, aes(y = sqrt(v), x = median_rmsea)) +
    geom_point() +
    geom_smooth(method = lm, 
                formula = y ~ poly(x, 2), 
                se = FALSE, color = "darkorange", lty = 2, size = .75) +
    geom_abline(intercept = 0, slope = 1, color = "gray") +
    theme_bw() + 
    labs(x = latex2exp::TeX("median $\\epsilon$"),
         y = latex2exp::TeX("$\\sqrt{v} = \\epsilon_T$")) +
    coord_fixed(xlim = c(0.01, .125), ylim = c(0.01, .125))
  
  ggsave(filename = here("img/wb-plot.png"),
         plot = wb_plot,
         dpi = 320,
         height = 4, 
         width = 4,
         scale = 1.1)
}

knitr::include_graphics(
  here("img/wb-plot.png"),
  dpi = 300
)
```

In summary, the design of the simulation study was as follows. The crossed combinations of number of factors, number of items per factor, factor correlation, and factor loading configurations corresponded to 60 population major factor models. For each of those population models, I generated 500 error-perturbed correlation matrices using three variations of the multiple-target TKL method, the CB method, and the WB method at each of three target model fit conditions corresponding to Very Good, Fair, and Poor model fit. In total, this resulted in `r 5 * 3 * 60` unique conditions and a total of `r scales::comma(5 * 3 * 60 * 500)` simulated error-perturbed correlation matrices. All of the independent variables in the study (and the levels of those variables) are summarized in \@ref(tab:study1-variables). R code for all aspects of the simulation study is provided in \@ref(main-simulation).

```{r study1-variables, results='asis'}
study_vars <- rbind(
  c("Factors" = "1, 3, 5",
    "Items/Factor" = "5, 15",
    "Factor Correlation ($\\phi$)" = "0, .3, .6",
    "Factor Loading" = "0.4 , 0.6, 0.8",
    "Target Model Fit" = "Very Good ($\\textrm{RMSEA}_{\\textrm{T}} = 0.025$, $\\textrm{CFI}_\\textrm{T} = .99$),",
    " " = "Fair ($\\textrm{RMSEA}_{\\textrm{T}} = 0.065$, $\\textrm{CFI}_\\textrm{T} = .95$),",
    "  " = "Poor ($\\textrm{RMSEA}_{\\textrm{T}} = 0.090$, $\\textrm{CFI}_\\textrm{T} = .90$)",
    "Model-Error Method" = "$\\textrm{TKL}_{\\textrm{RMSEA}}$, $\\textrm{TKL}_{\\textrm{CFI}}$, $\\textrm{TKL}_{\\textrm{RMSEA/CFI}}$, CB, WB")
)

papaja::apa_table(
  x = t(study_vars),
  caption = "Simulation Study Design Variables and Levels.",
  escape = FALSE,
  col.names = c("Variable", "Levels"),
  note = "$\\textrm{RMSEA}_{\\textrm{T}}$ = Target RMSEA value; $\\textrm{CFI}_\\textrm{T}$ = Target CFI value. TKL subscripts indicate which model fit indices were used as targets."
)
```

<!--chapter:end:chapters/03-chap3.Rmd-->


# Results {#results}

```{r read-results}
results_matrix <- readRDS(here("data/results_matrix_with_conditions.RDS"))
```

In the previous section, I described the simulation study I conducted to learn more about the behavior of different methods for generating error-perturbed population covariance (correlation) matrices. The simulation study included five model-error methods---the three multiple-target TKL variations, the CB method, and the WB method---and was designed to answer two primary questions. 

First, I wanted to know how the model-error methods differed in terms of the CFI, TLI, and CRMR fit indices they led to when used with the same error-free models and target RMSEA values. If there were no meaningful differences among the methods, it would indicate that the choice of one particular model-error method over another (among the methods considered here) is not an important variable in the design of simulation studies. On the other hand, if the model-error methods led to systematically different values of the alternative fit indices when matched on RMSEA and all other characteristics, it would suggest that they are not exchangeable. In that case, it would be useful to know which model-error methods frequently produced solutions with multiple fit indices that indicated the target level of qualitative model fit.

A second purpose of the study was to evaluate the effectiveness of the multiple-target TKL method for generating correlation matrices with model error that had RMSEA and CFI values that were close to the specified target values. It was not expected that the multiple-target TKL method would be able to produce correlation matrices with RMSEA and CFI values that were very close to the target values for all of the major-factor population models because of the relationship between RMSEA, CFI, and population model characteristics [@lai2016]. Therefore, I used the absolute deviation between the observed and target RMSEA and CFI values to compare the results from the multiple-target TKL method to the results from the CB and WB methods used in Study 1.

The remainder of the section is structured as follows. First, I report how frequently simulated matrices had properties that might make them unsuitable for use in a simulation study. For instance, the CB method sometimes produced $\bSigma$ matrices that were indefinite (i.e. having one or more negative eigenvalues). Additionally, the TKL-based model-error methods sometimes failed to converge using the L-BFGS-B method or led to solutions with nominally minor factors that would more rightly be considered major factors due to the number of salient factor loadings. Second, I report the distributions of the five fit indices investigated in this study (RMSEA, CFI, TLI, and CRMR) for each of the five model-error methods, conditioned on the other variables in the study design. Third, I report the extent to which the RMSEA and CFI values for the $\bSigma$ matrices produced by each of the model-error methods indicated similar levels of model fit. Fourth, I compare the RMSEA and CFI values corresponding to discrepancy between $\bSigma$ and $\bOmega$ to the RMSEA and CFI values corresponding to the discrepancy between $\bSigma$ and $\bOmegaHat$. Finally, I report results showing that the TKL-based optimization method was able to generate solutions with RMSEA and CFI values that were very close to the target values when the target value combinations were known to be possible.

## Indefinite Matrices (CB) {#indefinite-matrices}

```{r add-is-indefinite-variable}
results_matrix <- mutate(
  results_matrix,
  error = case_when(is.na(error) ~ " ",
                    TRUE ~ error)  
) %>% mutate(
  is_indefinite = str_detect(error, "indefinite")
)

num_cb <- filter(results_matrix, error_method == "CB") %>% nrow()
num_indefinite <- filter(results_matrix,
                         error_method == "CB",
                         is_indefinite == TRUE) %>% nrow()
percent_indefinite <- round((num_indefinite / num_cb) * 100, 1)
```

One drawback of the CB model-error method is that the resulting correlation matrix with model error can be indefinite when the specified target RMSEA value is large [@cudeck1992]. These matrices are undesirable because correlation and covariance matrices are, by definition, at least positive semi-definite (i.e., having strictly non-negative eigenvalues). Of the `r comma(num_cb)` correlation matrices with model error that were generated using the CB method, `r comma(num_indefinite)` (`r comma(percent_indefinite)`%) were indefinite. However, indefinite solutions were much more common for some conditions of the simulation design than others. \@ref(fig:fig-percent-indefinite-matrices) shows the percent of indefinite CB solutions for each level of model fit, number of items per factor, number of factors, and factor loading strength. (Exact percentages are reported in \@ref(tab:tab-percent-indefinite-matrices)). 

\@ref(fig:fig-percent-indefinite-matrices) shows at least three notable trends. First, the percent of indefinite solutions increased as model fit degraded. Second, the percent of indefinite solutions increased as the total number of items increased (i.e., as the number of factors and items per factor increased). Finally, the percent of indefinite solutions increased as factor loadings increased. In the best-case scenarios, conditions corresponding to models with 25 or fewer items led to indefinite solutions very infrequently (in less than 1% of cases). On the other hand, conditions with Poor model fit and 45 or more items led to indefinite correlation matrices in more than 90% of cases. These results show that the CB method would be an inefficient way to simulate positive semi-definite population correlation or covariance matrices with model error when input matrices are large and the target RMSEA value is relatively large. 

A potential strategy for dealing with indefinite solutions when using the CB method would be to simply generate solutions using the CB method until a sufficient number of positive semi-definite solutions have been obtained. However, the amount of time taken by the CB method increases quickly as the number of items increase, making the oversampling strategy impractical for problems with many items. In fact, using the CB method to generate even a small number of solution matrices becomes impractical for large input matrices. This is shown in \@ref(fig:cb-completion-time), which plots the completion time for the CB method when applied to a one-factor model with salient loadings fixed at 0.6 and the number of items varying between 5 and 120. Using a computer with an Intel Core i5-4570 3.20GHz CPU and 16GB of RAM, the CB method took just over 30 seconds to complete (on average) for an input correlation matrix with 65 items. For an input matrix with 115 items, the CB method took approximately four and a half minutes to complete. Such long completion times are often impractical for large simulation studies, particularly if indefinite solutions are discarded. In fact, I had to skip using the CB method in simulation conditions with 10 factors and 15 items per factor (150 items) because those conditions would have taken an impractical amount of time to complete. Timing a single example, the CB method took 15 minutes and 48 seconds to complete with a 150-item input correlation matrix. At that rate, it would have taken almost 11 days to complete one (out of 36) conditions of the simulation design with 150 items. 

(ref:cb-indefinite-table-caption) The percent of Cudeck and Browne (CB) model-error method solutions that were indefinite.

```{r tab-percent-indefinite-matrices}
cb_indefinite_table <- results_matrix %>%
  filter(error_method == "CB") %>%
  group_by(factors, items_per_factor, 
           loading_numeric, model_fit) %>%
  summarise(mean_indefinite = mean(is_indefinite)) %>%
  ungroup() %>%
  mutate(mean_indefinite = case_when(factors == 10 & items_per_factor == 15 ~ NA_real_,
                                     TRUE ~ mean_indefinite)) %>%
  pivot_wider(values_from = mean_indefinite,
              names_from = factors)

cb_indefinite_table %>%
  mutate(across(.cols = c(`1`:`5`), ~ . * 100)) %>%
  apa_table(col.names = c("Items/Factor", "Loading", "Model Fit", 
                          "1", "3", "5", "10"),
            digits = c(0, 1, 0, 1, 1, 1, 1),
            align = "rrlrrrr",
            col_spanners = list("Factors" = c(4, 7)),
            format.args = list("na_string" = "---"),
            caption = "(ref:cb-indefinite-table-caption)",
            label = "cb-indefinite",
            note = "The Cudeck-Browne method was not used for conditions with 10 major factors and 15 items per factor because it was prohibitively slow for those conditions.")
```

```{r fig-percent-indefinite-matrices, fig.cap = "The percent of Cudeck-Browne (CB) method solutions that were indefinite, conditioned on number of factors, factor loading, number of items per factor, and model fit."}
if (make_plots) {
  cb_percent_indefinite <- results_matrix %>%
    filter(error_method == "CB") %>%
    mutate(error= case_when(is.na(error) ~ " ",
                            TRUE ~ error),
           items_per_factor = as.factor(items_per_factor),
           factors = as.factor(factors),
           model_fit = factor(model_fit,
                              levels = c("Very Good", "Fair", "Poor"),
                              labels = c("Fit: Very Good", 
                                         "Fit: Fair", 
                                         "Fit: Poor"))) %>%
    group_by(factors, items_per_factor_rec, 
             loading_numeric, model_fit) %>%
    summarise(mean_indefinite = mean(
      str_detect(error, "indefinite")
    )) %>%
    ungroup() %>%
    ggplot(aes(y = mean_indefinite, x = loading_numeric,
               color = factors,
               shape = factors, 
               linetype = factors,
               group = factors)) +
    geom_line() + 
    geom_point() +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
    scale_x_continuous(breaks = c(.4, .6, .8)) +
    facet_grid(items_per_factor_rec ~ model_fit) +
    theme_bw() +
    labs(color = "Factors",
         fill = "Factors",
         linetype = "Factors",
         shape = "Factors",
         y = "Indefinite Solutions",
         x = "Factor Loading") +
    theme(legend.position = "bottom")
  
  ggsave(filename = here("img/cb_percent_indefinite.png"),
         plot = cb_percent_indefinite,
         dpi = "retina",
         width = 6,
         height = 5)
}

knitr::include_graphics(here("img/cb_percent_indefinite.png"))
```

```{r cb-completion-time, fig.cap = 'The amount of time (in minutes) taken to generate a single correlation matrix with model error using the CB method. Completion times were recorded 10 times for single-factor models with salient factor loadings fixed at 0.6 and number of items (\\emph{p}) varying between 5 and 115. The dashed black line is the LOESS regression line.', align = "center"}
if (make_plots) {
  set.seed(123)
  max_p <- 115
  cb_times <- data.frame(p = seq(5, max_p, by = 10), t = NA)
  safe_cb <- possibly(cb, otherwise = NA)
  
  t <- pblapply(
    X = 1:nrow(cb_times),
    FUN = function(i) {
      mod <- simFA(Model = list(NFac = 1, NItemPerFac = cb_times$p[i]),
                   Loadings = list(FacLoadRange = .6,
                                   FacLoadDist = "fixed"))
      t <- microbenchmark(
        safe_cb(mod, target_rmsea = 0.05),
        times = 10,
        unit = "milliseconds"
      )
      
      t
    }
  )
  
  names(t) <- cb_times$p
  times <- map_dfr(t, ~ .x$time)
  times <- times %>%
    as_tibble() %>%
    pivot_longer(cols = everything(),
                 names_to = "p",
                 values_to = "ns") %>%
    mutate(p = as.numeric(p)) %>%
    mutate(minutes = (ns * 1e-8) / 60) %>%
    select(-ns)
  
  times <- times %>% add_case(p = 150, minutes = 15.8)
  
  cb_time_plot <- ggplot(times, aes(x = p, y = minutes)) +
    geom_point(size = .75) +
    geom_smooth(size = .4, color =" black", linetype = "dashed", 
                method = "loess", level = 0, span = 0.35) +
    scale_x_continuous(breaks = seq(5, 150, by = 10)) +
    labs(x = "p",
         y = "Completion Time (minutes)",
         title = "CB method completion time",
         subtitle = "One-factor models with salient loadings of .6 and RMSEA = 0.05") +
    theme_bw()
  
  saveRDS(times, here("misc/cb_completion_times.RDS"))
  
  ggsave(filename = here("img", "cb_time_plot.png"),
         plot = cb_time_plot,
         dpi = 320,
         height = 5,
         width = 6)
}

knitr::include_graphics(path = here("img", "cb_time_plot.png"))
```

## L-BFGS-B Non-convergence (Genetic Algorithm)

The default optimization method for the multiple-target TKL method was L-BFGS-B. In most cases, this method worked well and converged to a solution relatively quickly. However, there were a small number of cases where the L-BGFS-B method failed to converge. Specifically, the L-BFGS-B method failed to converge 14 times ($<1$% of cases) and only failed to converge when the $\TKLrmseacfi$ method was used and when model fit was Poor. Non-convergence was also somewhat more likely for conditions with few factors compared to conditions with many factors. This can be seen in \@ref(tab:tab-l-bfgs-b-convergence), which shows the L-BFGS-B non-convergence rates by the number of factors, number of items per factor, and model fit for the $\TKLrmseacfi$ method.

Although non-convergence of the L-BFGS-B algorithm was rare, a natural question was whether the genetic algorithm that was used as a fallback option led to similar results compared to cases where the L-BFGS-B algorithm converged. This question was difficult to answer statistically because non-convergence occurred so infrequently. To get a general sense of whether results were similar for cases where the L-BFGS-B algorithm converged or did not converge, I plotted the CFI and RMSEA values for all converged and non-converged cases in conditions where the $\TKLrmseacfi$ model-error method was used and model fit was Poor. \@ref(fig:comparison-of-converged-vs-non-converged) shows that cases where the L-BFGS-B algorithm did not converge (shown in black) led to similar RMSEA and CFI values compared to cases where the algorithm converged (shown in gray). Thus, using a genetic algorithm seemed to provide reasonable results in the few cases where the L-BFGS-B algorithm failed to converge. However, the GA algorithm was much slower than the L-BFGS-B algorithm. In benchmarking done for conditions with one factor, fifteen items per factor, strong salient factor loadings of 0.8, and Poor model fit, the median completion time across 25 trials when using a genetic algorithm was 8.3 seconds, substantially longer than the median completion time of 23 milliseconds when using the L-BFGS-B algorithm.

```{r benchmark-optim-vs-ga, eval = FALSE, include = FALSE}
mod <- simFA(Model = list(NFac = 1, NItemPerFac = 15),
             Loadings = list(FacLoadRange = 0.8,
                             FacLoadDist = "fixed"))

time_optim <- function(x) {
  set.seed(42)
  noisemaker(mod = mod, 
             method = "TKL", 
             target_rmsea = 0.09,
             target_cfi = 0.90, 
             tkl_ctrl = list(WmaxLoading = .3,
                             NWmaxLoading = 2,
                             optim_type = "optim"))
}

time_ga <- function(x) {
  set.seed(42)
  noisemaker(mod = mod, 
             method = "TKL", 
             target_rmsea = 0.09,
             target_cfi = 0.90, 
             tkl_ctrl = list(WmaxLoading = .3,
                             NWmaxLoading = 2,
                             optim_type = "ga"))
}

ga_vs_optim_times <- microbenchmark::microbenchmark(
  "optim" = time_optim(),
  "ga" = time_ga(),
  times = 25L
)
```

```{r tab-l-bfgs-b-convergence, results='asis'}
bfgs_nonconvergence_table <- results_matrix %>%
  filter(error_method == "TKL (RMSEA/CFI)") %>%
  mutate(warning = case_when(is.na(warning) ~ " ",
                             TRUE ~ warning)) %>%
  group_by(factors, model_fit, items_per_factor) %>%
  summarise(mean_nonconvergence = mean(
    warning == "simpleWarning: `optim()` failed to converge, using `ga()` instead.\n"
  ) * 100) %>%
  ungroup() %>%
  pivot_wider(values_from = mean_nonconvergence,
              names_from = model_fit)

bfgs_nonconvergence_table %>%
  papaja::apa_table(col.names = c("Factors", "Items/Factor",
                                  "Very Good",
                                  "Fair",
                                  "Poor"),
                    digits = c(0, 0, 1, 1, 1),
                    align = "rrrrr",
                    font_size = "small",
                    col_spanners = list("Model Fit" = c(3, 5)),
                    label = "bfgs-convergence-table",
                    caption = "(ref:bfgs-convergence-table-caption)")
```

(ref:bfgs-convergence-table-caption) The percent of cases in each combination of conditions where the L-BFGS-B algorithm did not converge after 100 random starts and genetic optimization was used instead.

```{r comparison-of-converged-vs-non-converged, fig.cap = "RMSEA and CFI values for cases where the model-error method was $\\textrm{TKL}_{\\textrm{RMSEA/CFI}}$ and model fit was Poor. Grey dots indicate cases where the L-BFGS-G algorithm converged; black dots indicate cases that did not converge and where a genetic algorithm was used instead. The dashed black lines indicate the target RMSEA and CFI values."}
if (make_plots) {
  nonconverged_data <- results_matrix %>%
    filter(error_method == "TKL (RMSEA/CFI)") %>%
    mutate(converged = 
             warning != "simpleWarning: `optim()` failed to converge, using `ga()` instead.\n") %>%
    mutate(converged = case_when(is.na(converged) ~ TRUE,
                                 TRUE ~ converged)) %>%
    mutate(converged = factor(converged, 
                              levels = c(TRUE, FALSE), 
                              labels = c("Yes", "No")))
  
  nonconverged_rmsea_cfi <- nonconverged_data %>%
  filter(model_fit == "Poor") %>%
  ggplot(aes(y = cfi, x = rmsea)) +
  geom_point(size = 1, alpha = 0.8) +
  gghighlight(converged == "No", use_direct_label = FALSE,
              unhighlighted_params = list(alpha = 0.1)) +
  geom_vline(xintercept = 0.09, size = .25, color = "grey10", linetype = "dashed") +
  geom_hline(yintercept = 0.90, size = .25, color = "grey10", linetype = "dashed") +
  theme_bw() +
  labs(x = "RMSEA", y = "CFI", color = "L-BFGS-B Convergence")

  ggsave(filename = here("img/nonconverged_rmsea_cfi.png"),
         dpi = 320,
         width = 4.25, 
         height = 4)
}

knitr::include_graphics(here("img/nonconverged_rmsea_cfi.png"))
```

## Major Minor Factors ($\mathbf{W}$ Matrix Constraint Violations)

Recall that the multiple-objective TKL method included an optional penalty that penalized cases that had strong minor factors. Specifically, the penalty was applied if any minor factor had two or more absolute factor loadings greater than or equal to a specified value. The purpose of the penalty was to avoid introducing minor factors that might be more accurately characterized as major factors. To determine whether the penalty was effective at helping avoid major minor factors, I checked each of the minor factor loading ($\mathbf{W}$) matrices to determine whether any minor factor had two or more loadings greater than 0.3 in absolute value. 

The percent of cases where the constraints on $\mathbf{W}$ were violated for each level of number of factors, number of items per factor, factor loading strength, and model fit are shown in \@ref(fig:w-major-factors-plot) and reported in \@ref(tab:tab-major-minor-factors). Only results for the $\TKLrmsea$ method were included because the $\TKLrmseacfi$ and $\TKLcfi$ model-error methods seldom led to solutions that violated the constraints on $\mathbf{W}$. In fact, only 24 out of 180,000 cases (<0.01%) had violated $\mathbf{W}$ constraints for the $\TKLrmseacfi$ and $\TKLcfi$ methods combined. \@ref(fig:w-major-factors-plot) shows that the $\mathbf{W}$ constraints were violated most often when model fit was Fair or Poor, factor loadings were relatively low, and there were many total items (i.e., many factors and items per factor). These variables were included in the figure because they were most important in terms of whether a solution was likely to violate the constraints on $\mathbf{W}$, as indicated by the size of the logistic regression coefficients reported in \@ref(w-constraints-regression-table).

(ref:major-minor-factors) The percent of cases that violated the minor common factor loading constraints when the $\TKLrmsea$ method was used.

```{r tab-major-minor-factors, results='asis'}
percent_w_major_factors_table <- results_matrix %>%
  filter(error_method == "TKL (RMSEA)") %>%
  group_by(factors, items_per_factor, loading_numeric, model_fit) %>%
  mutate(w_has_major_factors = fn_value >= 1e06) %>%
  summarise(mean_w_major_factors =
              mean(w_has_major_factors, na.rm = TRUE) * 100) %>%
  ungroup() %>%
  pivot_wider(values_from = mean_w_major_factors,
              names_from = "model_fit")

percent_w_major_factors_table %>%
  apa_table(col.names = c("Factors", "Items per Factor", "Loading",
                          "Very Good", 
                          "Fair", 
                          "Poor"),
            col_spanner = list("Model Fit" = c(4, 6)),
            digits = c(0, 0, 1, 1, 1, 1),
            align = "rrlrrr",
            caption = "(ref:major-minor-factors)",
            label = "major-minor-factors",
            font_size = "small")
```

```{r w-major-factors-plot}
#| fig.cap = "The percent of cases where the constraints on $\\mathbf{W}$ were violated when the $\\textrm{TKL}_{\\textrm{RMSEA}}$ model-error method was used, conditioned on number of factors, number of items per factor, factor loading, and model fit."

if (make_plots) {
  percent_w_major_factors_table <- results_matrix %>%
    filter(!(error_method %in% c("CB", "WB"))) %>%
    group_by(factors, items_per_factor_rec, loading_numeric, 
             model_fit_rec, error_method_rec, error_method) %>%
    mutate(w_has_major_factors = fn_value >= 1e06) %>%
    summarise(mean_w_major_factors =
                mean(w_has_major_factors, na.rm = TRUE))
  
  w_major_factors_plot <- percent_w_major_factors_table %>%
    filter(error_method == "TKL (RMSEA)") %>%
    mutate(factors = as.factor(factors)) %>%
    ggplot(aes(y = mean_w_major_factors, 
               x = loading_numeric, 
               color = factors, 
               linetype = factors, 
               shape = factors, 
               group = factors)) +
    geom_line() +
    geom_point() +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    scale_y_continuous(label = scales::percent_format(accuracy = 1)) +
    scale_x_continuous(breaks = c(.4, .6, .8)) +
    facet_grid(items_per_factor_rec ~ model_fit_rec) +
    theme_bw() +
    labs(x = "Loadings", y = TeX("Cases with Violtated $W$ Constraints"), 
         color = "Factors", linetype = "Factors", shape = "Factors") +
    theme(legend.position = "bottom")
  
  ggsave(filename = here("img/major_minor_factors.png"),
         w_major_factors_plot,
         dpi = 320,
         width = 6, 
         height = 5)
}

knitr::include_graphics(here("img/major_minor_factors.png"))
```

To understand why certain conditions led to more $\mathbf{W}$ constraint violations than others, it is helpful to understand the relationship between those conditions and the TKL parameters ($\epsilon$ and $\nu_{\textrm{e}}$). \@ref(fig:eps-and-v-values-by-W-violations) shows the $\epsilon$ and $\nu_{\textrm{e}}$ values for each of the TKL model-error methods, conditioned on factor loading strength, number of factors, and number of items per factor. To conserve space, only conditions with Poor model fit and 1, 5, or 10 major factors were included in the figure (a complete version of this figure is shown in \@ref(eps-and-nu-full)). Overall, the figure shows that there was a trade-off between $\epsilon$ and $\nu_{\textrm{e}}$ such that higher values of $\nu_{\textrm{e}}$ were related to lower values of $\epsilon$ (and *vice versa*). Moreover, \@ref(fig:eps-and-v-values-by-W-violations) shows that the distributions of $\epsilon$ and $\nu_{\textrm{e}}$ differed depending on which TKL variant was used. The differences between the error-method variants were largest when the there were many items (i.e., many items and many items per factor) and when factor loadings were relatively weak. Under those circumstances, the $\TKLrmsea$ method led to higher values of $\nu_{\textrm{e}}$ than the $\TKLrmseacfi$ or $\textrm{TKL}_\textrm{CFI}$ methods. This effect suggested that higher values of $\nu_{\textrm{e}}$ were required to produce solutions with RMSEA values close to 0.09 when there were many items. On the other hand, the results for the $\TKLrmseacfi$ and $\TKLcfi$ methods indicated that lower $\nu_{\textrm{e}}$ values were required to obtain CFI values close to .90 when there were many items.

```{r eps-and-v-values-by-W-violations}
#| fig.cap = "Values of the TKL parameters ($\\epsilon$ and $\\nu_{\\textrm{e}}$) by model-error method, number of factors, number of items per factor, and factor loading strength when model fit was Poor. Results for conditions with three or five major factors were omitted to conserve space. TKL = Tucker, Koopman, and Linn."

if (make_plots) {
  eps_and_nu_plot <- results_matrix %>%
    mutate(w_has_major_factors = factor(w_has_major_factors, 
                                        levels = c(FALSE, TRUE),
                                        labels = c("No", "Yes"))) %>%
    filter(model_fit == "Poor",
           factors %in% c(1,10),
           !(error_method %in% c("CB", "WB"))) %>%
    ggplot(aes(y = v, x = eps, color = error_method)) +
    geom_point(alpha = .1, size = .75) +
    scale_color_brewer(
      palette = "Dark2", type = "qual",
      labels = list("TKL (RMSEA)" = TeX("$TKL_{RMSEA}$"),
                    "TKL (RMSEA/CFI)" = TeX("$TKL_{RMSEA/CFI}$"),
                    "TKL (CFI)" = TeX("$TKL_{CFI}"))) +
    scale_x_continuous(breaks = c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)) +
    facet_grid(factors_rec * items_per_factor_rec ~ loading_rec) +
    labs(y = TeX("$\\nu_e$"), x = TeX("$\\epsilon$"), color = "Error Method") +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +
    theme_bw() +
    theme(legend.position = "bottom")

  ggsave(filename = here("img/eps_and_nu_w_violations.png"),
         eps_and_nu_plot,
         dpi = 320,
         height = 7, width = 6)
}

knitr::include_graphics(here("img/eps_and_nu_w_violations.png"))
```

The apparent trade-off between $\epsilon$ and $\nu_{\textrm{e}}$ values made sense considering their roles in the TKL method. A high $\nu_{\textrm{e}}$ value indicated that much of the unique variance would be assigned to the minor common factors. If the value of $\epsilon$ was also high, it indicated that the first few minor factors would account for most of the minor factor variance. Therefore, the $\mathbf{W}$ constraints were more likely to be violated when both $\epsilon$ and $\nu_{\textrm{e}}$ were high, and less likely to be violated if either parameter was low. The link between $\epsilon$ and $\nu_{\textrm{e}}$ and constraint violations is shown in \@ref(fig:w-violation-example), which shows the values of $\epsilon$ and $\nu_{\textrm{e}}$ produced by the three TKL variants for conditions with Poor model fit, 10 factors, 15 items per factor, and factor loadings of 0.8. Each point (corresponding to a single case) was colored according to whether or not the $\mathbf{W}$ constraints were violated. The figure shows that the $\mathbf{W}$ constraints were violated when the values of $\epsilon$ and $\nu_{\textrm{e}}$ were both higher than some threshold values, which only happened when the $\textrm{TKL}_\textrm{RMSEA}$ was used.

```{r w-violation-example}
#| fig.cap = "The distribution of $\\epsilon$ and $\\nu_{\\textrm{e}}$ (and whether or not $\\mathbf{W}$ constraints were violated) for conditions with Poor model fit, 10 factors, 15 items per factor, and factor loadings of 0.8. TKL = Tucker, Koopman, and Linn."

if (make_plots) {
  eps_and_nu_violation_plot <- results_matrix %>%
    mutate(w_has_major_factors = factor(w_has_major_factors, 
                                        levels = c(FALSE, TRUE),
                                        labels = c("No", "Yes"))) %>%
    filter(model_fit == "Poor",
           factors == 10,
           items_per_factor == 15,
           loading_numeric == 0.8,
           !(error_method %in% c("CB", "WB"))) %>%
    ggplot(aes(y = v, x = eps, color = w_has_major_factors)) +
    geom_point(alpha = .3) +
    scale_color_manual(values = c("#CCCCCC", "#440154FF")) +
    facet_grid(~ error_method_rec, labeller = label_parsed) +
    labs(y = TeX("$\\nu_{e}$"), x = TeX("$\\epsilon$"), 
         color = "W Constraints Violated") +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5), 
                                reverse = TRUE)) +
    theme_bw() +
    theme(legend.position = "bottom", panel.spacing.x = unit(14, units = "points"))

  ggsave(filename = here("img/eps_and_nu_violation_example.png"),
         eps_and_nu_violation_plot,
         dpi = 320,
         height = 3, width = 6)
}

knitr::include_graphics(here("img/eps_and_nu_violation_example.png"))
```

### Choice of Penalty Value

The penalty value in \autoref{eq:rmsea-cfi-obj-function}, $\lambda$, can be set at a higher or lower value to change the penalty that is incurred when a potential solution has a $\mathbf{W}$ matrix that violates the user-specified constraints. In the present study, the penalty was set to be $\lambda = 1,000,000$ to ensure that the penalty was sufficiently large. However, it is possible that smaller values of $\lambda$ would have also been as effective or more effective at leading to solutions that satisfied the constraints on $\mathbf{W}$. To determine which values of $\lambda$ worked best, I simulated 200 correlation matrices with model error using the $\TKLrmsea$ method for each of 36 conditions in the main simulation study. These conditions were formed by crossing the number of major common factors (1, 3, 5, or 10), and nine values of $\lambda$ ($\lambda \in [0, \:0.1, \:1, \:10, \:100, \:1,000, \:10,000, \:100,000, \:1,000,000]$). All factors were orthogonal, with factor loadings fixed at 0.4, 15 items per factor, and a target RMSEA value of 0.09. These sets of values were chosen because they often resulted in solutions with violated $\mathbf{W}$ constraints in the main simulation study.[^effect-of-lambda-code]

[^effect-of-lambda-code]: Code for this simulation study is provided in \@ref(effect-of-lambda).

The percentages of solutions with violated $\mathbf{W}$ constraints for each combination of number of factors and $\lambda$ are shown in \@ref(fig:penalty-values). The figure shows that for conditions with one, three, or five factors, the proportion of solutions with violations of the $\mathbf{W}$ constraints decreased as $\lambda$ increased from zero to one and then leveled off for $\lambda$ values greater than one. For conditions with ten factors, all solutions had violated $\mathbf{W}$ constraints, regardless of the $\lambda$ value. Although only a subset of the conditions from the full study were included in this smaller simulation, the results suggested that using a smaller $\lambda$ value is unlikely to have resulted in a substantial decrease in the number of solutions with violated $\mathbf{W}$ constraints. On the other hand, the results also suggested that $\lambda$ values as small as one are likely just as effective as much larger values at preventing solutions with violated $\mathbf{W}$ constraints. 

```{r penalty-values, out.width='90%'}
#| fig.cap = "The percent of cases with violated $\\mathbf{W}$ constraints conditioned on the constraint penalty value ($\\lambda$) and the number of major common factors when the $\\textrm{TKL}_{\\textrm{RMSEA}}$ method was used. All conditions had 15 items per factor, major common factor loadings fixed at 0.4, and a target RMSEA value of 0.09. One hundred solution matrices were generated for each condition."
if (make_plots) {
  # Set the number of reps
  reps <- 200
  
  # Create a matrix of fully-crossed conditions
  factors <- unique(results_matrix$factors)
  items_per_factor <- 15
  loading <- c(.4)
  target_rmsea <- c(0.090)
  penalty <- c(0, .1, 1, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6)
  
  conditions_matrix <- expand.grid(
    factors = factors,
    items_per_factor = items_per_factor,
    loading = loading,
    target_rmsea = target_rmsea
  )
  
  
  # Function to check whether a W matrix has more than two factor loadings greater
  # than 0.3 for any minor factor.
  check_constraints <- function(W) {
    any(max(apply(abs(W) >= .3, 2, sum)) > 2)
  }
  
  # For each condition, generate a population correlation matrix without model
  # error and then generate 200 population correlations with model error using
  # the TKL (RMSEA) method
  set.seed(666)
  constraint_violations <- map_dfr(
    .x = 1:nrow(conditions_matrix), 
    .f = function(condition) {
      
      cat("\nWorking on condition", condition, "of", nrow(conditions_matrix))
      
      # Generate population correlation matrix without model error
      mod <- simFA(
        Model = list(NFac = conditions_matrix$factors[condition],
                     NItemPerFac = conditions_matrix$items_per_factor[condition],
                     Model = "orthogonal"),
        Loadings = list(FacLoadDist = "fixed",
                        FacLoadRange = conditions_matrix$loading[condition])
      )
      
      # Generate 200 population correlation matrices with model error
      pro_map_dfr(.x = 1:reps,
                  .f = function(x, target_rmsea) {
                    map_dfr(.x = penalty, 
                            .f = function(penalty, mod, target_rmsea, seed) {
                              set.seed(seed)
                              sol <- noisemaker(mod = mod, 
                                                method = "TKL", 
                                                target_rmsea = target_rmsea,
                                                target_cfi = NULL,
                                                tkl_ctrl = list(penalty = penalty,
                                                                NWmaxLoading = 2,
                                                                WmaxLoading = .3))
                              w_constraints_violated <- check_constraints(sol$W)
                              
                              c(penalty = penalty, 
                                constraints_violated = w_constraints_violated)
                            },
                            mod = mod,
                            target_rmsea = target_rmsea,
                            seed = sample(1e6, 1))
                  }, target_rmsea = conditions_matrix$target_rmsea[condition])
    }
  )
  
  # Bind the conditions matrix to the results to indicate which result belongs to
  # which condition
  constraint_violations <- bind_cols(
    conditions_matrix[rep(1:nrow(conditions_matrix), 
                          each = length(penalty) * reps),],
    constraint_violations
  )
  
  # Plot the results
  constraint_violations %>%
    mutate(factors = as.factor(factors),
           penalty = factor(penalty, 
                            labels = c("0", "0.1", "1", "10", 
                                       "100", "1,000", "10,000",
                                       "100,000", "1,000,000"))) %>%
    group_by(factors, penalty) %>%
    summarise(percent = mean(constraints_violated, na.rm = TRUE)) %>%
    ggplot(aes(x = penalty, y = percent, color = factors, shape = factors, 
               linetype = factors, group = factors)) +
    geom_point() +
    geom_line() +
    scale_y_continuous(labels = scales::percent) +
    scale_color_brewer(palette = "Dark2", type = "qual") +
    labs(y = "Cases with Violtated W Constraints",
         x = latex2exp::TeX("$\\lambda$"),
         color = "Factors", shape = "Factors", linetype = "Factors") +
    theme_bw() +
    theme(legend.position = "bottom")
  
  # Save the plot
  ggsave(filename = here("img/penalty_values.png"),
         dpi = 320,
         height = 4,
         width = 6)
}

knitr::include_graphics(here("img/penalty_values.png"))
```

The results from both the main simulation study and the smaller study of $\lambda$ values suggested that enforcing constraints on $\mathbf{W}$ using the penalty in \autoref{eq:rmsea-cfi-obj-function} was only somewhat effective when the $\TKLrmsea$ method was used. Regardless of the $\lambda$ value used, the $\TKLrmsea$ method often led to solutions with violated $\mathbf{W}$ constraints in conditions with Poor model fit, weak factor loadings, and many items or factors. In contrast, the $\TKLcfi$ and $\TKLrmseacfi$ methods rarely produced solutions that violated the constraints on $\mathbf{W}$. However, it was unclear whether the penalty worked better for the $\TKLcfi$ and $\TKLrmseacfi$ methods for some reason, or whether the inclusion of a target CFI value simply made it less likely for the $\mathbf{W}$ constraints to be violated regardless of whether the penalty was included. 

To test whether the inclusion of the target CFI value was sufficient to avoid violated $\mathbf{W}$ constraints when no penalty was applied, I conducted another small-scale simulation study. Specifically, I generated 200 $\bSigma$ matrices using a population correlation matrix with ten (orthogonal) major common factors, 15 items per factor, and salient factor loadings fixed at 0.4. Each of the $\bSigma$ matrices was generated using the $\TKLrmseacfi$ method with target RMSEA and CFI values of 0.09 and 0.90, respectively, and $\lambda = 0$ so that no penalty was applied. I chose this condition because nearly 100% of the solutions generated by the $\TKLrmsea$ for this condition in the main simulation study had violated $\mathbf{W}$ constraints, whereas the $\TKLcfi$ and $\TKLrmseacfi$ methods did not produce any solutions with violated $\mathbf{W}$. Thus, generating $\bSigma$ matrices for this condition using the $\TKLrmseacfi$ with $\lambda = 0$ was a reasonable test of whether using a penalty to enforce the $\mathbf{W}$ constraints was necessary when the $\TKLrmseacfi$ was used.[^tkl-cfi-penalty-code] Somewhat surprisingly, I found that none of the 200 solutions that were generated violated the $\mathbf{W}$ constraints when $\lambda$ was set to zero. This result suggested that the addition of a target CFI value in and of itself was useful for producing solutions without unacceptably strong minor factors.

[^tkl-cfi-penalty-code]: Code for this simulation study is provided in \@ref(tkl-cfi-penalty).

```{r is-a-penalty-useful-with-cfi-target}
if (make_plots) {
  
  # Check whether the TKL (CFI) method leads to solutions that violate the W 
  # constraints when no penalty is applied
  set.seed(42)
  
  # Create a population correlation matrix without model error (Omega)
  mod <- simFA(Model = list(NFac = 10,
                            NItemPerFac = 15,
                            Model = "orthogonal"),
               Loadings = list(FacLoadDist = "fixed",
                               FacLoadRange = .4))
  
  # Function to check whether a W matrix has more than two factor loadings greater
  # than 0.3 for any minor factor.
  check_constraints <- function(W) {
    any(max(apply(abs(W) >= .3, 2, sum)) > 2)
  }
  
  # Generate 200 population correlation matrices with model error (Sigma) using 
  # the TKL (CFI) method without a penalty, then check how often the constraints
  # on W were violated
  constraints_violated_vec <- pbmclapply(
    X = 1:200,
    FUN = function(x) {
      sol <- noisemaker(mod = mod, 
                        method = "TKL", 
                        target_rmsea = 0.09,
                        target_cfi = 0.9,
                        tkl_ctrl = list(penalty = 0))
      check_constraints(sol$W)
    },
    mc.cores = 4
  )
  
  # What percent of cases had violated W constraints?
  mean(constraints_violated_vec, na.rm = TRUE)
}
```

To better understand why the $\TKLrmsea$ method so often led to solutions that violated the constraints on $\mathbf{W}$ compared to the $\TKLcfi$ and $\TKLrmseacfi$ methods, I evaluated the objective function over a grid of $62,500$ $\epsilon$ and $\nu_{\textrm{e}}$ values for a single pair of $\bOmega$ and $\mathbf{W}^*$ matrices and each of the three TKL model-error methods. The $\bOmega$ population correlation matrix without model error was again set to correspond to a model with 10 orthogonal major factors and 15 items per factor, with salient factor loadings fixed at 0.4. The penalty value was fixed at 1,000 to penalize solutions with minor factors that had more than two factor loadings greater or equal to 0.3 in absolute value. The objective function surfaces for each of the three TKL model-error methods are shown in \@ref(fig:obj-function-landscape). To make it easier to compare the objective function surfaces, I made separate plots for cases where the $\mathbf{W}$ constraints were and were not violated. Moreover, I scaled the objective function values ($G_{\textrm{obj}}$) in each panel of \@ref(fig:obj-function-landscape) to fall between zero and one so that the surfaces could be easily compared. The figure showed clear differences between the objective function surfaces for the $\TKLrmsea$ method compared to the $\TKLcfi$ and $\TKLrmseacfi$ methods. In particular, the figure showed that the values of $F_{\textrm{obj}}$ decreased as $\epsilon$ and $\nu_{\textrm{e}}$ increased for the $\TKLrmsea$ method. In contrast, the values of $F_{\textrm{obj}}$ increased as $\epsilon$ and $\nu_{\textrm{e}}$ increased for the $\TKLcfi$ and $\TKLrmseacfi$ methods. 

The results shown in \@ref(fig:obj-function-landscape) helped explain why the $\TKLrmsea$ method often led to solutions with violated $\mathbf{W}$ constraints even when such solutions were heavily penalized. The figure shows that for $\epsilon$ and $\nu_{\textrm{e}}$ needed to be relatively large to produce solutions with RMSEA values that were close to the target RMSEA value of 0.09. In fact, the lowest $F_{\textrm{obj}}$ values were produced when $\epsilon$ and $\nu_{\textrm{e}}$ were as large as possible without violating the $\mathbf{W}$ constraints. The objective function value increased sharply after $\epsilon$ and $\nu_{\textrm{e}}$ became large enough to violate the $\mathbf{W}$ constraints, but then decreased again as $\epsilon$ and $\nu_{\textrm{e}}$ increased. Therefore, the optimization procedure was likely to move toward larger values of $\epsilon$ and $\nu_{\textrm{e}}$ corresponding to local minima, regardless of whether the constraints on $\mathbf{W}$ were violated.

```{r obj-function-landscape, out.width = '100%'}
#| fig.cap = "Heatmaps of the scaled objective function surface for combinations of $\\epsilon$ and $\\nu_{\\textrm{e}}$ values and each of the Tucker, Koopman, and Linn (TKL; 1969) model-error methods. Objective function values ($G_{\\textrm{obj}}$) were computed using a $\\bm{\\Omega}$ matrix with ten orthogonal major factors, 15 items per factor, and salient factor loadings fixed at 0.4. The penalty value, $\\lambda$, was fixed at 1,000 to penalize solutions with minor factors that had more than two factor loadings greater or equal to 0.3 in absolute value. To aid visualization, the objective function values for combinations of $\\epsilon$ and $\\nu_{\\textrm{e}}$ where the constraints on $\\mathbf{W}$ were violated were scaled and plotted separately from combinations that did not lead to constraint violations."

if (make_plots) {
  par_grid <- expand.grid(
    eps = seq(0, 1, length.out = 50),
    nu = seq(0, 1, length.out = 50)
  )
  
  simFA_mod <- simFA(Model = list(NFac = 10, 
                                  NItemPerFac = 15, 
                                  Model = "orthogonal"),
                     Loadings = list(FacLoadRange = .4,
                                     FacLoadDist = "fixed"),
                     ModelError = list(ModelError = TRUE),
                     Seed = 123)
  
  Rpop <- simFA_mod$Rpop
  W <- simFA_mod$W
  
  p <- nrow(Rpop)
  k <- ncol(simFA_mod$loadings)
  df <- (p * (p - 1) / 2) - (p * k) + (k * (k - 1) / 2) # model df
  target_rmsea <- 0.09
  target_cfi <- 0.9
  weights <- list("TKL_RMSEA" = c(1, 0), 
                  "TKL_CFI" = c(0, 1), 
                  "TKL_RMSEA_CFI" = c(1, 1))
  WmaxLoading <- .3
  NWmaxLoading <- 2
  return_values <- FALSE
  penalty <- 1000
  
  CovMajor <- simFA_mod$loadings %*% simFA_mod$Phi %*% t(simFA_mod$loadings)
  u <- 1 - diag(CovMajor)
  
  obj_val_list <- vector(mode = "list", length = 3L)
  i <- 1
  for (error_method_weights in weights) {
    val <- map2_dbl(
      par_grid$nu,
      par_grid$eps,
      ~ noisemaker::obj_func(par = c(.x, .y),
                             Rpop = Rpop,
                             W = W,
                             p = p,
                             u = u,
                             df = df,
                             target_rmsea = target_rmsea,
                             target_cfi = target_cfi,
                             weights = weights[[i]],
                             WmaxLoading = WmaxLoading,
                             NWmaxLoading = NWmaxLoading,
                             return_values = return_values,
                             penalty = penalty)
    )
    obj_val_list[[i]] <- val
    i <- i + 1
  }
  
  par_grid <- data.frame(
    eps = rep(par_grid$eps, 3),
    nu  = rep(par_grid$nu, 3),
    error_method = rep(c("TKL[RMSEA]", "TKL[CFI]", "TKL[RMSEA/CFI]"), 
                       each = nrow(par_grid)),
    value = c(obj_val_list[[1]], obj_val_list[[2]], obj_val_list[[3]])
  )
  
  par_grid <- par_grid %>%
    mutate(error_method = factor(error_method,
                                 levels = c("TKL[RMSEA]", 
                                            "TKL[CFI]", 
                                            "TKL[RMSEA/CFI]")))
  
  without_W_penalty <- par_grid %>%
    filter(value < 1000) %>%
    group_by(error_method) %>%
    mutate(value = (value - min(value, na.rm = TRUE)) / 
             (max(value, na.rm = TRUE) - min(value, na.rm = TRUE))) %>%
    ggplot(aes(x = eps, y = nu, fill = value, z = value)) +
    geom_raster() +
    geom_contour(color = "lightgrey") +
    scale_fill_viridis_c() +
    scale_x_continuous(expand = c(0,0)) +
    scale_y_continuous(expand = c(0,0), limits = c(0, .2)) +
    facet_grid( ~ error_method, labeller = label_parsed) +
    theme_bw() +
    labs(x = "",
         y = latex2exp::TeX("$\\nu_e$"),
         fill = latex2exp::TeX("$\\textit{G}_{obj}$"),
         title = "Constraints on W Not Violated") +
    theme(panel.spacing = unit(20, "points"))
  
  with_W_penalty <- par_grid %>%
    filter(value >= 1000) %>%
    group_by(error_method) %>%
    mutate(value = (value - min(value, na.rm = TRUE)) / 
             (max(value, na.rm = TRUE) - min(value, na.rm = TRUE))) %>%
    ggplot(aes(x = eps, y = nu, fill = value, z = value)) +
    geom_raster() +
    geom_contour(color = "grey") +
    scale_fill_viridis_c() +
    scale_x_continuous(expand = c(0,0)) +
    scale_y_continuous(expand = c(0,0), limits = c(0, .2)) +
    facet_grid( ~ error_method, labeller = label_parsed) +
    theme_bw() +
    labs(x = "",
         y = latex2exp::TeX("$\\nu_e$"),
         fill = latex2exp::TeX("$\\textit{G}_{obj}$"),
         title = "Constraints on W Violated") +
    theme(panel.spacing = unit(20, "points"))
  
  obj_func_plot <- without_W_penalty / with_W_penalty + 
    plot_layout(guides = "collect")
  
  # Save as a pdf --- need to insert epsilon axis labels manually because
  # R only has varepsilon.
  ggsave(filename = here("img/obj_func_landscape_ten_factor.pdf"),
         obj_func_plot,
         dpi = 320,
         height = 5.75,
         width = 8)
  
  saveRDS(par_grid, here("data/obj_function_values_ten_factor.RDS")) 
}

knitr::include_graphics(here("img/obj_func_landscape_ten_factor.png"))
```

In contrast to the $\TKLrmsea$ method, the $\TKLcfi$ and $\TKLrmseacfi$ methods rarely produced solutions that violated the constraints on $\mathbf{W}$. This result can be at least partially explained by comparing the objective function surfaces shown in \@ref(fig:obj-function-landscape). Unlike the $\TKLrmsea$ method, the objective function surfaces for the $\TKLcfi$ and $\TKLrmseacfi$ methods were lowest when both $\epsilon$ and $\nu_{\textrm{e}}$ were small. The optimization procedure was therefore unlikely to move toward values of $\epsilon$ and $\nu_{\textrm{e}}$ that produced solutions with violated $\mathbf{W}$ constraints. Moreover, even if the optimization procedure was initialized at large values of of $\epsilon$ and $\nu_{\textrm{e}}$ corresponding to violated $\mathbf{W}$ constraints, the gradient of the objective-function surface would have directed the optimization procedure back toward smaller $\epsilon$ and $\nu_{\textrm{e}}$ values that did not violate the $\mathbf{W}$ constraints. Therefore, adding a target CFI value in addition to (or instead of) a target RMSEA value ended up being far more effective than using an explicit penalty in \autoref{eq:rmsea-cfi-obj-function} for ensuring a meaningful distinction between major and minor common factors.

## Distributions of Fit Statistics

One of the primary questions the simulation study was intended to answer was whether the five model-error methods produced solutions with different fit index values when used with the same error-free models and target RMSEA and CFI values. In this section, I report the distributions of the RMSEA, CFI, TLI and CRMR model-fit indices for solutions produced by each of the five model-error methods ($\TKLrmsea$, $\TKLcfi$, $\TKLrmseacfi$, CB, and WB). All of the fit indices reported in this section reflect the discrepancy between $\bSigma$ and $\bOmega$. Plots for fit indices reflecting the discrepancy between $\bSigma$ and $\bOmegaHat$ similar to those presented in this section can be found in \@ref(sigma-hat-fit-indices). 

### RMSEA

Of the fit indices investigated in this study, the RMSEA value has been most often used as a measure of model fit when generating covariance or correlation matrices with model error [@kracht2022; @lorenzo-seva2020; @myers2015a; @maccallum2001; @briggs2003]. \@ref(fig:rmsea-distributions) shows box-plots summarizing the distributions of RMSEA values for each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. These variables were the most important in terms of the absolute difference between the target and observed RMSEA values, as evidenced by the effect sizes reported in the ANOVA summary table in \@ref(rmsea-anova-summary). Notice in \@ref(fig:rmsea-distributions) that the $\TKLrmsea$ and CB methods almost always produced solutions with RMSEA values that were very close to the target RMSEA values. This result makes sense because both methods use optimization to produce solutions with RMSEA values close to a specified target. However, a somewhat unexpected result was that the CB method occasionally produced solutions with RMSEA values that were much higher or lower than the target value, particularly in conditions with ten major factors.

After the $\TKLrmsea$ and CB methods, \@ref(fig:rmsea-distributions) shows that the WB method was the next best model-error method in terms of producing solutions with RMSEA values close to the target values. In fact, the WB method produced solutions with median RMSEA values that were as close to the target values as those from $\TKLrmsea$ and CB solutions. However, the WB method also led to more variable RMSEA values, particularly in conditions with Poor model fit and many factors.

The two methods that performed worst in terms of producing solutions with RMSEA values close to the targets were the $\TKLrmseacfi$ and $\TKLcfi$ methods. \@ref(fig:rmsea-distributions) shows that these methods often led to RMSEA values that were lower than the target values, except when there were relatively few factors and strong factor loadings. The largest differences between the observed and target RMSEA values for the $\TKLrmseacfi$ and $\TKLcfi$ methods occurred in conditions with Poor model fit and weak factor loadings. In those conditions, both methods led to RMSEA values that were considerably lower than the target values.

```{r rmsea-distributions, fig.cap = "Distributions of the RMSEA values for solutions produced by each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. The dashed lines indicate the target RMSEA value for each condition. Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.", fig.align='left', out.width=425}
if (make_plots) {
  rmsea_distributions <- 
    results_matrix %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = rmsea)) +
    geom_hline(aes(yintercept = target_rmsea), 
               color = "black", linetype = "dashed", size = .25) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = "RMSEA") +
    theme_bw() +
    theme(plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/rmsea_distributions.png"), 
    plot = rmsea_distributions,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/rmsea_distributions.png"))
```

### CFI

The distributions of CFI values for the solutions produced using the five model-error methods are shown in \@ref(fig:cfi-distributions), conditioned on number of factors, model fit, and factor loading strength. These were the most important variables in terms of the absolute difference between the target and observed CFI values, as indicated by the effect sizes shown in the ANOVA summary table in \@ref(cfi-anova-summary). As with \@ref(fig:rmsea-distributions), the middle levels of model fit and factor loading strength were omitted to converse space. \@ref(fig:cfi-distributions) shows that the results for CFI were nearly opposite to the results for RMSEA. Specifically, whereas the $\TKLrmsea$, CB, and WB methods produced solutions with RMSEA values much closer to the target values than those produced by the $\TKLrmseacfi$ or $\TKLcfi$ methods, the $\TKLrmseacfi$ and $\TKLcfi$ produced solutions with CFI values that were closer to the target CFI values compared to the other model-error methods in most conditions. The differences between the methods that optimized for CFI ($\TKLrmseacfi$ and $\TKLcfi$) and the other model-error methods were largest for conditions with Poor model fit, low factor loadings, and many factors (see the third row of \@ref(fig:cfi-distributions)). On the other hand, \@ref(fig:cfi-distributions) shows that all of the model-error methods produced similar CFI values (that were close to the target CFI value) for conditions with Very Good model fit and strong factor loadings.

It is worth highlighting the result that the $\TKLrmseacfi$ and $\TKLcfi$ produced solutions with very similar CFI values in most conditions. These two methods also produced solutions with very similar RMSEA values in many conditions, as shown in \@ref(fig:rmsea-distributions). Indeed, the $\TKLrmseacfi$ and $\TKLcfi$ methods led to much more similar results in terms of both RMSEA and CFI than the $\TKLrmseacfi$ and $\TKLrmsea$ methods. Put another way, optimizing for CFI alone generally led to similar results compared to optimizing for both CFI and RMSEA, whereas optimizing for RMSEA alone generally led to solutions with much different RMSEA and CFI values compared to optimizing for both CFI and RMSEA. This suggested that CFI was more sensitive to small changes in parameter values than RMSEA, a hypothesis that is explored in greater depth later on in this section.

The distributions of CFI values shown in \@ref(fig:cfi-distributions) also emphasize the importance of reporting multiple fit indices when simulating correlation or covariance matrices with model error when compared with the distributions of RMSEA values in \@ref(fig:rmsea-distributions). For instance, the $\TKLrmsea$ and CB methods produced solutions with RMSEA values close to the target RMSEA value of .09 in conditions with Poor model fit and weak factor loadings. However, \@ref(fig:cfi-distributions) shows that those two model-error methods led to solutions with unacceptably low CFI values in the same conditions.

```{r cfi-distributions, fig.cap = "Distributions of the CFI values for solutions produced by each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. The dashed lines indicate the target CFI value for each condition. Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.", out.width=425}
if (make_plots) {
  cfi_distributions <- 
    results_matrix %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = cfi)) +
    geom_hline(aes(yintercept = target_cfi), 
               color = "black", linetype = "dashed", size = .25) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = "CFI") +
    theme_bw() +
    theme(plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/cfi_distributions.png"), 
    plot = cfi_distributions,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/cfi_distributions.png"))
```

### TLI

The distributions of TLI values for the solutions produced using the five model-error methods are shown in \@ref(fig:tli-distributions), conditioned on number of factors, model fit, and factor loading strength. Overall, the distributions of TLI values were quite similar to the distributions of CFI values shown in \@ref(fig:cfi-distributions). In particular, the $\TKLcfi$ and $\TKLrmseacfi$ methods tended to produce solutions with higher TLI values than the other model-error methods, except for conditions with a single factor, Poor model fit, and strong factor loadings. Similar to CFI, the differences in TLI values resulting from the $\TKLcfi$ and $\TKLrmseacfi$ methods compared to the other model-error methods were most pronounced for conditions with many factors, weak factor loadings, and Poor model fit.

```{r tli-distributions, out.width="100%"}
#| fig.cap="Distributions of the TLI values for solutions produced by each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. The dashed lines indicate the threshold values of TLI that correspond to the targeted levels of model fit, according to Hu and Bentler (1999). Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  tli_distributions <- 
    results_matrix %>%
    mutate(tli = case_when(tli < 0 ~ 0, TRUE ~ tli)) %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = tli)) +
    geom_hline(aes(yintercept = target_cfi), 
               color = "black", linetype = "dashed", size = .25) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = "TLI") +
    theme_bw() +
    theme(panel.spacing.x = unit(10, units = "pt"),
          plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/tli_distributions.png"), 
    plot = tli_distributions,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/tli_distributions.png"))
```

### CRMR

The distributions of the CRMR model fit index by level of model fit, factor loading, number of major common factors, and model-error method are represented as boxplots in \@ref(fig:crmr-distributions). The figure shows that in conditions with Very Good model fit, all of the model-error methods led to CRMR values that were generally at or below 0.05, the rule-of-thumb CRMR threshold for good model fit given by Hu and Bentler [-@hu1999]. In general, all of the model fit methods led to similar CRMR distributions when model fit was Very Good. In those conditions, the largest differences in CRMR values between model-error methods occurred when there were many factors and low factor loadings, where the $\TKLrmseacfi$ and $\TKLcfi$ model-error methods led to lower CRMR values compared to the other methods.

Larger differences between the model-error methods emerged when model fit was Poor. In those conditions, only the $\TKLrmsea$ and WB methods consistently led to CRMR values that were near or above the threshold CRMR value for acceptable model fit given by Hu and Bentler [-@hu1999]. In fact, the $\TKLcfi$ and $\TKLrmseacfi$ methods often led to CRMR values that were less than the Hu and Bentler [-@hu1999] threshold for good model fit and were frequently the methods that led to the lowest CRMR values. The only conditions where the $\TKLcfi$ and $\TKLrmseacfi$ methods led to higher median CRMR values were conditions with one-factor models, salient factor loadings of 0.8, and Poor model fit.

```{r crmr-distributions, fig.cap="Distributions of the CRMR values for solutions produced by each of the model-error methods, conditioned on number of factors, model fit, and factor loading strength. Recommendations for threshold values of SRMR/CRMR are not as fine-grained as for some other fit indices, but threshold values of 0.05 and 0.08 were proposed by Hu and Bentler (1999) as more and less conservative upper-bounds for acceptable SRMR/CRMR values. The dashed lines indicate these two threshold values. Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne.", out.width="100%"}
if (make_plots) {
  crmr_distributions <- 
    results_matrix %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    mutate(target_crmr = case_when(model_fit == "Poor" ~ 0.08,
                                   model_fit == "Very Good" ~ 0.05)) %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = crmr)) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    geom_hline(yintercept = 0.05, 
               color = "black", linetype = "dashed", size = .25) +
    geom_hline(yintercept = 0.08, 
               color = "black", linetype = "dashed", size = .25) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = "CRMR") +
    theme_bw() +
    theme(plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/crmr_distributions.png"), 
    plot = crmr_distributions,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/crmr_distributions.png"))
```

## Fit Index Agreement

In the previous section, I reported results in terms of the RMSEA, CFI, TLI, and CRMR values individually. However, it is often recommended to evaluate model fit using more than one fit index [@hu1999; @raykov2012]. Therefore, I was interested in the extent to which the model-error methods included in this simulation study led to solutions with fit indices indicating similar levels of model fit. In this section, I report the results in terms of fit index agreement. Specifically, I address two main questions. First, how well did model-error methods do at producing solutions with RMSEA and CFI values that were close to the target values corresponding to a particular level of model fit? Second, to what extent did the model-error methods produce solutions with fit index values that corresponded to the same qualitative interpretation of model fit? For both questions, I focused on the agreement between RMSEA and CFI, both for the sake of simplicity and because RMSEA and CFI were the fit indices that I used as target values for the model-error methods.

To evaluate how well each of the model error-methods did at producing solutions that had both RMSEA and CFI values that were close to the target values, I used the metric

\begin{equation}
D = |\textrm{RMSEA}_{\textrm{obs}} - \textrm{RMSEA}_{\textrm{T}}| + |\textrm{CFI}_{\textrm{obs}} - \textrm{CFI}_{\textrm{T}}|,
(\#eq:distance-between-observed-and-target)
\end{equation}

\noindent where $\textrm{RMSEA}_{\textrm{obs}}$ and $\textrm{CFI}_{\textrm{obs}}$ denote the observed RMSEA and CFI values for each solution and $\textrm{RMSEA}_{\textrm{T}}$ and $\textrm{CFI}_{\textrm{T}}$ denote the target RMSEA and CFI values. A small $D$ value indicated that a solution was good in the sense that the observed RMSEA and CFI values were close to the target RMSEA and CFI values. On the other hand, a large $D$ value indicated a poor solution with either one or both of the observed RMSEA and CFI values far from the corresponding target values.

\@ref(fig:d3-plot) shows box-plots of the $D$ values for each of the model-error methods conditioned on the number of major common factors, model fit, and factor loading strength.[^d3-full-figure] These variables had the largest effect on $D$ values, as indicated by the effect sizes reported in the ANOVA summary table in \@ref(d-anova-summary). The figure shows that all of the model-error methods led to good results (i.e., small $D$ values) in conditions with Very Good model fit and strong factor loadings. Similarly, all of the model-error methods led to reasonably good results in conditions with one or three major factors, Poor model fit, and strong factor loadings. In the remaining conditions where the model-error methods did not all lead to small $D$ values, the $\TKLcfi$ and $\TKLrmseacfi$ methods led to smaller $D$ values than the other model-error methods. In particular, the $\TKLcfi$ and $\TKLrmseacfi$ methods led to much smaller $D$ values than the alternative model-error methods in conditions with many major common factors and weak factor loadings, especially when model fit was Poor. The CB method typically led to the next-smallest $D$ values in these conditions, followed by the $\TKLrmsea$ and WB methods.

[^d3-full-figure]: Some conditions were omitted from this figure to conserve space. A full version is provided in \@ref(full-d3-plot).

```{r d3-plot, out.width="100%"}
#| fig.cap = "The sum of the absolute differences between the observed and target RMSEA and CFI values ($D$), conditioned on number of factors, model fit, and factor loading strength. Note that some levels of model fit and factor loading strength were omitted to conserve space. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."
if (make_plots) {
  d_plot <- 
    results_matrix %>%
    filter(rmsea < 3, loading_numeric != .6, model_fit != "Fair") %>%
    ggplot(aes(x = fct_rev(error_method_rec), y = d3)) +
    geom_boxplot(outlier.size = .5, outlier.stroke = .25, lwd = .5,
                 fatten = 1.5) +
    coord_flip() +
    scale_x_discrete(labels = parse(text=levels(results_matrix$error_method_rec)[5:1])) +
    facet_grid(model_fit_rec * loading_rec ~ 
                 factors_rec) +
    labs(x = "",
         y = latex2exp::TeX("\\textit{D}")) +
    theme_bw() +
    theme(plot.margin = margin(0, 0, 0, -14, unit = "pt"))
  
  ggsave(
    filename = here("img/d_plot.png"), 
    plot = d_plot,
    dpi = 320,
    height = 4, width = 5.25,
    scale = 1.4
  )
}

knitr::include_graphics(here("img/d_plot.png"))
```

In addition to determining how far the observed RMSEA and CFI values differed from the target RMSEA and CFI values (in terms of $D$), I obtained a second perspective on model fit agreement by determining how often fit indices led to the same qualitative assessment of model fit using rule-of-thumb threshold values of RMSEA and CFI such as those reported by Hu and Bentler [-@hu1999]. To calculate the rates of qualitative model fit agreement, I first assigned each simulated correlation matrix into one of three qualitative model fit categories based on its observed RMSEA and CFI values. For RMSEA, a simulated correlation matrix was considered to have good model fit if the observed RMSEA value was less than or equal to 0.05. If the RMSEA was greater than 0.05 but less than .10, it was considered to have acceptable model fit. RMSEA values greater than 0.10 were considered to represent unacceptable model fit. Similarly, observed CFI values greater than .95 were considered to represent good model fit, CFI values between .95 and .90 were considered to represent acceptable model fit, and CFI values below .90 were considered to represent unacceptable model fit.

```{r calculate-agreement-rate, echo = FALSE, include = FALSE}
agreement_rates <- results_matrix %>% mutate(
  rmsea_subj_fit = case_when(rmsea <= 0.05 ~ "Good",
                             rmsea > 0.05 & rmsea <= 0.1 ~ "Acceptable",
                             rmsea > 0.1 ~ "Unacceptable"),
  cfi_subj_fit = case_when(cfi >= .95 ~ "Good",
                           cfi < .95 & cfi >= .9 ~ "Acceptable",
                           cfi < .9 ~ "Unacceptable")
) %>% mutate(
  fit_agreement = (rmsea_subj_fit == cfi_subj_fit)
) %>% group_by(error_method) %>% 
  summarise(mean_fit_agreement = mean(fit_agreement, na.rm = TRUE)) %>%
  pivot_wider(names_from = error_method, values_from = mean_fit_agreement)

colnames(agreement_rates) <-
  c("tkl_rmsea", "tkl_cfi", "tkl_rmsea_cfi", "cb", "wb")
```

The percent of simulated $\bSigma$ matrices that led to fit indices indicating qualitative agreement on model fit are shown in \@ref(fig:fit-agreement-fig), conditioned on number of factors, factor loading strength, model fit, and model-error method. These variables had the largest effects on qualitative model fit index agreement rates, as indicated by the logistic regression coefficient estimates reported in \@ref(qual-fit-regression-summary). \@ref(fig:fit-agreement-fig) shows that no single model-error method always led to the highest rates of qualitative fit agreement. When averaged over all conditions, the $\TKLcfi$ model-error method led to the highest rate of qualitative fit index agreement (`r scales::percent(agreement_rates$tkl_cfi, accuracy = .1)`), followed by the CB (`r scales::percent(agreement_rates$cb, accuracy = .1)`), $\TKLrmseacfi$ (`r scales::percent(agreement_rates$tkl_rmsea_cfi, accuracy = .1)`), $\TKLrmsea$ (`r scales::percent(agreement_rates$tkl_rmsea, accuracy = .1)`), and WB (`r scales::percent(agreement_rates$wb, accuracy = .1)`) methods. 

Breaking these results out further by level of target model fit, the $\TKLcfi$ and $\TKLrmseacfi$ methods almost always led to qualitative model fit agreement rates of 100% in conditions with Very Good model fit. The other model-error methods also led to qualitative model fit agreement rates of nearly 100% in conditions with Very Good model fit and strong factor loadings (i.e., Loading = 0.8), but led to much lower rates of agreement in conditions with Very Good model fit and weaker factor loadings. Results for conditions with Fair or Poor model fit were less straightforward. When target model fit was Fair, the $\TKLcfi$ method often led to the highest rates of qualitative fit agreement of the model-error methods, particularly when factor loadings were weak and when there were many major common factors. When target model fit was Poor, all of the model-error methods had low qualitative fit agreement rates except in conditions with strong factor loadings.

```{r fit-agreement-fig, out.width='100%'}
#| fig.cap = "The percent of cases where the observed RMSEA and CFI values led to the same qualitative evaluation of model fit based on the threshold values suggested by Hu and Bentler (1999). TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  
  labels <- list("TKL (RMSEA)" = TeX("$TKL_{RMSEA}$"),
                 "TKL (RMSEA/CFI)" = TeX("$TKL_{RMSEA/CFI}$"),
                 "TKL (CFI)" = TeX("$TKL_{CFI}"))
  
  results_matrix %>%
    mutate(
      rmsea_subj_fit = case_when(rmsea <= 0.05 ~ "Good",
                                 rmsea > 0.05 & rmsea <= 0.1 ~ "Acceptable",
                                 rmsea > 0.1 ~ "Unacceptable"),
      cfi_subj_fit = case_when(cfi >= .95 ~ "Good",
                               cfi < .95 & cfi >= .9 ~ "Acceptable",
                               cfi < .9 ~ "Unacceptable")
    ) %>% mutate(
      fit_agreement = (rmsea_subj_fit == cfi_subj_fit)
    ) %>% group_by(factors, loading_rec, model_fit_rec, error_method) %>%
    summarize(agreement_rate = mean(fit_agreement, na.rm = TRUE)) %>%
    ggplot(aes(y = agreement_rate, x = factors, color = error_method, 
               shape = error_method, linetype = error_method, 
               group = error_method)) +
    scale_y_continuous(label = scales::percent) +
    scale_x_continuous(breaks = 1:10) +
    geom_point() +
    geom_line() +
    scale_color_brewer(palette = "Dark2", type = "qual", labels = labels) +
    scale_shape_discrete(labels = labels) +
    scale_linetype_discrete(labels = labels) +
    guides(color = guide_legend(title = "Error Method"),
           shape = guide_legend(title = "Error Method"),
           linetype = guide_legend(title = "Error Method")) + 
    labs(y = "Fit Index Agreement", x = "Factors",
         shape = "Error Method", color = "Error Method", linetype = "Error Method") +
    facet_grid(model_fit_rec ~ loading_rec) +
    theme_bw() +
    theme(legend.position = "bottom")
  
  ggsave(filename = here("img/fit_index_agreement.png"),
         height = 6,
         width = 7,
         dpi = 320)
}

knitr::include_graphics(here("img/fit_index_agreement.png"))
```
Although the $\TKLcfi$ and $\TKLrmseacfi$ methods led to somewhat different results in terms of qualitative model fit agreement, it was both interesting and unexpected that these methods led to similar results in terms of $D$ and observed RMSEA and CFI values, whereas the $\TKLrmsea$ method often led to results that were more similar to those of the CB and WB methods. The similarity between the $\TKLcfi$ and $\TKLrmseacfi$ results suggested that the CFI had more influence in \autoref{eq:rmsea-cfi-obj-function} than RMSEA, even when the fit indices were equally weighted. Evidence for this hypothesis is provided in \@ref(fig:rmsea-cfi-distributions), which shows observed RMSEA and CFI values for solutions from the $\TKLrmsea$, $\TKLcfi$, and $\TKLrmseacfi$ methods, conditioned on number of factors and model fit. The figure shows that solutions generated using the $\TKLrmsea$ method had little variability in terms of RMSEA values. On the other hand, the CFI values for those solutions often had much more variability, particularly in conditions with weak factor loadings. When the $\TKLcfi$ method was used, solutions had RMSEA and CFI values that were generally constrained to a small range within each condition. These results indicated that the range of possible CFI values was much larger for solutions with a fixed RMSEA value than the range of RMSEA values for solutions with a fixed CFI value in many conditions. Therefore, the $\TKLcfi$ and $\TKLrmseacfi$ might have led to such similar results because including a CFI target constrained the range of RMSEA values. Put another way, incorporating a target RMSEA value in addition to a target CFI value might have had little effect compared to using only a target CFI value because making small changes to CFI often led to large changes in RMSEA.

```{r rmsea-cfi-distributions, out.width='100%'}
#| fig.cap = "RMSEA and CFI values for the TKL-based model-error methods, conditioned on number of factors and model fit. The dashed vertical and horizontal lines indicate the target RMSEA and CFI values, respectively. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."
if (make_plots) {
    labels <- list("TKL (RMSEA)" = TeX("$TKL_{RMSEA}$"),
                 "TKL (RMSEA/CFI)" = TeX("$TKL_{RMSEA/CFI}$"),
                 "TKL (CFI)" = TeX("$TKL_{CFI}"))
  
  results_matrix %>%
    filter(rmsea < 3, str_detect(error_method, "TKL"),
           factors != 3, loading_numeric != .6) %>%
    ggplot(aes(x = rmsea, y = cfi)) +
    geom_vline(aes(xintercept = target_rmsea), size = .3, linetype = "dashed") +
    geom_hline(aes(yintercept = target_cfi), size = .3, linetype = "dashed") +
    geom_jitter(alpha = .1, size = 1.25, 
                aes(color = error_method, shape = error_method)) +
    scale_color_brewer(palette = "Dark2", type = "qual", labels = labels) +
    scale_shape_discrete(labels = labels) +
    facet_grid(model_fit_rec ~ factors_rec * loading_rec, scales = "fixed") +
    labs(x = "RMSEA", y = "CFI", color = "Error Method", shape = "Error Method") +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +
    theme_bw() +
    theme(legend.position = "bottom",
          panel.spacing.x = unit(8, units = "points"))
  
  ggsave(filename = here("img/rmsea-cfi-distributions.png"),
         dpi = 320,
         scale = 1.1,
         height = 5, width = 7)
}

knitr::include_graphics(path = here("img/rmsea-cfi-distributions.png"))
```

To better understand the conditional distribution of CFI values for solutions with fixed RMSEA values, I used the $\TKLrmsea$ method to generate solutions with a range of target RMSEA values for a subset of conditions from the main simulation study. Specifically, I generated 100 solutions for each of 16 target RMSEA values equally-spaced between 0.025 and 0.100 for each condition of the main simulation design with uncorrelated major common factors. (Conditions with correlated major common factors were omitted to make the number of conditions manageable.) The results can be seen in \@ref(fig:rmsea-conditional-cfi), which shows the CFI and RMSEA values for each condition. The solid black lines in the figure indicate where RMSEA is equal to $1 - \textrm{CFI}$ and can be used to easily determine which fit index changed most quickly as a function of the other fit index. RMSEA and $1 - \textrm{CFI}$ are comparable in this context because they indicate roughly the same level of qualitative model fit over the range of target RMSEA values. For example, both $\textrm{RMSEA} = 0.05$ and $\textrm{CFI} = .95$ have been used as threshold values for good model fit [@hu1999]. \@ref(fig:rmsea-conditional-cfi) shows that CFI values decreased faster than RMSEA values increased for most of the included conditions, with the exception of conditions with relatively few major factors and strong factor loadings. Moreover, the figure shows that there was little variability in the conditional CFI values for a particular RMSEA value, except in conditions with few items and weak factor loadings.

```{r rmsea-conditional-cfi, out.width='100%'}
#| fig.cap = "Observed CFI and RMSEA values for solutions generated using the $\\textrm{TKL}_{\\textrm{RMSEA}}$ method. For each combination of number of factors, number of items per factor, and factor loading strength, 100 solutions were generated for each of 16 target RMSEA values equally-spaced between 0.025 and 0.100. The solid black line indicates where RMSEA and $1 - \\textrm{CFI}$ were equal."

if (make_plots) {
  # Show possible CFI values for a range of RMSEA values
  reps <- 100
  
  factors <- unique(results_matrix$factors)
  items_per_factor <- unique(results_matrix$items_per_factor)
  loading <- unique(results_matrix$loading_numeric)
  target_rmsea <- seq(0.025, 0.1, by = 0.005)
  
  conditions_matrix <- expand.grid(
    factors = factors,
    items_per_factor = items_per_factor,
    loading = loading,
    target_rmsea = target_rmsea
  )
  
  set.seed(666)
  rmsea_cfi_vals <- pro_map_dfr(
    .x = 1:nrow(conditions_matrix), 
    .f = function(condition) {
      mod <- simFA(
        Model = list(NFac = conditions_matrix$factors[condition],
                     NItemPerFac = conditions_matrix$items_per_factor[condition],
                     Model = "orthogonal"),
        Loadings = list(FacLoadDist = "fixed",
                        FacLoadRange = conditions_matrix$loading[condition])
      )
      
      map_dfr(.x = 1:reps,
              .f = function(x, target_rmsea) {
                sol <- noisemaker(mod = mod, 
                                  method = "TKL", 
                                  target_rmsea = target_rmsea,
                                  target_cfi = NULL,
                                  tkl_ctrl = list(penalty = 1e6,
                                                  NWmaxLoading = 2,
                                                  WmaxLoading = .3))
                w_constraints_violated <- sum(sol$W[,1] >= .3) > 2
                c(rmsea = sol$rmsea, cfi = sol$cfi, 
                  w_constraints_violated = w_constraints_violated)
              }, target_rmsea = conditions_matrix$target_rmsea[condition])
    }
  )
  
  rmsea_cfi_vals <- bind_cols(
    rmsea_cfi_vals,
    conditions_matrix[rep(1:nrow(conditions_matrix), each = reps),]
  )
  
  rmsea_cfi_vals <- rmsea_cfi_vals %>%
    mutate(
      factors_rec = factor(factors, 
                           labels = levels(results_matrix$factors_rec)),
      loading_rec = factor(loading, 
                           labels = levels(results_matrix$loading_rec)),
      items_per_factor_rec = factor(items_per_factor, 
                                    labels = levels(results_matrix$items_per_factor_rec))
    )
  
  saveRDS(rmsea_cfi_vals, file = here("data/rmsea_conditional_cfi.RDS"))
  # rmsea_cfi_vals <- readRDS(here("data/rmsea_conditional_cfi.RDS"))
  
  rmsea_cfi_vals %>%
    ggplot(aes(x = rmsea, y = cfi)) +
    geom_point(size = 1, alpha = .1) +
    geom_abline(aes(slope = -1, intercept = 1), size = .5, alpha = .5) +
    facet_grid(loading_rec * items_per_factor_rec ~ factors_rec) +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +
    labs(x = "RMSEA", y = "CFI") +
    theme_bw() +
    theme(legend.position = "bottom",
          panel.spacing.x = unit(14, units = "points"))
  
  ggsave(filename = here("img/rmsea_conditional_cfi.png"),
         height = 7,
         width = 7,
         scale = 1.15,
         dpi = 320)
}

knitr::include_graphics(
  path = here("img/rmsea_conditional_cfi.png")
)
```

In addition to looking at the conditional distribution of CFI values at fixed RMSEA values, I also looked at the conditional distribution of RMSEA values at fixed CFI values. Similar to the previous procedure for RMSEA, I used the $\TKLcfi$ method to generate 100 solutions with 19 target CFI values equally-spaced between .90 and .99 for each condition of the main simulation design with uncorrelated major common factors. The results are shown in \@ref(fig:cfi-conditional-rmsea). As in the previous figure, the points in \@ref(fig:cfi-conditional-rmsea) were often below the solid black line indicating (roughly) equivalent qualitative model fit. Points falling below the line denoted cases with RMSEA values that indicated better model fit than their corresponding CFI values. The figure also shows that CFI changed more quickly as a function of RMSEA than RMSEA changed as a function of CFI. Finally, the figure shows that there was little variability in the conditional RMSEA values for a particular CFI value in all of the conditions except conditions with one factor and five items per factor.

```{r cfi-conditional-rmsea, out.width = '100%'}
#| fig.cap = "Observed RMSEA and CFI values for solutions generated using the $\\textrm{TKL}_{\\textrm{CFI}}$ method. For each combination of number of factors, number of items per factor, and factor loading strength, 100 solutions were generated for each of 19 target CFI values equally-spaced between 0.90 and 0.99. The solid black line indicates where RMSEA and $1 - \\textrm{CFI}$ were equal."

if (make_plots) {
  reps <- 100
  
  factors <- unique(results_matrix$factors)
  items_per_factor <- unique(results_matrix$items_per_factor)
  loading <- unique(results_matrix$loading_numeric)
  target_cfi <- seq(0.90, 0.99, by = 0.005)
  
  conditions_matrix <- expand.grid(
    factors = factors,
    items_per_factor = items_per_factor,
    loading = loading,
    target_cfi = target_cfi
  )
  
  set.seed(666)
  cfi_rmsea_vals <- pblapply(
    X = 1:nrow(conditions_matrix), 
    FUN = function(condition) {
      mod <- simFA(
        Model = list(NFac = conditions_matrix$factors[condition],
                     NItemPerFac = conditions_matrix$items_per_factor[condition],
                     Model = "orthogonal"),
        Loadings = list(FacLoadDist = "fixed",
                        FacLoadRange = conditions_matrix$loading[condition])
      )
      
      map_dfr(.x = 1:reps,
              .f = function(x, target_cfi) {
                sol <- noisemaker(mod = mod, 
                                  method = "TKL", 
                                  target_rmsea = NULL,
                                  target_cfi = target_cfi,
                                  tkl_ctrl = list(penalty = 1e6,
                                                  NWmaxLoading = 2,
                                                  WmaxLoading = .3))
                w_constraints_violated <- sum(sol$W[,1] >= .3) > 2
                c(rmsea = sol$rmsea, cfi = sol$cfi, 
                  w_constraints_violated = w_constraints_violated)
              }, target_cfi = conditions_matrix$target_cfi[condition])
    }
  )
  
  cfi_rmsea_vals <- bind_rows(cfi_rmsea_vals)
  cfi_rmsea_vals <- bind_cols(
    cfi_rmsea_vals,
    conditions_matrix[rep(1:nrow(conditions_matrix), each = reps),]
  )
  
  cfi_rmsea_vals <- cfi_rmsea_vals %>%
    mutate(
      factors_rec = factor(factors, 
                           labels = levels(results_matrix$factors_rec)),
      loading_rec = factor(loading, 
                           labels = levels(results_matrix$loading_rec)),
      items_per_factor_rec = factor(items_per_factor, 
                                    labels = levels(results_matrix$items_per_factor_rec))
    )
  
  saveRDS(cfi_rmsea_vals, file = here("data/cfi_conditional_rmsea.RDS"))
  
  cfi_rmsea_vals %>%
    ggplot(aes(y = rmsea, x = cfi)) +
    geom_point(size = 1, alpha = .1) +
    geom_abline(aes(slope = -1, intercept = 1), size = .5, alpha = .5) +
    facet_grid(loading_rec * items_per_factor_rec ~ factors_rec) +
    guides(color = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +
    labs(y = "RMSEA", x = "CFI") +
    theme_bw() +
    theme(legend.position = "bottom",
          panel.spacing.x = unit(14, units = "points"))
  
  ggsave(filename = here("img/cfi_conditional_rmsea.png"),
         height = 7,
         width = 7,
         scale = 1.15,
         dpi = 320)
}

knitr::include_graphics(here("img/cfi_conditional_rmsea.png"))
```

Taken together, the results in \@ref(fig:rmsea-cfi-distributions), \@ref(fig:rmsea-conditional-cfi), and \@ref(fig:cfi-conditional-rmsea) can help explain why the $\TKLcfi$ and $\TKLrmseacfi$ model-error methods often led to similar results. Because CFI values decreased more quickly than RMSEA values increased in most conditions, changes to parameter values that led to an RMSEA value that was closer to the target RMSEA value but moved the CFI value further from the target CFI value were more costly than changes that prioritized the target CFI value. Moreover, \@ref(fig:rmsea-conditional-cfi) and \@ref(fig:cfi-conditional-rmsea) also provided indications of which conditions were most likely to lead to conflicting qualitative interpretations of RMSEA and CFI. Points far from the line indicated RMSEA and CFI pairs that led to conflicting qualitative interpretations of model fit because RMSEA and $1 - \textrm{CFI}$ indicated approximately the same qualitative model fit over the range of target RMSEA values. Both figures show that conditions with weak factor loadings, many factors and items, and high RMSEA values (or low CFI values) were most likely to result in conflicting fit index interpretations.

The frequent disagreement between RMSEA and CFI can be also be understood by considering how these fit indices differ in how they describe model fit. Recall from \@ref(population-model-fit) that in the population context, absolute fit indices (e.g., RMSEA) indicate how different the $\bSigma$ and $\bOmega$ matrices are from one another. On the other hand, relative model fit indices (e.g., CFI) indicate how much better the population model fits $\bSigma$ compared to the independence model. Generating a $\bSigma$ matrix corresponding to a particular RMSEA value is therefore a relatively straightforward matter of perturbing $\bOmega$ until the discrepancy between $\bSigma$ and $\bOmega$ results in the target RMSEA value. On the other hand, generating a $\bSigma$ matrix corresponding to a particular CFI value is more complex because CFI takes into account both the fit of the hypothesized model indicated by $\bOmega$ and the fit of the baseline (independence) model. Thus, perturbing the elements of $\bOmega$ to create $\bSigma$ negatively affects the fit of the baseline model in addition to the fit of the hypothesized model because the independence model implies that $\bSigma$ should be an identity matrix. However, unless the hypothesized model leads to a much worse fit (i.e., a much larger minimized discrepancy function value) than the baseline model, the resulting CFI value will be relatively small.

This is demonstrated in \@ref(fig:cfi-as-factors-increase), which shows the minimized discrepancy function values for the hypothesized and baseline models as the number of major factors increased from one to ten, along with the corresponding CFI and RMSEA values. The hypothesized models (i.e., the population models without model error) were all orthogonal models with salient major factor loadings of 0.4, 15 items per factor, and between 1 and 10 common factors. The $\bSigma$ matrices were generated using the $\TKLrmsea$ method with a target RMSEA value of 0.09. To interpret Panel A of the figure, recall that $\textrm{CFI} = 1 - F_t / F_b$ as defined in \autoref{eq:cfi}, where $F_t / F_b$ is the ratio of the minimized discrepancy function values for the hypothesized and baseline models. Panel A of \@ref(fig:cfi-as-factors-increase) shows that both $F_t$ and $F_b$ increased as the number of factors increased, holding everything else constant. However, $F_t$ grew fast enough that $F_t / F_b$ decreased as the number of factors increased, resulting in lower CFI values as RMSEA remained fixed (as shown in Panels B and C). To keep CFI constant as the number of factors increased, $F_t$ would have needed to increase at the same rate as $F_b$. For instance, Panel A includes a line indicating the values of $F_t$ required to produce a CFI value of 0.90 for each number of factors. Although the figure only shows results for a specific set of conditions, it demonstrates the tension between RMSEA and CFI that occurs as the order of $\bOmega$ increases. Specifically, it shows that fixing RMSEA at a value indicating Fair or Poor model fit when there are many items or factors requires an $F_m$ value that is too large (relative to $F_b$) to produce an acceptable CFI value.

```{r cfi-as-factors-increase, out.width = "100%"}
#| fig.cap = "Panel A: Minimized discrepancy function values, CFI, and RMSEA values for $\\Sigma$ matrices generated from orthogonal models with salient major factor loadings of 0.4, 15 items per factor, and between 1 and 10 factors. The $\\Sigma$ matrices were generated using the $\\textrm{TKL}_{\\textrm{RMSEA}}$ model-error method with a target RMSEA value of 0.09. The line in the left-most panel labeled ``Target for CFI = .90'' indicates the value of $F_t$ that would be needed to obtain a CFI value of .90, given the value of $F_b$. Panels B and C: Observed CFI and RMSEA values for each simulated $\\Sigma$ matrix in Panel A."

if (make_plots) {
  # Show why CFI degrades as the dimensions of Sigma increase
  factors <- 1:10
  out <- cbind("factors" = factors, "Ft" = NA, "Fb" = NA, "cfi" = NA, "rmsea" = NA)
  
  set.seed(42)
  out <- lapply(
    X = factors, 
    FUN = function(factor_num) {
      m1 <- simFA(Model = list(NFac = factor_num, NItemPerFac = 15),
                  Loadings = list(FacLoadRange = .4,
                                  FacLoadDist = "fixed"))
      
      error_mod <- noisemaker(mod = m1, method = "TKL", target_rmsea = 0.09)
      
      Omega <- m1$Rpop
      Sigma <- error_mod$Sigma
      p <- nrow(Sigma)
      Ft <- log(det(Omega)) - log(det(Sigma)) + sum(diag(Sigma %*% solve(Omega))) - p
      Fb <- -log(det(Sigma))
      
      c(factors = factor_num, 
        Ft = Ft, 
        Fb = Fb, 
        cfi = cfi(Sigma, Omega), 
        rmsea = rmsea(Sigma, Omega, k = factor_num))
    }
  )
  
  out <- out %>% 
    bind_rows() %>% 
    mutate(target = .1 * Fb)
  
  out <- out %>% 
    pivot_longer(-factors, names_to = "func_type")
  
  labels <- list("Ft" = TeX("$\\mathit{F}_t$"),
                 "Fb" = TeX("$\\mathit{F}_b$"),
                 "target" = TeX("Target $\\mathit{F}_t$ for CFI=.90"))
  
  p1 <- out %>% 
    filter(func_type %in% c("Fb", "Ft", "target")) %>% 
    ggplot(aes(x = factors, y = value, color = func_type, linetype = func_type)) + 
    geom_point(size = 1) +
    geom_line() +
    scale_x_continuous(breaks = c(2,4,6,8,10)) +
    scale_color_brewer(palette = "Dark2", type = "qual", labels = labels) +
    scale_linetype_discrete(labels = labels) +
    guides(color = guide_legend(title = ""), 
           linetype = guide_legend(title = "")) +
    labs(x = "Factors", y = "Minimized Discrepancy Function Value", 
         color = "Model", linetype = "Model") +
    theme_bw() +
    theme(legend.position = "bottom")
  
  p2 <- out %>% 
    filter(func_type == "cfi") %>% 
    ggplot(aes(x = factors, y = value)) + 
    geom_point(size = 1) +
    geom_line() +
    scale_x_continuous(breaks = c(2,4,6,8,10)) +
    labs(y = "CFI", x = "Factors") +
    theme_bw()
  
  p3 <- out %>% 
    filter(func_type == "rmsea") %>% 
    ggplot(aes(x = factors, y = value)) + 
    geom_point(size = 1) +
    geom_line() +
    scale_x_continuous(breaks = c(2,4,6,8,10)) +
    scale_y_continuous(limits = c(0, 0.1)) +
    labs(y = "RMSEA", x = "Factors") +
    theme_bw()
  
  design = "12\n13\n44"
  cfi_factors_plot <- p1 + p2 + p3 + 
    guide_area() + 
    plot_layout(design = design, guides = "collect", heights = c(4,4,1)) +
    plot_annotation(tag_levels = "A") &
    theme(plot.tag = element_text(size = 9))
  
  ggsave(filename = here("img/cfi_factors_plot.png"),
         plot = cfi_factors_plot,
         dpi = 320,
         height = 4.5,
         width = 7)
}

knitr::include_graphics(here("img/cfi_factors_plot.png"))
```

The trade-off between CFI and RMSEA can also be seen by examining the differences between the $\mathbf{\Sigma}$ matrices produced by each of the model-error methods for a condition that often led to conflicting RMSEA and CFI values. \@ref(fig:c129-heat-maps) shows heat-maps of the $\mathbf{\Sigma}$ matrices for each of the five model-error methods. The $\bSigma$ matrices corresponded to the condition with ten orthogonal major common factors, five items per factor, weak factor loadings of 0.3, and Poor model fit.[^more-sigma-heat-maps] The figure shows clear differences between the model-error methods that incorporated a target CFI value ($\TKLcfi$ and $\TKLrmseacfi$) and the remaining model-error methods that only incorporated a target RMSEA value. The $\bSigma$ matrices produced by the $\TKLcfi$ and $\TKLrmseacfi$ methods included relatively small amounts of model error (in terms of the differences between the elements of $\bSigma$ and $\bOmega$), as can be seen by looking at their heat-map representations in \@ref(fig:c129-heat-maps). The block-diagonal structure of $\bOmega$ was well-preserved in both of the $\bSigma$ matrices produced by the $\TKLrmseacfi$ and $\TKLcfi$ model-error methods, with no large off-block-diagonal correlations. On the other hand, the $\bSigma$ matrices produced by the other model-error methods had many off-block-diagonal correlations that were relatively large, many as large (or larger) than the block-diagonal correlations. The figure thus reaffirms the previously reported result that the model-error methods that included a target CFI value led to solutions that perturbed $\bOmega$ less than solutions from model-error methods that did not include a target CFI value.

[^more-sigma-heat-maps]: Heat-maps of both the $\bSigma$ and $\hat{\bOmega}$ matrices for the five model-error methods and conditions with ten orthogonal factors, weak factor loadings, Poor model fit, and five or ten items per factor are shown in \@ref(sigma-heat-maps).

```{r c129-heat-maps, out.width = "85%"}
#| fig.cap = "Correlation matrices with model error ($\\bm{\\Sigma}$) for each model-error method from the condition with ten orthogonal factors, five items per factor, weak factor loadings of 0.3, and Poor model fit. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."
if (make_plots) {
  # Function to make heatmap plots
  matrix_plot <- function(R, title="", fill_limits = "auto") {
    
    if (length(fill_limits) == 1) {
      if (fill_limits == "auto") {
        fill_limits = NULL
      }
    }
    
    R <- R %>%
      as_tibble(rownames = NA) %>%
      rownames_to_column(var = "Var1") %>%
      pivot_longer(cols = !matches("Var1"), 
                   names_to = "Var2", values_to = "value") %>%
      mutate(value = case_when(Var1 == Var2 ~ NA_real_,
                               TRUE ~ value),
             Var1 = paste0("V", formatC(as.numeric(str_extract(Var1, "[0-9]+")),
                                        width = 3,
                                        flag = "0")),
             Var2 = paste0("V", formatC(as.numeric(str_extract(Var2, "[0-9]+")),
                                        width = 3,
                                        flag = "0"))) %>%
      mutate(Var2 = fct_rev(as.factor(Var2)))
    
    ggplot(R, aes(x = Var1, y = Var2, fill = value)) +
      geom_tile() +
      colorspace::scale_fill_continuous_diverging("Purple-Green", 
                                                  limits = fill_limits) +
      # scale_fill_viridis_c(limits = fill_limits) +
      labs(title = title, fill = "Correlation") +
      theme(axis.text = element_blank(),
            axis.ticks = element_blank(), 
            axis.title = element_blank()) +
      coord_equal()
  }
  
  # Load RpopME matrices
  RpopME_matrices <- readRDS(here("../model-error-simulation/misc/RpopME_matrices.RDS"))
  
  mod <- simFA(Model = list(NFac = 10, NItemPerFac = 5, Model = "orthogonal"),
               Loadings = list(FacLoadRange = .4, FacLoadDist = "fixed"))
  
  out_129 <- RpopME_matrices$C129
  out_129$Rpop <- mod$Rpop
  
  # Compute estimated population matrices
  Rtheta_list <- purrrgress::pro_map(out_129[1:5], .f = function(R) {
    fout_129 <- factanal(covmat = R, 
                         factors = 10, 
                         rotation = "none", 
                         n.obs = 1000, 
                         control = list(nstart = 100))
    Fhat <- fout_129$loadings
    Rhat <- Fhat %*% t(Fhat)
    diag(Rhat) <- 1
    Rhat
  })
  
  p1 <- matrix_plot(out_129$RpopME_tkl_rmsea, 
                    title = latex2exp::TeX("$TKL_{RMSEA}$"),
                    fill_limits = c(-.3, .3))
  p2 <- matrix_plot(out_129$RpopME_tkl_cfi, 
                    title = latex2exp::TeX("$TKL_{CFI}$"),
                    fill_limits = c(-.3, .3))
  p3 <- matrix_plot(out_129$RpopME_tkl_rmsea_cfi, 
                    title = latex2exp::TeX("$TKL_{RMSEA/CFI}$"),
                    fill_limits = c(-.3, .3))
  p4 <- matrix_plot(out_129$RpopME_wb, 
                    title = "CB",
                    fill_limits = c(-.3, .3))
  p5 <- matrix_plot(out_129$RpopME_wb, 
                    title = "WB",
                    fill_limits = c(-.3, .3))
  
  c129_plot <- (p1 + p2 + p3 + p4 + p5) + 
    plot_layout(guides = "collect", nrow = 2) +
    plot_annotation()
  
  ggsave(filename = here("img/c129_R_plot_Sigma_only.png"),
         plot = c129_plot,
         dpi = 320,
         height = 5, width = 7.25)
}

knitr::include_graphics(here("img/c129_R_plot_Sigma_only.png"))
```

## Fit Indices Indicating Lack-of-Fit Between $\bSigma$ and $\bOmegaHat$

Although the lack-of-fit between $\bSigma$ and $\bOmega$ was of primary interest in this dissertation, the lack-of-fit between $\bSigma$ and $\bOmegaHat$ might also be of interest to some researchers. Recall that $\bOmegaHat$ denotes the implied population correlation matrix obtained by analyzing $\bSigma$ using the major-factor model. Thus, the fit index values based on $\bSigma$ and $\bOmegaHat$ represent the "best-case scenario" fit index values a researcher could expect to obtain because the lack-of-fit is due only to model approximation error without any sampling error.

Figures reporting the distributions of RMSEA, CFI, TLI, and CRMR values representing the lack-of-fit between $\bSigma$ and $\bOmegaHat$ (analogous to \@ref(fig:rmsea-distributions), \@ref(fig:cfi-distributions), \@ref(fig:tli-distributions), and \@ref(fig:crmr-distributions)) are provided in \@ref(sigma-hat-fit-indices). In addition to the distributions of the $\bOmegaHat$ fit indices, I was also interested in how they related to the corresponding fit indices indicating lack-of-fit between $\bSigma$ and $\bOmega$. In particular, I was interested in determining how $\textrm{RMSEA}_{\bOmegaHat}$ was related to RMSEA and how $\textrm{CFI}_{\bOmegaHat}$ was related to CFI.

### Differences between RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$

To better understand the relationship between the observed $\textrm{RMSEA}_{\bOmegaHat}$ and RMSEA values from the simulation study, \@ref(fig:rmsea-theta-vs-theta-hat) shows box-plots of the differences between $\textrm{RMSEA}_{\bOmegaHat}$ and RMSEA, denoted as 

\begin{equation}
\textrm{RMSEA}_{\Delta} =  \textrm{RMSEA} - \textrm{RMSEA}_{\bOmegaHat},
\end{equation}

\noindent conditioned on each of the model-error methods, factor loading strength, and level of model fit. These variables had the largest effect on $\RMSEA_{\Delta}$, as indicated by the effect sizes reported in the ANOVA summary table in \@ref(rmsea-delta-anova). \@ref(fig:rmsea-theta-vs-theta-hat) shows that the $\textrm{RMSEA}_{\bOmegaHat}$ values were almost always lower than the RMSEA values corresponding to the same $\bSigma$ matrix, resulting in positive values of $\textrm{RMSEA}_{\Delta}$. The vast majority of these differences were quite small, with a median difference of only .003. However, \@ref(fig:rmsea-theta-vs-theta-hat) shows that $\textrm{RMSEA}_{\Delta}$ was affected by the level of model fit such that $\textrm{RMSEA}_{\Delta}$ was largest when model fit was Poor and smallest when model fit was Very Good. In the most extreme instance, a $\bSigma$ matrix with Poor model fit had an $\textrm{RMSEA}_{\bOmegaHat}$ value of .006 and an RMSEA value of .308, indicating very different qualitative interpretations of model fit. Thus, although the differences between RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$ were often negligible, in some cases they were large enough to indicate completely different qualitative interpretations of model fit for RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$.

```{r rmsea-theta-vs-theta-hat, out.width = '100%'}
#| fig.cap = "Box-plots of the differences between RMSEA and $\\textrm{RMSEA}_{\\hat{\\Omega}}$ (denoted as $\\textrm{RMSEA}_{\\Delta}$) conditioned on model-error method, levels of model fit, and factor loading. Note that some outliers were omitted from the plot to aid visualization. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  rmsea_diff <- results_matrix %>%  
    filter(rmsea < 3) %>% 
    mutate(model_fit_rec = str_replace_all(model_fit_rec, " ", "~"),
           model_fit_rec = fct_relevel(model_fit_rec,
                                       "Fit:~Very~Good",
                                       "Fit:~Fair",
                                       "Fit:~Poor")) %>% 
    mutate(error_method_rec = fct_rev(error_method_rec)) %>% 
    ggplot(aes(x = rmsea - rmsea_thetahat, y = error_method_rec)) +
    geom_boxplot(width = .4, outlier.size = .4, outlier.alpha = .05, size = .4) +
    scale_x_continuous(limits = c(0, .075)) +
    scale_y_discrete(labels = scales::label_parse()) +
    labs(x = latex2exp::TeX("$RMSEA_{\\Delta}$"),
         y = "") +
    facet_grid(model_fit_rec ~ loading_rec,
               labeller = label_parsed) +
    theme_bw()
  
  ggsave(here("img/rmsea_diff.png"),
         rmsea_diff,
         dpi = 320,
         height = 5)
}

knitr::include_graphics(here("img/rmsea_diff.png"))
```

\@ref(fig:rmsea-theta-vs-theta-hat) also shows that the relationship between RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$ differed depending on the model-error method that was used. For instance, the CB method consistently produced $\bSigma$ matrices with small $\textrm{RMSEA}_{\Delta}$ values, whereas the other model-error methods often produced solutions with large $\textrm{RMSEA}_{\Delta}$ values. In theory, the CB method should have led to solutions with equal RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$ values because $\bOmegaHat = \bOmega$ when the CB method converges to a valid solution (see \@ref(population-model-fit)). However, in the present simulation study the CB method produced RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$ values that were often very close but not exactly equal (corresponding to small but non-zero $\textrm{RMSEA}_{\Delta}$ values). The $\textrm{RMSEA}_{\Delta}$ values for the CB method were smallest when model fit was Very Good and largest when model fit was Poor, as can be seen by moving from top to bottom of \@ref(fig:rmsea-theta-vs-theta-hat). Although these $\textrm{RMSEA}_{\Delta}$ values were often non-zero, they were generally small enough to be of little concern to most researchers. It is likely that these discrepancies were due to the CB method converging to local minima, which Cudeck and Browne [-@cudeck1992] acknowledge is possible when the target RMSEA value is large.

In addition to being affected by choice of model-error method, $\textrm{RMSEA}_{\Delta}$ was affected by the strength of the major common factor loadings. Moving across \@ref(fig:rmsea-theta-vs-theta-hat) from left to right, the differences between RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$ values tended to decrease as factor loadings increased from 0.3 to 0.8. The figure also shows that there was an interaction between model-error method and factor loading such that $\textrm{RMSEA}_{\Delta}$ decreased the most for the $\TKLrmsea$ and CB methods as factor loadings increased, compared to the alternative model-error methods.

The number of major common factors and the number of items per factor also had effects on $\textrm{RMSEA}_{\Delta}$. These effects are shown in \@ref(fig:rmsea-theta-vs-theta-hat-factors), which contains box-plots of $\textrm{RMSEA}_{\Delta}$ conditioned on number of factors and number of items per factor. The figure shows that the TKL-based methods were most affected by the number of factors and the number of items per factor, but that the direction of the effect depended on the particular method. For instance, the $\TKLrmsea$ method led to solutions with median $\textrm{RMSEA}_{\Delta}$ values that tended to increase along with the number of major factors when there were 15 items per factor. For conditions with only five items per factor, the median $\textrm{RMSEA}_{\Delta}$ first decreased as the number of factors increased from one to three, and then increased slightly as the number of factors increased further. In contrast to $\TKLrmsea$ method, the $\TKLrmseacfi$ and $\TKLcfi$ methods led to median $\textrm{RMSEA}_{\Delta}$ values that decreased as the number of factors increased for conditions with five items per factor. For conditions with 15 items per factor, the median $\textrm{RMSEA}_{\Delta}$ remained stable as the number of factors increased. For the CB and WB model-error methods, the number of factors had little effect on median $\textrm{RMSEA}_{\Delta}$. Finally, all of the model-error methods produced solutions with smaller median $\textrm{RMSEA}_{\Delta}$ values as the number of items per factor increased, with the exception of the $\TKLrmsea$ method for conditions with five or ten factors.

```{r rmsea-theta-vs-theta-hat-factors, out.width = '100%'}
#| fig.cap = "Box-plots of the differences between RMSEA and $\\textrm{RMSEA}_{\\hat{\\Omega}}$ (denoted as $\\textrm{RMSEA}_{\\Delta}$) conditioned on model-error method, number of factors, and number of items per factor. Note that some outliers were omitted from the plot to aid visualization. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  rmsea_diff_factors <- results_matrix %>%  
    filter(rmsea < 3) %>% 
    mutate(model_fit_rec = str_replace_all(model_fit_rec, " ", "~"),
           model_fit_rec = fct_relevel(model_fit_rec,
                                       "Fit:~Very~Good",
                                       "Fit:~Fair",
                                       "Fit:~Poor")) %>% 
    mutate(error_method_rec = fct_rev(error_method_rec)) %>% 
    ggplot(aes(x = rmsea - rmsea_thetahat, y = error_method_rec)) +
    geom_boxplot(width = .4, outlier.size = .4, outlier.alpha = .05, size = .4) +
    scale_x_continuous(limits = c(0, .075)) +
    scale_y_discrete(labels = scales::label_parse()) +
    labs(x = latex2exp::TeX("$RMSEA_{\\Delta}$"),
         y = "") +
    facet_grid(items_per_factor_rec ~ factors_rec,
               labeller = label_parsed) +
    theme_bw()
  
    ggsave(here("img/rmsea_diff_factors.png"),
         rmsea_diff_factors,
         dpi = 320,
         height = 3.5,
         width = 7)
}

knitr::include_graphics(here("img/rmsea_diff_factors.png"))
```

### Differences between CFI and $\textrm{CFI}_{\bOmegaHat}$

As with RMSEA and $\textrm{RMSEA}_{\bOmegaHat}$, the difference between CFI and $\textrm{CFI}_{\bOmegaHat}$ was denoted as

\begin{equation}
\textrm{CFI}_{\Delta} =  \textrm{CFI} - \textrm{CFI}_{\bOmegaHat}.
\end{equation}

```{r calculate-error-method-medians-cfi-delta, echo = FALSE, include = FALSE}
cfi_method_medians <- results_matrix %>% 
  group_by(error_method) %>% 
  summarise(median_cfi_delta = median(cfi - cfi_thetahat, na.rm = TRUE)) %>% 
  pivot_wider(names_from = error_method, values_from = median_cfi_delta)

results_matrix %>% 
  group_by(error_method, factors, items_per_factor) %>% 
  summarise(median_cfi_delta = median(cfi - cfi_thetahat, na.rm = TRUE)) %>% 
  pivot_wider(names_from = error_method, values_from = median_cfi_delta) %>% 
  apa_table(col.names = c("Factors", "Items/Factor", 
                          "$\\textrm{TKL}_{\\textrm{RMSEA}}$",
                          "$\\textrm{TKL}_{\\textrm{RMSEA/CFI}}$",
                          "$\\textrm{TKL}_{\\textrm{CFI}}$",
                          "CB",
                          "WB"),
            digits = c(0, 0, 2, 2, 2, 2, 2),
            align = "rrrrrrr",
            format.args = list(na_string = ""),
            col_spanners = list("Model-Error Method" = 3:7),
            caption = "A caption.")
```

\noindent \@ref(fig:cfi-theta-vs-theta-hat) contains box-plots of $\textrm{CFI}_{\Delta}$ conditioned on model-error method, level of model fit, and factor loading strength. Similar to the previous results for $\textrm{RMSEA}_{\Delta}$, the figure shows that $\textrm{CFI}_{\Delta}$ values were largest when model fit was Poor and smallest when model fit was Very Good. Moreover, the figure also shows differences among the model-error methods indices. In general, the $\TKLrmsea$ and WB methods led to the most extreme $\textrm{CFI}_{\Delta}$ values[^extreme-cfi-delta-value], whereas the $\TKLrmseacfi$, $\TKLcfi$, and CB methods often led to less extreme values, particularly in conditions with weak factor loadings or Poor model fit. In fact, the $\TKLrmseacfi$ and $\TKLcfi$ model-error methods sometimes led to $\textrm{CFI}_{\Delta}$ values that were less extreme than those from the CB method. As discussed in the previous section, this was somewhat surprising because the CB method was expected to produce solutions such that $\bOmega = \bOmegaHat$ and $\textrm{CFI}_{\Delta} = 0$ and suggests that the CB method often converged to local minima in conditions with weak factor loadings and Poor or Fair model fit.

[^extreme-cfi-delta-value]: For instance, the most extreme value, $\textrm{CFI}_{\Delta} = .818$, was obtained using the $\TKLrmsea$ model-error method and a model with ten factors (correlated .3 with one another), 15 items per factor, weak factor loadings, and Poor model fit.

```{r cfi-theta-vs-theta-hat, out.width = '100%'}
#| fig.cap = "Box-plots of the differences between CFI and $\\textrm{CFI}_{\\hat{\\bm{\\Omega}}}$ (denoted as $\\textrm{CFI}_{\\Delta}$) conditioned on model-error method, levels of model fit, and factor loading. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  cfi_diff <- results_matrix %>%  
    filter(rmsea < 3) %>% 
    mutate(model_fit_rec = str_replace_all(model_fit_rec, " ", "~"),
           model_fit_rec = fct_relevel(model_fit_rec,
                                       "Fit:~Very~Good",
                                       "Fit:~Fair",
                                       "Fit:~Poor")) %>% 
    mutate(error_method_rec = fct_rev(error_method_rec)) %>% 
    ggplot(aes(x = cfi - cfi_thetahat, y = error_method_rec)) +
    geom_boxplot(width = .4, outlier.size = .4, outlier.alpha = .05, size = .4) +
    # scale_x_continuous(limits = c(0, .075)) +
    scale_y_discrete(labels = scales::label_parse()) +
    labs(x = latex2exp::TeX("$CFI_{\\Delta}$"),
         y = "") +
    facet_grid(model_fit_rec ~ loading_rec,
               labeller = label_parsed) +
    theme_bw()
  
  ggsave(here("img/cfi_diff.png"),
         cfi_diff,
         dpi = 320,
         height = 5)
}

knitr::include_graphics(here("img/cfi_diff.png"))
```

In addition to the effects of factor loading and model fit, $\textrm{CFI}_{\Delta}$ was also affected by the number of factors and items per factor. To help understand these effects, \@ref(fig:cfi-theta-vs-theta-hat-nfac) shows box-plots of $\textrm{CFI}_{\Delta}$ conditioned on model-error method, number of factors, and number of items per factor. The figure shows that $\textrm{CFI}_{\Delta}$ tended to increase as the number of factors increased for the $\TKLrmsea$ and WB methods, but that the number of factors had only a small effect (or no effect at all) on $\cfiDelta$ for the other model-error methods. Considering the effect of number of items per factor, \@ref(fig:cfi-theta-vs-theta-hat-nfac) shows that the number of items per factor had almost no effect on the median $\cfiDelta$ values for each of the model-error methods except for the $\TKLrmsea$ method in conditions with ten factors. However, there was more variation in $\cfiDelta$ values for the $\TKLrmsea$ method in conditions with 15 items per factor compared to conditions with five items per factor. In contrast, the CB and WB methods led to less variable $\cfiDelta$ values as the number of items per factor increased from five to 15.

```{r cfi-theta-vs-theta-hat-nfac, out.width = '100%'}
#| fig.cap = "Box-plots of the differences between CFI and $\\textrm{CFI}_{\\hat{\\Omega}}$ (denoted as $\\textrm{CFI}_{\\Delta}$) conditioned on model-error method, number of factors, and number of items per factor. Note that some outliers were omitted from the plot to aid visualization. TKL = Tucker, Koopman, and Linn; CB = Cudeck and Browne; WB = Wu and Browne."

if (make_plots) {
  cfi_diff_factors <- results_matrix %>%  
    filter(rmsea < 3) %>% 
    mutate(model_fit_rec = str_replace_all(model_fit_rec, " ", "~"),
           model_fit_rec = fct_relevel(model_fit_rec,
                                       "Fit:~Poor",
                                       "Fit:~Fair",
                                       "Fit:~Very~Good")) %>% 
    mutate(error_method_rec = fct_rev(error_method_rec)) %>% 
    ggplot(aes(x = cfi - cfi_thetahat, y = error_method_rec)) +
    geom_boxplot(width = .4, outlier.size = .4, outlier.alpha = .05, size = .4) +
    scale_y_discrete(labels = scales::label_parse()) +
    labs(x = latex2exp::TeX("$CFI_{\\Delta}$"),
         y = "") +
    facet_grid(items_per_factor_rec ~ factors_rec,
               labeller = label_parsed) +
    theme_bw()
  
    ggsave(here("img/cfi_diff_factors.png"),
         cfi_diff_factors,
         dpi = 320,
         height = 3.5,
         width = 7)
}

knitr::include_graphics(here("img/cfi_diff_factors.png"))
```

Overall, the results of the present simulation study suggested that any of the model-error methods evaluated in this simulation study could provide a useful starting point for researchers who would like to generate $\bSigma$ matrices with particular $\RMSEA_{\bOmegaHat}$ or $\CFI_{\bOmegaHat}$ values. However, all of the model-error methods (including the CB methods) sometimes produced solutions with large $\rmseaDelta$ or $\cfiDelta$ values, particularly in conditions with Poor model fit and weak factor loadings. Thus, researchers interested in generating solutions with particular $\textrm{RMSEA}_{\bOmegaHat}$ or $\textrm{CFI}_{\bOmegaHat}$ values should check the fit indices of each solution and reject solutions with fit indices outside of the desired range. Because the fit indices based on $\bOmegaHat$ almost always indicated better fit than the indices based on $\bOmega$, it might also be useful to specify target RMSEA values that are slightly higher (or CFI values that are slightly lower) than the desired $\textrm{RMSEA}_{\bOmegaHat}$ or $\textrm{CFI}_{\bOmegaHat}$ values.

## Model Fit Index Recovery for the TKL-Based Methods

The TKL-based model-error methods were designed to find a $\bSigma$ matrix that had either an RMSEA value or a CFI value (or both) that were close to a specified value. When only one target model-fit index was used (i.e., $\TKLrmsea$ or $\TKLcfi$), the observed RMSEA and CFI values were very close to the target values. However, when both RMSEA and CFI fit index targets were used simultaneously with the $\TKLrmseacfi$ method, many solutions failed to have RMSEA and CFI values that were both very close to the target values. This could indicate that the optimization procedure was not working well and often failed to find an optimal solution. However, an alternative explanation is that some combinations of RMSEA and CFI might not have been possible for certain conditions (e.g., with major factor loadings fixed at a particular value, or with a certain number of major common factors).

To determine whether the $\TKLrmseacfi$ was able to find near-optimal solutions, I first needed to find combinations of RMSEA and CFI values that were known to be possible. If the $\TKLrmseacfi$ method was able to produce solutions with RMSEA and CFI values that were close to these target RMSEA and CFI values, it would suggest that the $\TKLrmseacfi$ was working well. More importantly, if the $\TKLrmseacfi$ method was not able to produce solutions with fit indices close to these known-to-be-possible combinations of RMSEA and CFI, it would suggest that researchers cannot rely upon the $\TKLrmseacfi$ model-error method to produce optimal or near-optimal solutions. 

To find RMSEA and CFI value combinations that were known to be possible, I used the standard TKL method implemented in the `simFA()` function to generate a correlation matrix with model error for every condition in the simulation design. Next, I computed the RMSEA and CFI values for each simulated correlation matrix. I then used those values as target RMSEA and CFI values for the $\TKLrmseacfi$ method and generated 50 correlation matrices with model error for each condition .[^fit-recovery-code] 

[^fit-recovery-code]: Code for this simulation study is provided in \@ref(check-model-fit-recovery-code).

The results from this small simulation study are reported in \@ref(fig:fit-index-recovery), which shows the known-to-be-possible target values of RMSEA and CFI (indicated by solid black lines) and the observed RMSEA and CFI values from the $\TKLrmseacfi$ method for each condition. The figure shows that the observed RMSEA and CFI values were nearly identical to the target values for most conditions. The conditions where the observed fit indices had the most variability were conditions with one or three factors, five items, and weak factor loadings. However, even in those conditions many solutions had CFI and RMSEA values that were nearly identical to the target values. These results indicate that using the $\TKLrmseacfi$ method repeatedly with different initial values of $\mathbf{W}$,  $\epsilon$, and $\nu_{\textrm{e}}$ and then selecting the solution with RMSEA and CFI values closest to the target values seems to be an effective approach for obtaining optimal (or nearly-optimal) solutions.

```{r fit-index-recovery}
#| fig.cap = "Observed RMSEA and CFI values for the $\\textrm{TKL}_{\\textrm{RMSEA/CFI}}$ model-error method and the corresponding known-to-be-possible target RMSEA and CFI values (indicated by the black lines), conditioned on number of factors, number of items per factor, factor loading strength, and factor correlation. Note that some levels were omitted to conserve space; the full figure is shown in \\@ref(fit-index-recovery-full)."
if (make_plots) {
  noisy_data <- readRDS(here("data/noisy_data.RDS"))
  
  target_values <- noisy_data %>%
    filter(factor_corr != "Factor Cor.: 0.3",
           factor_loading != "Loading: 0.6") %>%
    select(factors, items_per_factor, factor_corr, factor_loading,
           target_rmsea, target_cfi) %>%
    distinct()
  
  noisy_data %>%
    filter(factor_corr != "Factor Cor.: 0.3",
           factor_loading != "Loading: 0.6") %>% 
    ggplot(aes(x = rmsea, y = cfi)) +
    geom_point(alpha = .3, size = 1) +
    geom_hline(aes(yintercept = target_cfi), data = target_values,
               size = .25) +
    geom_vline(aes(xintercept = target_rmsea), data = target_values,
               size = .25) +
    facet_grid(factors * factor_loading ~ items_per_factor * factor_corr) +
    labs(x = "RMSEA", y = "CFI") +
    theme_bw()
  
  ggsave(filename = "rmsea_cfi_recovery.png",
         path = here("img"),
         plot = last_plot(),
         dpi = 320,
         height = 10,
         width = 7)
}

knitr::include_graphics(here("img/rmsea_cfi_recovery.png"))
```

<!--chapter:end:chapters/04-chap4.Rmd-->


# Discussion {#discussion}

In this dissertation, I conducted a large-scale simulation study to compare several methods for simulating population correlation matrices with model error. The model-error methods I compared were the Cudeck and Browne [CB\; -@cudeck1992] method, the Wu and Browne [WB\; -@wu2015] method, and three variations of the Tucker, Koopman, and Linn [TKL\; -@tucker1969] model-error method using a novel optimization procedure to automatically select values of the TKL method parameters, $\epsilon$ and $\nu_{\textrm{e}}$. The addition of the optimization procedure allowed the TKL method to be used with specified target values of RMSEA (the $\textrm{TKL}_{\textrm{RMSEA}}$ variation), CFI (the $\textrm{TKL}_{\textrm{CFI}}$ variation), or both fit indices simultaneously (the $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ variation). Moreover, the optimization procedure also allowed users to impose constraints on the loadings of the minor common factors introduced by the TKL method to ensure that there was a clear delineation between major and minor common factors. To facilitate the use of all of the model-error methods discussed in this dissertation, I also developed an R package (*noisemaker*) that serves as an easy-to-use, unified interface for simulating correlation matrices with model error.

Through this simulation study, I hoped to answer two primary questions about the model-error methods I investigated. First, I wanted to know whether the five model-error methods included in the study (the $\textrm{TKL}_{\textrm{RMSEA}}$, $\textrm{TKL}_{\textrm{CFI}}$, $\textrm{TKL}_{\textrm{RMSEA/CFI}}$, CB, and WB methods) led to different values of the CFI, TLI, and CRMR fit indices when used with the same error-free population correlation matrices and target RMSEA values. If all of the model-error methods led to the same (or similar) fit index values, it would have suggested that the choice of which model-error method to use is not very important when conducting simulation studies involving covariance structure models. The second question I wanted to answer was related to the efficacy of the modified TKL method with the proposed optimization procedure (referred to as the multiple-target TKL method). That is, I was interested in determining how well the multiple-target TKL method was able to generate correlation matrices with model error that had RMSEA and CFI values that were close to the specified values. Note that in the following discussion of the simulation results I focus on the RMSEA and CFI fit indices for two reasons. First, RMSEA and CFI target values were used in the simulation study and are often used as indications of model fit when generating population correlation matrices with model error [@tucker1969; @cudeck1992; @kracht2022; @trichtinger2020]. Second, the CRMR and TLI indices led to results that were similar to the results for RMSEA and CFI, respectively.

Concerning the first primary research question, the results indicated that there were important differences between the five model-error methods in terms of the observed model fit indices they led to. Although all of the model-error methods led to similar RMSEA, CFI, TLI, and CRMR values in conditions with few major factors, strong factor loadings, and good model fit, they led to much more disparate results in other conditions. In particular, the results of the simulation study indicated that there were important differences between model-error methods that incorporated only a target RMSEA value (i.e., the $\textrm{TKL}_{\textrm{RMSEA}}$, CB, and WB methods) and methods that incorporated a target CFI value (the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods). 

When evaluated on RMSEA, the model-error methods that only incorporated target RMSEA values generally produced solutions with observed RMSEA values very close to the target values. In particular, the $\textrm{TKL}_{\textrm{RMSEA}}$ and CB methods led to RMSEA values that were almost always extremely close to the target values, whereas there was slightly more variability in the RMSEA values from the WB method. On the other hand, the model-error methods that incorporated CFI target values often led to RMSEA values that were lower than the target values, particularly in conditions with many major common factors and weak factor loadings. The result that the $\textrm{TKL}_{\textrm{RMSEA}}$, CB, and WB model-error methods led to solutions with RMSEA values that were close to the target values is perhaps unsurprising, given that this was what all three methods were designed to do. However, it confirms that the CB method worked as expected and provides evidence that the  modifications of the TKL and WB methods to incorporate target RMSEA values were successful. On the other hand, the $\TKLrmsea$ and $\TKLrmseacfi$ methods both led to solutions with RMSEA values that were further from the target RMSEA values in most conditions, particularly when model fit was Poor or when there were many major common factors.

When evaluated on CFI, the two model-error methods that incorporated target CFI values ($\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$) generally produced observed CFI values that were closer to the target values than the other model error methods. Although all of the model-error methods led to observed CFI values that were close the target values in conditions with strong factor loadings and Very Good model fit, the $\textrm{TKL}_{\textrm{RMSEA}}$, CB, and WB methods often led to unacceptably low CFI values in other conditions. Interestingly, the $\TKLcfi$ and $\TKLrmseacfi$ methods often led to quite similar results, despite the $\TKLrmseacfi$ method incorporating a target RMSEA value. This seemed to be because CFI was more sensitive to changes in parameter values compared to RMSEA, particularly in conditions with many factors, many items per factor, and low factor loadings. A small change in parameter values that produced an RMSEA value slightly closer to the target value often resulted in a large change in CFI away from the target value. Correspondingly, small changes in parameter values that produced a CFI value closer to the target value generally had only a small effect on the RMSEA value. As a result, the $\TKLrmseacfi$ method tended to "prioritize" target CFI values, despite both RMSEA and CFI being weighted equally in the objective function.

The result that the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods led to RMSEA values that were often far from the target suggested a problem with using only target RMSEA values to generate correlation matrices with model error. Namely, the problem was that solutions with RMSEA values that were close to target RMSEA values often had CFI values that indicated a worse qualitative level of model fit. For instance, in conditions with five factors, weak factor loadings, and Very Good (target) model fit, the $\textrm{TKL}_{\textrm{RMSEA}}$ method produced solutions with RMSEA values that were very close to the target RMSEA value of 0.025. However, none of corresponding CFI values for those solutions reached the target value of .99 (considered to represent Very Good model fit), and many CFI values were below .90 (a liberal threshold for acceptable model fit). These results indicate that using RMSEA alone to adjudicate model fit makes it not only possible, but *likely* that simulated population correlation matrices would be included in conditions with nominally excellent model fit as indicated by RMSEA values, but with unacceptably poor model fit as indicated by CFI values. These results agreed with results reported by Kracht and Waller [-@kracht2022], who used the TKL method with manually-selected parameter values to produce matrices with RMSEA values in a particular range. They found that although they were able to select parameter values that led to solutions with RMSEA values in the desired ranges, the CFI values for those solutions were often below the standard cutoff values. Furthermore, they reported that CFI values were lowest in conditions with many items, low factor loadings, and poor model fit.

The fact that all of the model error method often produced solutions with RMSEA and CFI values indicating different levels of qualitative model fit presents a problem for researchers who would like to generate population correlation matrices with fit indices indicating a particular degree of model fit. Choosing to use the $\textrm{TKL}_{\textrm{RMSEA}}$, CB, or WB methods would likely lead to correlation matrices with the desired RMSEA values for most conditions, but unacceptably low CFI values. On the other hand, choosing to use the $\textrm{TKL}_{\textrm{CFI}}$ or $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ would lead to almost the complete opposite problem; solutions would be likely to have observed CFI values near the target values, but would also be likely to have smaller-than-desired RMSEA values. 

To determine which of the model-error methods led to the highest rates of fit index agreement, I evaluated fit index agreement in two ways. First, I evaluated each model error method in terms of the sum of the absolute differences between the observed and target RMSEA and CFI values, defined as $D$ in \autoref{eq:distance-between-observed-and-target}. When evaluated on this criterion, the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods led to much better results than the other investigated model-error methods, having the lowest median $D$ values over all conditions. Moreover, the $\textrm{TKL}_{\textrm{CFI}}$ and $\textrm{TKL}_{\textrm{RMSEA/CFI}}$ methods often led to much lower $D$ values than the alternatives (particularly in conditions with many factors, weak factor loadings, and Poor model fit) and rarely led to higher $D$ values. The second way I evaluated fit-index agreement was by using rule-of-thumb RMSEA and CFI threshold values to categorize correlation matrices as having good, acceptable, or unacceptable model fit and then determining how often RMSEA and CFI values led to the same level of qualitative model fit for each model-error method. The results of the simulation study indicated that the $\TKLcfi$ method was the most likely to produce solutions with qualitative model fit agreement, followed by the CB, $\TKLrmseacfi$, $\TKLrmsea$, and WB methods. These results suggest that the $\TKLcfi$ model is the best choice (of the model-error methods considered here) for researchers who would like to generate correlation matrices with model error and who would like to ensure that the matrices they generate have RMSEA and CFI values that indicate the same level of qualitative model fit. However, it is also important to note that in many conditions (e.g., in conditions with many factors, Weak factor loadings, and Poor model fit), all of the model-error methods had qualitative fit agreement rates close to zero.

The result that the $\TKLcfi$ method led to both the smallest average $D$ values and the highest rate of qualitative fit agreement (using RMSEA and CFI) was quite surprising. Before conducting the simulation study, I predicted that the $\TKLrmseacfi$ method would lead to both the smallest $D$ values and the highest rates of qualitative fit agreement because it incorporated both target RMSEA and CFI values. However, the simulation study results indicated that CFI values tended to change more quickly as a function of RMSEA values than *vice versa*, particularly for conditions with many factors, many items per factor, and strong factor loadings. Put another way, changing the TKL parameter values to produce a solution with an RMSEA value incrementally closer to the target value often resulted in a large change to the CFI value. This provided a plausible explanation for why the $\TKLrmseacfi$ and $\TKLcfi$ methods often led to similar results. Namely, because CFI values were more sensitive to changes to the TKL parameters than RMSEA, the $\TKLrmseacfi$ solutions were primarily influenced by the CFI targets (and thus often produced results similar to the $\TKLcfi$ method) despite the CFI and RMSEA targets being weighted equally in the objective function. 

Based on the results of the simulation study, I recommend that researchers who want to generate population correlation matrices with a particular level of model fit (as indicated by RMSEA and CFI values) should use the $\TKLcfi$ model-error method. I also recommend the $\TKLrmseacfi$ method as an acceptable alternative. This method produced lower rates of qualitative fit agreement compared to the $\TKLcfi$ method, but very similar results in terms of $D$. Although the $\TKLcfi$ method led to the smallest $D$ values for the particular combinations of RMSEA and CFI target values included in this study, it is possible that the $\TKLrmseacfi$ method might lead to smaller $D$ values when different combinations of target RMSEA and CFI values are used. I recommend that researchers experiment with both options before committing to use either in a particular study. 

Although the CB method led to the second-highest rate of qualitative model fit agreement, it also often led to substantially higher $D$ values compared to the $\TKLcfi$ and $\TKLrmseacfi$ model-error methods. Moreover, it had several drawbacks that keep me from recommending it for use in simulation studies. First, the CB method was prohibitively slow whenever $\bOmega$ was large (i.e., whenever there were many factors or items per factor).  Using the CB method with 150-variable conditions was so time-consuming that I had to drop them from my simulation design. The fact that the CB method is time consuming when $\bOmega$ is large was all the more problematic because the CB method often produced indefinite $\bSigma$ matrices in those conditions. As noted in \@ref(indefinite-matrices), indefinite $\bSigma$ matrices are unacceptable candidates for population correlation matrices with model error because all correlation and covariance matrices are at least positive semi-definite by definition [@wothke1993; @lorenzo-seva2020a; @kracht2022]. Researchers hoping to obtain positive semi-definite $\bSigma$ matrices using the CB method with large $\bOmega$ matrices could simply generate a large number of solutions, rejecting indefinite $\bSigma$ matrices. However, given the completion time of the CB method and the high rates of indefinite solutions reported for many conditions of the simulation study, this is unlikely to be a feasible approach. Finally, it is unclear whether all of the desiderata of the CB method are, in fact, desirable. Specifically, it is not self-evident that the vector of population parameters should be perfectly recovered when the model is applied to $\bSigma$ using maximum likelihood (as it is in the CB method). Certainly, this constraint is not enforced by any of the alternative model-error methods. Even if a researcher finds the constraint reasonable and chooses to use the CB method as a result, the simulation results showed that the CB method often failed to find a solution such that $\bOmegaHat = \bOmega$ in some conditions.

Most of the issues associated with the CB method did not affect the $\TKLrmsea$ or WB model-error methods. For instance, neither method produced indefinite $\bSigma$ matrices and both methods had much shorter completion times compared with the CB method. However, I do not recommend either model error method for general use in simulation studies. Both the $\TKLrmsea$ and WB methods led to relatively high $D$ values and relatively low rates of qualitative model fit agreement compared to the alternative model-error methods. Additionally, the $\TKLrmsea$ method often led to solutions with strong minor factors that would more appropriately be considered major factors. Even the inclusion of a large penalty term ($\lambda$) did not prevent the $\TKLrmsea$ method from producing solutions that violated the constraint that no minor factor should have more than two absolute loadings greater than 0.3. On the other hand, the $\TKLcfi$ or $\TKLrmseacfi$ methods almost never led to solutions that violated the minor factor constraints. In fact, the $\TKLcfi$ and $\TKLrmseacfi$ methods did not produce solutions that violated the minor factor constraints even when $\lambda = 0$ (i.e., when no penalty was applied). Thus, the study results indicated that the inclusion of a reasonable target CFI value was almost always sufficient to avoid generating solutions with strong minor factors. Researchers who want to use the $\TKLrmsea$ method could discard solutions that violated the minor factor constraints and continue generating new solutions until a sufficient number of acceptable solutions were found. However, the study results indicated the $\TKLrmsea$ method almost always led to solutions that violated the minor factor constraints in conditions with many factors, many items per factor, and Poor model fit. Therefore, the strategy of discarding solutions that violated the minor factor constraints is likely to be highly inefficient (if not completely impractical) for those conditions.

## Limitations and Future Work

As with any study, I acknowledge that the simulation study reported in this dissertation was subject to certain limitations. For instance, I designed the study to include a wide range of models that might plausibly be encountered in psychological research. However, there were still many types of models that were not included in the study design. For instance, the study design only included models with equal numbers of salient items per factor, all factor correlations fixed at the same value, and all non-zero factor loadings fixed at the same value. Although these models were artificially simple, they were chosen because they made it easier to isolate the effects of each of the independent variables (e.g., number of factors, number of items per factor, factor loading, etc.) and because similar models have been used in previous Monte Carlo simulation studies [@kracht2022; @debelak2016; @debelak2013; @auerswald2019]. Nevertheless, future research should investigate how the results from the CB, WB, and TKL-based model-error methods differ when used with more complex models. The implementations of these model error methods in the *noisemaker* R package, along with the simulation code in \@ref(main-simulation) should facilitate this future work.

A second limitation of the present study was that it only investigated model error only in the context of factor analysis models and not covariance structure models more generally. The choice to focus on factor analysis models was largely motivated by the fact that the TKL method is specific to the factor analysis model. Unlike the CB and WB model-error methods, which can be used with any covariance structure model, the TKL method requires modification to be applied to covariance structure models other than the common factor model [e.g., @trichtinger2020]. Where possible, it would be useful to extend the TKL method (and the multiple-objective TKL method) to additional types of covariance structure models. It might also be useful to investigate whether automated procedures could be developed to incorporate target CFI values into the CB or WB methods. For instance, the target RMSEA value used in the CB and WB methods could be treated as a tuning parameter and optimized to find the value that leads to solutions with CFI values that are close to a user-specified target value. As with the multiple-objective TKL method, users could also specify how much weight to give each fit index. Although evaluating the CB method many times in an optimization loop is likely to be prohibitively time-consuming with large models, the procedure could work well for the WB method (or for the CB method when models are small). Future work should investigate the effectiveness of optimizing CB and WB target RMSEA values to incorporate CFI targets.

In addition to extending the simulation design and investigating ways to incorporate CFI targets in the CB and WB methods, future work should be done to build and improve flexible, robust, and easy-to-use implementations of model-error methods for use in Monte Carlo simulation studies. The *noisemaker* package (and the function of the same name) that was developed as a part of this dissertation provides a simple, unified interface for generating correlation matrices with model error using the new multiple-objective TKL method, the CB method, and WB method. Moreover, the *noisemaker* handles specification of the population model (without model error) using the `simFA()` function from the *fungible* package. Together, the *noisemaker* and *fungible* packages provide a powerful collection of functions for researchers who wish to simulate factor analysis models and data sets. 

Future work should continue to improve the *noisemaker* package by adding features and improving ease-of-use. For example, an update to the `noisemaker()` function has been planned that will let users to specify allowable ranges of $\epsilon$ and $\nu_{\textrm{e}}$ parameters when using the multiple-objective TKL method. Although it is difficult to know which precise values of $\epsilon$ and $\nu_\textrm{e}$ are reasonable for a particular model, it is sometimes possible to specify plausible ranges of these parameters. For instance, consider a population model with one major common factor and ten items, each with a factor loading of 0.8. The major common factor therefore accounts for 64% of the variance in each item. Setting $\nu_\textrm{e} = .75$ would then mean that 91% of the item variance would be accounted for by the (reliable) major and minor common factors. In some contexts, this might be reasonable; in many psychological contexts, it would be considered unreasonable. Similarly, a researcher might consider it unlikely that all of the minor factors should be equally strong. In that case, values of $\epsilon$ very close to zero would be considered unrealistic. To allow users to specify allowable ranges of $\epsilon$ and $\nu_\textrm{e}$ for the multiple-objective TKL method, those ranges could be used as the parameter boundary constraints in the L-BFGS-B algorithm instead of $\epsilon, \nu_\textrm{e} \in [0,1]$. This feature has already been implemented in a development version of *noisemaker*[^noisemaker-dev] that will be submitted to CRAN once testing has been completed. 

[^noisemaker-dev]: The most recent development version of the *noisemaker* package is available at [github.com/JustinKracht/noisemaker](https://www.github.com/JustinKracht/noisemaker), along with a vignette demonstrating how it can be used.

In conclusion, the work in this dissertation should provide a valuable resource for researchers who would like to incorporate model error into Monte Carlo simulation studies of factor analysis models. I have reported an overview of existing model-error methods and have proposed an extension of the TKL model-error method that allows researchers to specify target RMSEA values, target CFI values, or both simultaneous. By conducting an extensive simulation study, I showed that using the proposed multiple-target TKL method with target RMSEA and CFI values (or with only a target CFI value) often led to solutions with better quantitative and qualitative fit index agreement compared to the alternative model-error methods. Finally, I developed the R *noisemaker* package to make it easy for researchers to use any of the model-error methods investigated in this simulation study. 

<!--chapter:end:chapters/05-chap5.Rmd-->

# References {-}

\noindent
    <!-- To remove the indentation of the first entry.-->

\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}
    <!--To create a hanging indent and spacing between entries.  
    These three lines may need to be removed for styles that don't require the hanging indent. -->
    
<div id="refs"></div>

\setlength{\parindent}{0.20in}
\setlength{\leftskip}{0pt}
  <!-- To return to normal indentation and spacing for the appendices. -->

<!--chapter:end:chapters/90-references.Rmd-->

# Glossary of Mathematical Symbols {#symbol-glossary}

+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| Symbol                            | Definition                                                                                                                                                                                                                                                                                                                              |                                                         |
+===================================+=========================================================================================================================================================================================================================================================================================================================================+=========================================================+
| $\mathbf{\Omega}$                 | A $p\times p$ population covariance (correlation) matrix without model error.                                                                                                                                                                                                                                                           |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\boldsymbol{\gamma}$             | A vector of $h$ free model parameters.                                                                                                                                                                                                                                                                                                  |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\hat{\boldsymbol{\gamma}}$       | An estimated vector of free parameters.                                                                                                                                                                                                                                                                                                 |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\boldsymbol{\gamma}_0$           | A particular vector of free parameters such that $\boldsymbol{\Omega} = \boldsymbol{\Omega}(\boldsymbol{\gamma}_0)$ .                                                                                                                                                                                                                   |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\mathbf{E}$                      | A symmetric $p \times p$ matrix representing the effects of model error.                                                                                                                                                                                                                                                                |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\boldsymbol{\Sigma}$             | A $p \times p$ population covariance (correlation) matrix with model error such that $\boldsymbol{\Sigma} = \boldsymbol{\Omega} + \mathbf{E}$.                                                                                                                                                                                          |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\mathbf{S}$                      | A $p \times p$ sample covariance (correlation) matrix.                                                                                                                                                                                                                                                                                |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\hat{F}$                         | A minimum objective function value for a hypothesized model $\\hat{F} = F(\\mathbf{S}, \\boldsymbol{\\Omega}(\\hat{\\boldsymbol{\\gamma}}))$ obtained by minimizing a discrepancy function.                                                                                                                                           |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $N$                               | Sample size for $\mathbf{S}$.                                                                                                                                                                                                                                                                                                           |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\boldsymbol{\Lambda}$            | A $p \times k$ factor pattern matrix.                                                                                                                                                                                                                                                                                                 |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\boldsymbol{\Phi}$               | A $k \times k$ major factor correlation matrix.                                                                                                                                                                                                                                                                                       |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\boldsymbol{\Psi}$               | A $p \times p$ diagonal matrix containing the uniqueness variances.                                                                                                                                                                                                                                                                   |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\mathbf{W}$                      | A $p \times q$ matrix of minor factor loadings for $q$ minor common factors in the Tucker, Koopman, and Linn (TKL; 1969) method.                                                                                                                                                                                                      |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\mathbf{W}^*$                    | A $p \times q$ provisional matrix that is scaled to create $\mathbf{W}$.                                                                                                                                                                                                                                                              |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\nu_\textrm{e} \in [0,1]$        | A parameter used in the TKL method that controls the proportion of uniqueness variances that is reapportioned to the minor common factors.                                                                                                                                                                                              |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\epsilon \in [0,1]$              | A parameter used in the TKL method that controls how minor common factor variance is distributed among the minor common factors.                                                                                                                                                                                                        |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\mathcal{N}(\mu, \sigma^2)$      | A normal distribution with a mean ($\mu$) and variance ($\sigma^2$).                                                                                                                                                                                                                                                                    |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\boldsymbol{\Psi}^*$             | A provisional matrix of uniqueness variances used to create $\mathbf{W}$.                                                                                                                                                                                                                                                               |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\textrm{dg}(\cdot)$              | An operator such that for a square matrix $\mathbf{A}$, $\textrm{dg}(\mathbf{A})$ returns a diagonal matrix containing the diagonal elements of $\mathbf{A}$.                                                                                                                                                                           |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\mathbf{L}$                      | A super matrix containing major and minor factor loadings used in the extension of the TKL method proposed by Hong (1999).                                                                                                                                                                                                              |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\mathbf{C}$                      | The correlation matrix for both the major and minor common factors used in the extension of the TKL method proposed by Hong (1999).                                                                                                                                                                                                     |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\boldsymbol{\Gamma}$             | A $q \times q$ matrix of correlations between the minor common factors used in the extension of the TKL method proposed by Hong (1999).                                                                                                                                                                                               |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\boldsymbol{\Upsilon}$           | A $k \times q$ matrix of correlations between the major and minor common factors used in the extension of the TKL method proposed by Hong (1999).                                                                                                                                                                                     |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\delta$                          | A desired discrepancy function value specified in the Cudeck and Browne model-error method (CB; 1992) such that $F(\\boldsymbol{\\Sigma}, \\boldsymbol{\\Omega}(\\boldsymbol{\\gamma})) = \\delta$.                                                                                                                                   |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\boldsymbol{\Sigma}_0$           | A $p \times p$ covariance (correlation) matrix for a particular vector of model parameters ($\boldsymbol{\gamma}_0$) such that $\\boldsymbol{\\Sigma}\_0 = \boldsymbol{\Omega}(\\boldsymbol{\\gamma}\_0) + \mathbf{E}$.                                                                                                             |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\mathbf{Z}$                      | A fixed $p \times p$ matrix used in the general discrepancy function specification in \autoref{eq:disc-fun}. Different choices of $\mathbf{Z}$ make the general discrepancy function equivalent to either the ordinary least squares or maximum likelihood discrepancy functions.                                                     |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $|\cdot|$                         | The determinant operator such that for a square matrix $\mathbf{A}$, $|\mathbf{A}|$ is the determinant of $\mathbf{A}$.                                                                                                                                                                                                                 |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\boldsymbol{\gamma}_\textrm{ML}$ | The parameter vector that is the minimizer of the maximum likelihood discrepancy function.                                                                                                                                                                                                                                              |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\mathbf{B}$                      | A $\frac{1}{2}(p^2 + p) \times h$ matrix that depends on $\mathbf{Z}$ and $\dot{\boldsymbol{\Sigma}}_i = [\partial \boldsymbol{\Sigma}(\boldsymbol{\gamma})/ \partial \gamma_i]$, where $h$ is the number of free model parameters in $\boldsymbol{\gamma}$. A full definition of $\mathbf{B}$ is provided in Cudeck and Browne (1992). |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\textrm{vecs}$                   | An operator defined such that for a symmetric matrix, $\mathbf{A}$, $\textrm{vecs} \: \mathbf{A} = \begin{bmatrix} a_{11} & a_{12} & a_{22} & a_{13} & \dots & a_{pp} \end{bmatrix}^\prime$.                                                                                                                                            |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| $\tilde{\mathbf{e}}$              | A $\\frac{1}{2}(p\^2 + p) \times 1$ provisional error vector such that $\tilde{\mathbf{e}} = \mathop{\textrm{vecs}} \tilde{\mathbf{E}}$, where $\tilde{\mathbf{E}}$ is the provisional error matrix that is scaled to create $\mathbf{E}$ in the CB method.                                                                           |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
|                                   |                                                                                                                                                                                                                                                                                                                                         |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
|                                   |                                                                                                                                                                                                                                                                                                                                         |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
|                                   |                                                                                                                                                                                                                                                                                                                                         |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
|                                   |                                                                                                                                                                                                                                                                                                                                         |                                                         |
+-----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+

<!--chapter:end:chapters/91-appendix-a.Rmd-->

# (APPENDIX) Appendix {-}

# R Code {#appendix-a}

\setstretch{1}
    <!-- Set Appendices to single spacing -->

## Implementations of Model Error Methods {#noisemaker-code}

The implementations of the model error methods used are also available bundled as an R package, *noisemaker*, which can be downloaded from [https://www.github.com/JustinKracht/noisemaker](https://www.github.com/JustinKracht/noisemaker).

```{r find-code-files, echo = FALSE, include = FALSE}
code_files <- list.files(here::here("code/noisemaker"), full.names = TRUE)
code_files <- code_files[stringr::str_ends(code_files, ".R")]

# Function to read code files and include them in R Markdown
read_files <- function(files) {
  unlist(lapply(files, xfun::read_utf8))
}
```

\small

```{r write-noisemaker-code, eval = FALSE, code = read_files(code_files), echo = TRUE}

```

## Main Simulation Code {#main-simulation}

```{r find-simulation-code-files}
simulation_code_files <- list.files(here::here("code/model-error-simulation"), 
                                    full.names = TRUE)
simulation_code_files <- simulation_code_files[stringr::str_ends(simulation_code_files, ".R")]
```

\small

```{r write-simulation-code, eval = FALSE, code = read_files(simulation_code_files), echo = TRUE}

```

## Secondary Simulation: Check Effect of $\lambda$ Values {#effect-of-lambda}

\small

```{r check-lambda-values, eval = FALSE, echo = TRUE}
# Set the number of reps
reps <- 200

# Create a matrix of fully-crossed conditions
factors <- unique(results_matrix$factors)
items_per_factor <- 15
loading <- c(.4)
target_rmsea <- c(0.090)
penalty <- c(0, .1, 1, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6)

conditions_matrix <- expand.grid(
  factors = factors,
  items_per_factor = items_per_factor,
  loading = loading,
  target_rmsea = target_rmsea
)


# Function to check whether a W matrix has more than two factor loadings greater
# than 0.3 for any minor factor.
check_constraints <- function(W) {
  any(max(apply(abs(W) >= .3, 2, sum)) > 2)
}

# For each condition, generate a population correlation matrix without model
# error and then generate 200 population correlations with model error using
# the TKL (RMSEA) method
set.seed(666)
constraint_violations <- map_dfr(
  .x = 1:nrow(conditions_matrix), 
  .f = function(condition) {
    
    cat("\nWorking on condition", condition, "of", nrow(conditions_matrix))
    
    # Generate population correlation matrix without model error
    mod <- simFA(
      Model = list(NFac = conditions_matrix$factors[condition],
                   NItemPerFac = conditions_matrix$items_per_factor[condition],
                   Model = "orthogonal"),
      Loadings = list(FacLoadDist = "fixed",
                      FacLoadRange = conditions_matrix$loading[condition])
    )
    
    # Generate 200 population correlation matrices with model error
    pro_map_dfr(.x = 1:reps,
                .f = function(x, target_rmsea) {
                  map_dfr(.x = penalty, 
                          .f = function(penalty, mod, target_rmsea, seed) {
                            set.seed(seed)
                            sol <- noisemaker(mod = mod, 
                                              method = "TKL", 
                                              target_rmsea = target_rmsea,
                                              target_cfi = NULL,
                                              tkl_ctrl = list(penalty = penalty,
                                                              NWmaxLoading = 2,
                                                              WmaxLoading = .3))
                            w_constraints_violated <- check_constraints(sol$W)
                            
                            c(penalty = penalty, 
                              constraints_violated = w_constraints_violated)
                          },
                          mod = mod,
                          target_rmsea = target_rmsea,
                          seed = sample(1e6, 1))
                }, target_rmsea = conditions_matrix$target_rmsea[condition])
  }
)

# Bind the conditions matrix to the results to indicate which result belongs to
# which condition
constraint_violations <- bind_cols(
  conditions_matrix[rep(1:nrow(conditions_matrix), 
                        each = length(penalty) * reps),],
  constraint_violations
)

# Plot the results
constraint_violations %>%
  mutate(factors = as.factor(factors),
         penalty = factor(penalty, 
                          labels = c("0", "0.1", "1", "10", 
                                     "100", "1,000", "10,000",
                                     "100,000", "1,000,000"))) %>%
  group_by(factors, penalty) %>%
  summarise(percent = mean(constraints_violated, na.rm = TRUE)) %>%
  ggplot(aes(x = penalty, y = percent, color = factors, shape = factors, 
             linetype = factors, group = factors)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(labels = scales::percent) +
  scale_color_brewer(palette = "Dark2", type = "qual") +
  labs(y = "Cases with Violtated W Constraints",
       x = latex2exp::TeX("$\\lambda$"),
       color = "Factors", shape = "Factors", linetype = "Factors") +
  theme_bw() +
  theme(legend.position = "bottom")

# Save the plot
ggsave(filename = here("img/penalty_values.png"),
       dpi = 320,
       height = 4,
       width = 6)
```

## Secondary Simulation: Check Whether $\textrm{TKL}_{\textrm{CFI}}$ Requires A Penalty {#tkl-cfi-penalty}

\small

```{r check-whether-tkl-cfi-needs-penalty, eval = FALSE, echo=TRUE}
# Check whether the TKL (CFI) method leads to solutions that violate the W 
# constraints when no penalty is applied
set.seed(42)

# Create a population correlation matrix without model error (Omega)
mod <- simFA(Model = list(NFac = 10,
                          NItemPerFac = 15,
                          Model = "orthogonal"),
             Loadings = list(FacLoadDist = "fixed",
                             FacLoadRange = .4))

# Function to check whether a W matrix has more than two factor loadings greater
# than 0.3 for any minor factor.
check_constraints <- function(W) {
  any(max(apply(abs(W) >= .3, 2, sum)) > 2)
}

# Generate 200 population correlation matrices with model error (Sigma) using 
# the TKL (CFI) method without a penalty, then check how often the constraints
# on W were violated
constraints_violated_vec <- pbmclapply(
  X = 1:200,
  FUN = function(x) {
    sol <- noisemaker(mod = mod, 
                      method = "TKL", 
                      target_rmsea = 0.09,
                      target_cfi = 0.9,
                      tkl_ctrl = list(penalty = 0))
    check_constraints(sol$W)
  },
  mc.cores = 4
)

# What percent of cases had violated W constraints?
mean(constraints_violated_vec, na.rm = TRUE)
```

## Secondary Simulation: Check Recovery of Model Fit Indices with Known-Possible Values {#check-model-fit-recovery-code}

```{r find-secondary-simulation-code-files}
secondary_simulation_code_files <- list.files(here::here("code/misc"), 
                                    full.names = TRUE)
secondary_simulation_code_files <- secondary_simulation_code_files[stringr::str_ends(secondary_simulation_code_files, ".R")]
```

\small

```{r write-secondary-simulation-code, eval = FALSE, code = read_files(secondary_simulation_code_files), echo = TRUE}

```




<!--chapter:end:chapters/92-appendix-b.Rmd-->

